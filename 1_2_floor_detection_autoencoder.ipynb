{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Example With Boston Dataset: Standardized \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "#load data\n",
    "path = 'C:/Users/mys12/Desktop/northeastern/summer2018/data_Robust_Fingerprinting-master/DISTRIBUTED_OPENSOURCE/FINGERPRINTING_DB'\n",
    "\n",
    "train_rss = pd.read_csv(path + '/Training_rss_21Aug17.csv', header = 0)\n",
    "train_coord = pd.read_csv(path + '/Training_coordinates_21Aug17.csv', header = 0)\n",
    "\n",
    "test_rss = pd.read_csv(path + '/Test_rss_21Aug17.csv', header = 0)\n",
    "test_coord = pd.read_csv(path + '/Test_coordinates_21Aug17.csv', header = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_labels:  (array(['0.0', '11.1', '14.8', '3.7', '7.4'], dtype=object), array([1264,  699,  109, 1108,  770], dtype=int64))\n",
      "(3950, 992)\n",
      "train_labels:  (array(['0.0', '11.1', '14.8', '3.7', '7.4'], dtype=object), array([226, 118,  17, 197, 138], dtype=int64))\n",
      "(696, 992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test\n",
    "test_r = test_rss.values\n",
    "test_ch = test_coord.ix[:,-1]\n",
    "test_labels = np.asarray(test_ch.map(str))\n",
    "print(\"test_labels: \", np.unique(test_labels, return_counts=True))\n",
    "\n",
    "test_labels = np.asarray(pd.get_dummies(test_labels))\n",
    "\n",
    "#Scale transforms data to center to the mean and component wise scale to unit variance\n",
    "normalizer = preprocessing.Normalizer().fit(np.asarray(test_r))\n",
    "test_r_features = normalizer.transform(np.asarray(test_r))\n",
    "print(test_r_features.shape)\n",
    "\n",
    "# train \n",
    "train_r = train_rss.values\n",
    "train_ch = train_coord.iloc[:,-1]\n",
    "building_floors_str = train_ch.map(str)  #convert all the building floors to strings\n",
    "print(\"train_labels: \", np.unique(building_floors_str, return_counts=True))\n",
    "\n",
    "train_labels = np.asarray(building_floors_str)\n",
    "#convert labels to categorical variables, dummy_labels has type 'pandas.core.frame.DataFrame'\n",
    "dummy_label = pd.get_dummies(train_labels)\n",
    "train_labels = np.asarray(dummy_label)\n",
    "\n",
    "train_r_features = normalizer.transform(np.asarray(train_r))\n",
    "print(train_r_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate len(test_r_features) of floats in between 0 and 1\n",
    "train_val_split = np.random.rand(len(test_r_features))\n",
    "#convert train_val_split to an array of booleans: if elem < 0.7 = true, else: false\n",
    "train_val_split = train_val_split < 0.70 #should contain ~70% percent true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will then split our given training set into training + validation \n",
    "# change the train to test, because the test has more data\n",
    "train_X = test_r_features[train_val_split]  \n",
    "train_y = test_labels[train_val_split]\n",
    "val_X = test_r_features[~train_val_split]\n",
    "val_y = test_labels[~train_val_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 100\n",
    "batch_size = 64\n",
    "input_size = 992\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=input_size, activation='tanh', bias=True))\n",
    "    model.add(Dense(128, activation='tanh', bias=True))\n",
    "    model.add(Dense(64, activation='tanh', bias=True))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(e):   \n",
    "    e.add(Dense(128, input_dim=64, activation='tanh', bias=True))\n",
    "    e.add(Dense(256, activation='tanh', bias=True))\n",
    "    e.add(Dense(input_size, activation='tanh', bias=True))\n",
    "    e.compile(optimizer='adam', loss='mse')\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=992, activation=\"tanh\", use_bias=True)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"tanh\", use_bias=True)`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"tanh\", use_bias=True)`\n",
      "  \"\"\"\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=64, activation=\"tanh\", use_bias=True)`\n",
      "  \n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"tanh\", use_bias=True)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(992, activation=\"tanh\", use_bias=True)`\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.7499e-04     \n",
      "Epoch 2/100\n",
      "2770/2770 [==============================] - 0s - loss: 1.2332e-04     \n",
      "Epoch 3/100\n",
      "2770/2770 [==============================] - 0s - loss: 9.5317e-05     \n",
      "Epoch 4/100\n",
      "2770/2770 [==============================] - 0s - loss: 8.2359e-05     \n",
      "Epoch 5/100\n",
      "2770/2770 [==============================] - 0s - loss: 7.3340e-05     \n",
      "Epoch 6/100\n",
      "2770/2770 [==============================] - 0s - loss: 6.5863e-05     \n",
      "Epoch 7/100\n",
      "2770/2770 [==============================] - 0s - loss: 6.0079e-05     \n",
      "Epoch 8/100\n",
      "2770/2770 [==============================] - 0s - loss: 5.5825e-05     \n",
      "Epoch 9/100\n",
      "2770/2770 [==============================] - 0s - loss: 5.2495e-05     \n",
      "Epoch 10/100\n",
      "2770/2770 [==============================] - 0s - loss: 4.9815e-05     \n",
      "Epoch 11/100\n",
      "2770/2770 [==============================] - 0s - loss: 4.7307e-05     \n",
      "Epoch 12/100\n",
      "2770/2770 [==============================] - 0s - loss: 4.5245e-05     \n",
      "Epoch 13/100\n",
      "2770/2770 [==============================] - 0s - loss: 4.3486e-05     \n",
      "Epoch 14/100\n",
      "2770/2770 [==============================] - 0s - loss: 4.1839e-05     \n",
      "Epoch 15/100\n",
      "2770/2770 [==============================] - 0s - loss: 4.0483e-05     \n",
      "Epoch 16/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.9319e-05     \n",
      "Epoch 17/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.8223e-05     \n",
      "Epoch 18/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.7261e-05     \n",
      "Epoch 19/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.6438e-05     \n",
      "Epoch 20/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.5615e-05     \n",
      "Epoch 21/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.4905e-05     \n",
      "Epoch 22/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.4256e-05     \n",
      "Epoch 23/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.3609e-05     \n",
      "Epoch 24/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.3025e-05     \n",
      "Epoch 25/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.2439e-05     - ETA: 0s - loss: 3\n",
      "Epoch 26/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.1980e-05     \n",
      "Epoch 27/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.1503e-05     \n",
      "Epoch 28/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.1116e-05     \n",
      "Epoch 29/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.0660e-05     \n",
      "Epoch 30/100\n",
      "2770/2770 [==============================] - 0s - loss: 3.0273e-05     \n",
      "Epoch 31/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.9921e-05     \n",
      "Epoch 32/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.9597e-05     \n",
      "Epoch 33/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.9290e-05     \n",
      "Epoch 34/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.8980e-05     \n",
      "Epoch 35/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.8680e-05     \n",
      "Epoch 36/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.8420e-05     \n",
      "Epoch 37/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.8178e-05     \n",
      "Epoch 38/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.7984e-05     \n",
      "Epoch 39/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.7667e-05     \n",
      "Epoch 40/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.7475e-05     \n",
      "Epoch 41/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.7249e-05     \n",
      "Epoch 42/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.7043e-05     \n",
      "Epoch 43/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.6885e-05     \n",
      "Epoch 44/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.6618e-05     \n",
      "Epoch 45/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.6433e-05     \n",
      "Epoch 46/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.6334e-05     \n",
      "Epoch 47/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.6128e-05     \n",
      "Epoch 48/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5929e-05     \n",
      "Epoch 49/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5775e-05     \n",
      "Epoch 50/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5540e-05     \n",
      "Epoch 51/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5444e-05     \n",
      "Epoch 52/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5317e-05     \n",
      "Epoch 53/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5119e-05     \n",
      "Epoch 54/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.5041e-05     \n",
      "Epoch 55/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4875e-05     \n",
      "Epoch 56/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4838e-05     \n",
      "Epoch 57/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4696e-05     \n",
      "Epoch 58/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4642e-05     \n",
      "Epoch 59/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4496e-05     \n",
      "Epoch 60/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4472e-05     \n",
      "Epoch 61/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4372e-05     \n",
      "Epoch 62/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4286e-05     \n",
      "Epoch 63/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4208e-05     \n",
      "Epoch 64/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4205e-05     \n",
      "Epoch 65/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4135e-05     \n",
      "Epoch 66/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4065e-05     \n",
      "Epoch 67/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.4049e-05     \n",
      "Epoch 68/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3997e-05     \n",
      "Epoch 69/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3925e-05     \n",
      "Epoch 70/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3912e-05     \n",
      "Epoch 71/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3921e-05     \n",
      "Epoch 72/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3862e-05     \n",
      "Epoch 73/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3802e-05     \n",
      "Epoch 74/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3817e-05     \n",
      "Epoch 75/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3789e-05     \n",
      "Epoch 76/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3775e-05     \n",
      "Epoch 77/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3907e-05     \n",
      "Epoch 78/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3806e-05     \n",
      "Epoch 79/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3682e-05     \n",
      "Epoch 80/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3761e-05     \n",
      "Epoch 81/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3744e-05     \n",
      "Epoch 82/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3681e-05     \n",
      "Epoch 83/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3674e-05     \n",
      "Epoch 84/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3721e-05     \n",
      "Epoch 85/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3628e-05     \n",
      "Epoch 86/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3787e-05     \n",
      "Epoch 87/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3796e-05     \n",
      "Epoch 88/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3616e-05     \n",
      "Epoch 89/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3575e-05     \n",
      "Epoch 90/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3539e-05     \n",
      "Epoch 91/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3551e-05     \n",
      "Epoch 92/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3605e-05     \n",
      "Epoch 93/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3563e-05     \n",
      "Epoch 94/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3537e-05     \n",
      "Epoch 95/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3543e-05     \n",
      "Epoch 96/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3489e-05     \n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2770/2770 [==============================] - 0s - loss: 2.3644e-05     \n",
      "Epoch 98/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3600e-05     \n",
      "Epoch 99/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3508e-05     \n",
      "Epoch 100/100\n",
      "2770/2770 [==============================] - 0s - loss: 2.3435e-05     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf61a2bd30>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = encoder()\n",
    "d = decoder(e)\n",
    "d.fit(train_X, train_X, nb_epoch=nb_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(d):\n",
    "    num_to_remove = 3\n",
    "    for i in range(num_to_remove):\n",
    "        d.pop()\n",
    "    d.add(Dense(128, input_dim=64, activation='tanh', bias=True))\n",
    "    d.add(Dense(128, activation='tanh', bias=True))\n",
    "    d.add(Dense(num_classes, activation='softmax', bias=True))\n",
    "    d.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=64, activation=\"tanh\", use_bias=True)`\n",
      "  \"\"\"\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"tanh\", use_bias=True)`\n",
      "  \n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, activation=\"softmax\", use_bias=True)`\n",
      "  import sys\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2770 samples, validate on 1180 samples\n",
      "Epoch 1/100\n",
      "2770/2770 [==============================] - 0s - loss: 1.0045 - acc: 0.5477 - val_loss: 0.7864 - val_acc: 0.7568\n",
      "Epoch 2/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.5047 - acc: 0.8007 - val_loss: 0.3567 - val_acc: 0.8686\n",
      "Epoch 3/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.3651 - acc: 0.8578 - val_loss: 0.2922 - val_acc: 0.9051\n",
      "Epoch 4/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.3055 - acc: 0.8931 - val_loss: 0.2589 - val_acc: 0.9203\n",
      "Epoch 5/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2983 - acc: 0.8996 - val_loss: 0.4030 - val_acc: 0.8415\n",
      "Epoch 6/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.3567 - acc: 0.8697 - val_loss: 0.3585 - val_acc: 0.8703\n",
      "Epoch 7/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2672 - acc: 0.9083 - val_loss: 0.4817 - val_acc: 0.8203\n",
      "Epoch 8/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.3055 - acc: 0.8899 - val_loss: 0.2852 - val_acc: 0.8958\n",
      "Epoch 9/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2671 - acc: 0.9105 - val_loss: 0.3233 - val_acc: 0.8932\n",
      "Epoch 10/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2595 - acc: 0.9090 - val_loss: 0.2652 - val_acc: 0.9127\n",
      "Epoch 11/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2342 - acc: 0.9162 - val_loss: 0.3061 - val_acc: 0.8780\n",
      "Epoch 12/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2431 - acc: 0.9162 - val_loss: 0.2542 - val_acc: 0.9161\n",
      "Epoch 13/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2509 - acc: 0.9116 - val_loss: 0.4197 - val_acc: 0.8517\n",
      "Epoch 14/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2735 - acc: 0.9032 - val_loss: 0.2845 - val_acc: 0.9042\n",
      "Epoch 15/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2297 - acc: 0.9195 - val_loss: 0.2205 - val_acc: 0.9373\n",
      "Epoch 16/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2317 - acc: 0.9188 - val_loss: 0.2846 - val_acc: 0.9076\n",
      "Epoch 17/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2166 - acc: 0.9256 - val_loss: 0.2228 - val_acc: 0.9347\n",
      "Epoch 18/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2279 - acc: 0.9206 - val_loss: 0.2252 - val_acc: 0.9305\n",
      "Epoch 19/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1997 - acc: 0.9368 - val_loss: 0.2685 - val_acc: 0.9110\n",
      "Epoch 20/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2103 - acc: 0.9274 - val_loss: 0.2261 - val_acc: 0.9212\n",
      "Epoch 21/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1842 - acc: 0.9375 - val_loss: 0.2392 - val_acc: 0.9297\n",
      "Epoch 22/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1984 - acc: 0.9361 - val_loss: 0.2342 - val_acc: 0.9331\n",
      "Epoch 23/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2721 - acc: 0.9043 - val_loss: 0.2371 - val_acc: 0.9229\n",
      "Epoch 24/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2049 - acc: 0.9267 - val_loss: 0.2332 - val_acc: 0.9229\n",
      "Epoch 25/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1946 - acc: 0.9285 - val_loss: 0.2435 - val_acc: 0.9186\n",
      "Epoch 26/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1594 - acc: 0.9495 - val_loss: 0.3431 - val_acc: 0.8856\n",
      "Epoch 27/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2246 - acc: 0.9199 - val_loss: 0.2307 - val_acc: 0.9271\n",
      "Epoch 28/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1730 - acc: 0.9433 - val_loss: 0.2336 - val_acc: 0.9237\n",
      "Epoch 29/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1783 - acc: 0.9372 - val_loss: 0.2761 - val_acc: 0.9068\n",
      "Epoch 30/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1902 - acc: 0.9300 - val_loss: 0.2255 - val_acc: 0.9364\n",
      "Epoch 31/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2166 - acc: 0.9227 - val_loss: 0.2573 - val_acc: 0.9195\n",
      "Epoch 32/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1696 - acc: 0.9397 - val_loss: 0.2663 - val_acc: 0.9153\n",
      "Epoch 33/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2063 - acc: 0.9260 - val_loss: 0.2163 - val_acc: 0.9314\n",
      "Epoch 34/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1777 - acc: 0.9372 - val_loss: 0.4946 - val_acc: 0.8331\n",
      "Epoch 35/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2275 - acc: 0.9191 - val_loss: 0.2695 - val_acc: 0.9102\n",
      "Epoch 36/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2279 - acc: 0.9184 - val_loss: 0.3463 - val_acc: 0.8678\n",
      "Epoch 37/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1886 - acc: 0.9310 - val_loss: 0.2089 - val_acc: 0.9339\n",
      "Epoch 38/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1568 - acc: 0.9430 - val_loss: 0.3004 - val_acc: 0.9051\n",
      "Epoch 39/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1515 - acc: 0.9462 - val_loss: 0.2040 - val_acc: 0.9432\n",
      "Epoch 40/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1604 - acc: 0.9444 - val_loss: 0.2360 - val_acc: 0.9305\n",
      "Epoch 41/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1905 - acc: 0.9336 - val_loss: 0.3758 - val_acc: 0.8847\n",
      "Epoch 42/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1623 - acc: 0.9430 - val_loss: 0.2246 - val_acc: 0.9280\n",
      "Epoch 43/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1509 - acc: 0.9523 - val_loss: 0.2132 - val_acc: 0.9424\n",
      "Epoch 44/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1654 - acc: 0.9394 - val_loss: 0.2814 - val_acc: 0.8992\n",
      "Epoch 45/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1808 - acc: 0.9394 - val_loss: 0.3410 - val_acc: 0.8949\n",
      "Epoch 46/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1935 - acc: 0.9350 - val_loss: 0.4363 - val_acc: 0.8339\n",
      "Epoch 47/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1762 - acc: 0.9343 - val_loss: 0.2473 - val_acc: 0.9169\n",
      "Epoch 48/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1648 - acc: 0.9394 - val_loss: 0.2292 - val_acc: 0.9331\n",
      "Epoch 49/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1356 - acc: 0.9513 - val_loss: 0.2952 - val_acc: 0.9085\n",
      "Epoch 50/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1780 - acc: 0.9347 - val_loss: 0.2542 - val_acc: 0.9169\n",
      "Epoch 51/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1646 - acc: 0.9415 - val_loss: 0.2725 - val_acc: 0.9093\n",
      "Epoch 52/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1437 - acc: 0.9513 - val_loss: 0.2494 - val_acc: 0.9220\n",
      "Epoch 53/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1822 - acc: 0.9325 - val_loss: 0.2225 - val_acc: 0.9322\n",
      "Epoch 54/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1747 - acc: 0.9336 - val_loss: 0.3011 - val_acc: 0.8924\n",
      "Epoch 55/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1634 - acc: 0.9422 - val_loss: 0.2477 - val_acc: 0.9246\n",
      "Epoch 56/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1860 - acc: 0.9303 - val_loss: 0.2253 - val_acc: 0.9331\n",
      "Epoch 57/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1549 - acc: 0.9455 - val_loss: 0.3196 - val_acc: 0.8864\n",
      "Epoch 58/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.2013 - acc: 0.9227 - val_loss: 0.2162 - val_acc: 0.9415\n",
      "Epoch 59/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1262 - acc: 0.9552 - val_loss: 0.2432 - val_acc: 0.9186\n",
      "Epoch 60/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1181 - acc: 0.9603 - val_loss: 0.2142 - val_acc: 0.9415\n",
      "Epoch 61/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1616 - acc: 0.9444 - val_loss: 0.2524 - val_acc: 0.9220\n",
      "Epoch 62/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1386 - acc: 0.9502 - val_loss: 0.2293 - val_acc: 0.9237\n",
      "Epoch 63/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1339 - acc: 0.9552 - val_loss: 0.2395 - val_acc: 0.9305\n",
      "Epoch 64/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1573 - acc: 0.9437 - val_loss: 0.2078 - val_acc: 0.9339\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2770/2770 [==============================] - 0s - loss: 0.1934 - acc: 0.9278 - val_loss: 0.4666 - val_acc: 0.8695\n",
      "Epoch 66/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1665 - acc: 0.9433 - val_loss: 0.2609 - val_acc: 0.9246\n",
      "Epoch 67/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1500 - acc: 0.9477 - val_loss: 0.2344 - val_acc: 0.9288\n",
      "Epoch 68/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1259 - acc: 0.9560 - val_loss: 0.2086 - val_acc: 0.9441\n",
      "Epoch 69/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1113 - acc: 0.9610 - val_loss: 0.2205 - val_acc: 0.9415\n",
      "Epoch 70/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1178 - acc: 0.9570 - val_loss: 0.2350 - val_acc: 0.9271\n",
      "Epoch 71/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1282 - acc: 0.9484 - val_loss: 0.2257 - val_acc: 0.9305\n",
      "Epoch 72/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1361 - acc: 0.9527 - val_loss: 0.3745 - val_acc: 0.8873\n",
      "Epoch 73/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1485 - acc: 0.9480 - val_loss: 0.2519 - val_acc: 0.9254\n",
      "Epoch 74/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1422 - acc: 0.9466 - val_loss: 0.3005 - val_acc: 0.9076\n",
      "Epoch 75/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1370 - acc: 0.9487 - val_loss: 0.2459 - val_acc: 0.9263\n",
      "Epoch 76/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1396 - acc: 0.9491 - val_loss: 0.2759 - val_acc: 0.9144\n",
      "Epoch 77/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1637 - acc: 0.9383 - val_loss: 0.2522 - val_acc: 0.9254\n",
      "Epoch 78/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1280 - acc: 0.9549 - val_loss: 0.2498 - val_acc: 0.9237\n",
      "Epoch 79/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1206 - acc: 0.9556 - val_loss: 0.2357 - val_acc: 0.9347\n",
      "Epoch 80/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1226 - acc: 0.9523 - val_loss: 0.2762 - val_acc: 0.9169\n",
      "Epoch 81/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1187 - acc: 0.9574 - val_loss: 0.2245 - val_acc: 0.9364\n",
      "Epoch 82/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1155 - acc: 0.9585 - val_loss: 0.2311 - val_acc: 0.9305\n",
      "Epoch 83/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1038 - acc: 0.9632 - val_loss: 0.2916 - val_acc: 0.9093\n",
      "Epoch 84/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1818 - acc: 0.9289 - val_loss: 0.4769 - val_acc: 0.8076\n",
      "Epoch 85/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1743 - acc: 0.9361 - val_loss: 0.2429 - val_acc: 0.9254\n",
      "Epoch 86/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1831 - acc: 0.9347 - val_loss: 0.3828 - val_acc: 0.8585\n",
      "Epoch 87/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1156 - acc: 0.9603 - val_loss: 0.2245 - val_acc: 0.9424\n",
      "Epoch 88/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1304 - acc: 0.9523 - val_loss: 0.2745 - val_acc: 0.9212\n",
      "Epoch 89/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1195 - acc: 0.9606 - val_loss: 0.2214 - val_acc: 0.9415\n",
      "Epoch 90/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1387 - acc: 0.9487 - val_loss: 0.2909 - val_acc: 0.9093\n",
      "Epoch 91/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1267 - acc: 0.9531 - val_loss: 0.2643 - val_acc: 0.9169\n",
      "Epoch 92/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1166 - acc: 0.9556 - val_loss: 0.2295 - val_acc: 0.9432\n",
      "Epoch 93/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.0927 - acc: 0.9675 - val_loss: 0.2371 - val_acc: 0.9297\n",
      "Epoch 94/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.0952 - acc: 0.9653 - val_loss: 0.2143 - val_acc: 0.9398\n",
      "Epoch 95/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1207 - acc: 0.9567 - val_loss: 0.2882 - val_acc: 0.9186\n",
      "Epoch 96/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1631 - acc: 0.9397 - val_loss: 0.3151 - val_acc: 0.9093\n",
      "Epoch 97/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1580 - acc: 0.9451 - val_loss: 0.2455 - val_acc: 0.9390\n",
      "Epoch 98/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.1216 - acc: 0.9552 - val_loss: 0.2246 - val_acc: 0.9339\n",
      "Epoch 99/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.0914 - acc: 0.9668 - val_loss: 0.3425 - val_acc: 0.9195\n",
      "Epoch 100/100\n",
      "2770/2770 [==============================] - 0s - loss: 0.0858 - acc: 0.9664 - val_loss: 0.2900 - val_acc: 0.9203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf6407ac88>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = classifier(d)\n",
    "c.fit(train_X, train_y, validation_data=(val_X, val_y), nb_epoch=nb_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/696 [=======================>......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "loss, acc = c.evaluate(train_r_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306821404004 0.913793102763\n"
     ]
    }
   ],
   "source": [
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
