{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers \n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import auto_regression as ar\n",
    "import regular_regression as rr\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#test\n",
    "#load data\n",
    "path = 'C:/Users/mys12/Desktop/northeastern/summer2018/data_Robust_Fingerprinting-master/DISTRIBUTED_OPENSOURCE/FINGERPRINTING_DB'\n",
    "\n",
    "test_rss = pd.read_csv(path + '/Training_rss_21Aug17.csv', header = 0)\n",
    "test_coord = pd.read_csv(path + '/Training_coordinates_21Aug17.csv', header = 0)\n",
    "test_rss = test_rss.replace(100, 0)\n",
    "\n",
    "train_rss = pd.read_csv(path + '/Test_rss_21Aug17.csv', header = 0)\n",
    "train_coord = pd.read_csv(path + '/Test_coordinates_21Aug17.csv', header = 0)\n",
    "train_rss = train_rss.replace(100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train_rss, train_coord], axis=1, ignore_index=True)\n",
    "test = pd.concat([test_rss, test_coord], axis=1, ignore_index=True)\n",
    "train = np.asarray(train)\n",
    "test = np.asarray(test)\n",
    "\n",
    "# first floor\n",
    "train1 = train[train[:,-1]==0.0]\n",
    "normalizer = preprocessing.Normalizer().fit(train1[:,:-3])\n",
    "train1_r=normalizer.transform(train1[:,:-3])\n",
    "train1_c=train1[:,-3:-1]\n",
    "print(train1_r.shape[1])\n",
    "\n",
    "test1 = test[test[:,-1]==0.0]\n",
    "test1_r=normalizer.transform(test1[:,:-3])\n",
    "test1_c=test1[:,-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata(rss, locations):\n",
    "    # the origin of the room\n",
    "    origin = np.amin(locations,axis=0)\n",
    "    #size of the room\n",
    "    room_size = np.amax(locations, axis=0)-origin\n",
    "    # position respect to origin\n",
    "    train_Yy = locations - origin\n",
    "    train_Xx = np.asarray(rss, dtype=np.float64)\n",
    "    return train_Xx, train_Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val(rss, locations):\n",
    "#     train_Xx, train_Yy = predata(rss, locations)\n",
    "#     train_x, val_x, train_y, val_y = train_test_split(train_Xx, train_Yy, test_size=0.2)\n",
    "#     return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    error = np.sqrt(np.sum((predictions - labels)**2, 1))\n",
    "    return error, np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = predata(train1_r, train1_c)\n",
    "testX, testY = predata(test1_r, test1_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=rr.train_model\n",
    "clf2=ar.regression\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=4)\n",
    "clf3 = neigh.fit\n",
    "\n",
    "clf4 = rr.train_model2\n",
    "\n",
    "\n",
    "dr = DecisionTreeRegressor(max_depth = 9)\n",
    "clf5 = dr.fit\n",
    "\n",
    "clfs = [clf1,clf2,clf3,clf4,clf5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaped(predict):\n",
    "    size = predict.shape[0]\n",
    "    j = predict.reshape((2*size, 1))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "#     X_train = reshaped(X_train)\n",
    "#     y_train = reshaped(y_train)\n",
    "#     X_test = reshaped(X_test)\n",
    "\n",
    "    blend_train = np.zeros((y_train.shape[0],2))\n",
    "    blend_test = np.zeros((X_test.shape[0],2))\n",
    "    blend_test_skf = np.zeros((X_test.shape[0],2,5)) \n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(list(kf.split(X_train))):\n",
    "        print(\"Fold\", i)   \n",
    "\n",
    "        kf_X_train = X_train[train_index]\n",
    "        kf_y_train = y_train[train_index]\n",
    "        kf_X_test = X_train[test_index]\n",
    "        kf_y_test = y_train[test_index]\n",
    "        \n",
    "        model = clf(kf_X_train,kf_y_train)\n",
    "        \n",
    "        blend_train[test_index]=model.predict(kf_X_test)  # 992*2\n",
    "        \n",
    "        blend_test_skf[:,:,i] = model.predict(X_test)   # 1*292*2\n",
    "    \n",
    "    blend_test[:,:]=blend_test_skf.mean(axis=2)\n",
    "    return blend_train, blend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "8.50018640807\n"
     ]
    }
   ],
   "source": [
    "blend_train3, blend_test3 = get_oof(clf3, trainX, trainY, testX)\n",
    "e3, e3_mean = accuracy(blend_test3, testY)\n",
    "print(e3_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 103.203   ,   37.66458 ],\n",
       "       [  82.40365 ,   27.53373 ],\n",
       "       [ 114.986   ,   37.99638 ],\n",
       "       [ 132.448   ,   28.57548 ],\n",
       "       [  55.4249  ,   15.64583 ],\n",
       "       [  94.796   ,   53.47178 ],\n",
       "       [  43.45965 ,   19.14968 ],\n",
       "       [ 105.108   ,   24.80828 ],\n",
       "       [  53.98    ,   16.07823 ],\n",
       "       [  66.5732  ,   31.89973 ],\n",
       "       [  77.25165 ,   22.00058 ],\n",
       "       [  69.73575 ,   24.38083 ],\n",
       "       [  67.9996  ,   23.37908 ],\n",
       "       [  53.8442  ,   25.46698 ],\n",
       "       [ 132.      ,   38.07608 ],\n",
       "       [ 119.89    ,   69.55053 ],\n",
       "       [ 133.158   ,   46.06573 ],\n",
       "       [  95.0293  ,   35.93773 ],\n",
       "       [ 139.5045  ,   28.26263 ],\n",
       "       [  23.47205 ,   24.09053 ],\n",
       "       [  59.3365  ,   38.46588 ],\n",
       "       [  80.73075 ,   18.97433 ],\n",
       "       [ 107.9495  ,   18.32258 ],\n",
       "       [  93.3715  ,   62.86078 ],\n",
       "       [ 121.742   ,   66.81118 ],\n",
       "       [  50.91115 ,   24.21203 ],\n",
       "       [  53.66365 ,   16.965875],\n",
       "       [  99.4035  ,   37.05533 ],\n",
       "       [  55.6733  ,   15.04993 ],\n",
       "       [  80.44565 ,   35.13448 ],\n",
       "       [ 105.946   ,   30.64963 ],\n",
       "       [  95.5035  ,   56.77608 ],\n",
       "       [ 140.1745  ,   32.65023 ],\n",
       "       [  86.0861  ,   28.47718 ],\n",
       "       [  97.045   ,   25.55613 ],\n",
       "       [  91.31    ,   67.11598 ],\n",
       "       [ 108.192   ,   19.92863 ],\n",
       "       [  59.18855 ,   16.72813 ],\n",
       "       [  93.6258  ,   28.65898 ],\n",
       "       [  87.147   ,   28.23988 ],\n",
       "       [  58.86105 ,   67.00298 ],\n",
       "       [  83.4294  ,   24.40193 ],\n",
       "       [ 119.0515  ,   15.85505 ],\n",
       "       [ 120.592   ,   70.11523 ],\n",
       "       [  75.74265 ,   24.92688 ],\n",
       "       [ 120.339   ,   67.93643 ],\n",
       "       [  46.0224  ,   29.83498 ],\n",
       "       [  23.98045 ,   21.73963 ],\n",
       "       [ 107.3665  ,   19.71243 ],\n",
       "       [  83.81935 ,   16.94948 ],\n",
       "       [ 167.0805  ,   53.69358 ],\n",
       "       [  66.8767  ,   29.01123 ],\n",
       "       [  61.42175 ,   61.13543 ],\n",
       "       [  61.66475 ,   60.82213 ],\n",
       "       [  65.4572  ,   52.66523 ],\n",
       "       [ 139.192   ,   37.27838 ],\n",
       "       [  64.6813  ,   39.37738 ],\n",
       "       [ 111.7715  ,   16.23503 ],\n",
       "       [ 130.1085  ,   37.81398 ],\n",
       "       [  64.05415 ,   25.52198 ],\n",
       "       [ 120.5695  ,   29.65138 ],\n",
       "       [  92.535   ,   37.88638 ],\n",
       "       [ 108.1535  ,   22.96703 ],\n",
       "       [  53.7186  ,   15.80243 ],\n",
       "       [  72.8145  ,   24.35993 ],\n",
       "       [  92.702   ,   35.20523 ],\n",
       "       [  96.45135 ,   25.81983 ],\n",
       "       [  66.3013  ,   38.90863 ],\n",
       "       [  38.1808  ,   25.15393 ],\n",
       "       [  77.8025  ,   27.52133 ],\n",
       "       [  92.628   ,   29.67583 ],\n",
       "       [  92.885   ,   31.86118 ],\n",
       "       [  81.81255 ,   18.19063 ],\n",
       "       [ 148.986   ,   46.05153 ],\n",
       "       [  83.261   ,   37.74878 ],\n",
       "       [  80.5948  ,   22.71338 ],\n",
       "       [  31.8318  ,   23.78293 ],\n",
       "       [ 111.3245  ,   21.88668 ],\n",
       "       [ 103.239   ,   22.07788 ],\n",
       "       [ 171.2365  ,   55.26523 ],\n",
       "       [ 136.961   ,   26.93518 ],\n",
       "       [  73.8446  ,   26.92423 ],\n",
       "       [  33.4953  ,   52.95548 ],\n",
       "       [  77.7311  ,   27.26073 ],\n",
       "       [ 105.141   ,   21.40898 ],\n",
       "       [  97.868   ,   33.80928 ],\n",
       "       [  96.623   ,   44.91438 ],\n",
       "       [ 111.51    ,   15.04548 ],\n",
       "       [  59.5124  ,   67.85333 ],\n",
       "       [  56.6484  ,   17.29233 ],\n",
       "       [  64.33085 ,   30.72638 ],\n",
       "       [  25.3268  ,   22.67198 ],\n",
       "       [  96.352   ,   55.47383 ],\n",
       "       [  25.3268  ,   22.67198 ],\n",
       "       [ 104.69    ,   42.10408 ],\n",
       "       [  59.4638  ,   24.70708 ],\n",
       "       [  83.90035 ,   41.45273 ],\n",
       "       [  48.8514  ,   23.80433 ],\n",
       "       [  23.13415 ,   21.81743 ],\n",
       "       [  94.4414  ,   35.95873 ],\n",
       "       [  63.9793  ,   28.67143 ],\n",
       "       [  66.4     ,   39.79603 ],\n",
       "       [  95.873   ,   56.07508 ],\n",
       "       [  99.002   ,   38.67548 ],\n",
       "       [  17.9082  ,   21.06398 ],\n",
       "       [ 127.1265  ,   33.64188 ],\n",
       "       [  98.455   ,   48.98038 ],\n",
       "       [  62.05605 ,   22.03193 ],\n",
       "       [  51.56295 ,   16.10623 ],\n",
       "       [  82.4658  ,   20.58298 ],\n",
       "       [ 112.9515  ,   34.27363 ],\n",
       "       [  93.825   ,   55.66258 ],\n",
       "       [  64.32245 ,   21.06558 ],\n",
       "       [ 117.155   ,   45.70738 ],\n",
       "       [  95.7488  ,   53.14778 ],\n",
       "       [ 113.2775  ,   31.91173 ],\n",
       "       [  75.1012  ,   40.64918 ],\n",
       "       [  98.671   ,   29.16458 ],\n",
       "       [ 105.911   ,   17.42573 ],\n",
       "       [  63.4082  ,   57.69708 ],\n",
       "       [  28.99925 ,   22.60738 ],\n",
       "       [ 105.684   ,   18.39478 ],\n",
       "       [  65.42295 ,   25.84623 ],\n",
       "       [  81.8362  ,   18.96468 ],\n",
       "       [  43.91705 ,   24.43593 ],\n",
       "       [  86.1076  ,   20.43898 ],\n",
       "       [  41.753   ,   26.70818 ],\n",
       "       [ 130.543   ,   40.16788 ],\n",
       "       [ 114.038   ,   36.12573 ],\n",
       "       [  78.45655 ,   32.68833 ],\n",
       "       [  54.62155 ,   16.68968 ],\n",
       "       [  69.76985 ,   35.20798 ],\n",
       "       [  89.7997  ,   37.44448 ],\n",
       "       [  80.8542  ,   22.93918 ],\n",
       "       [  86.64665 ,   22.64303 ],\n",
       "       [  96.565   ,   30.30523 ],\n",
       "       [  83.4736  ,   17.93908 ],\n",
       "       [ 111.388   ,   38.03468 ],\n",
       "       [ 111.918   ,   33.71298 ],\n",
       "       [  64.7795  ,   33.02998 ],\n",
       "       [  59.31085 ,   23.99758 ],\n",
       "       [  94.289   ,   25.03413 ],\n",
       "       [  99.701   ,   48.65388 ],\n",
       "       [  62.41535 ,   59.19318 ],\n",
       "       [  62.5286  ,   38.24408 ],\n",
       "       [ 101.295   ,   43.18628 ],\n",
       "       [  93.06505 ,   42.81338 ],\n",
       "       [  22.3881  ,   22.94128 ],\n",
       "       [  33.7244  ,   50.97368 ],\n",
       "       [ 119.44    ,   71.03298 ],\n",
       "       [ 105.5905  ,   33.13653 ],\n",
       "       [  22.6809  ,   21.63213 ],\n",
       "       [ 116.677   ,   38.61593 ],\n",
       "       [  10.70205 ,   49.01038 ],\n",
       "       [  59.1662  ,   66.19978 ],\n",
       "       [ 119.6175  ,   29.58103 ],\n",
       "       [  92.494   ,   32.89208 ],\n",
       "       [  97.7365  ,   38.22018 ],\n",
       "       [  43.62525 ,   24.39553 ],\n",
       "       [ 118.3835  ,   33.73153 ],\n",
       "       [ 110.2455  ,   28.55383 ],\n",
       "       [ 126.6745  ,   21.82698 ],\n",
       "       [ 113.2975  ,   16.42043 ],\n",
       "       [  74.92555 ,   27.35808 ],\n",
       "       [  84.36025 ,   21.99188 ],\n",
       "       [  82.91015 ,   33.59333 ],\n",
       "       [  86.86375 ,   14.74639 ],\n",
       "       [  64.3843  ,   55.53938 ],\n",
       "       [  91.2497  ,   32.01518 ],\n",
       "       [ 144.5115  ,   44.15473 ],\n",
       "       [  53.21995 ,   14.84793 ],\n",
       "       [  67.9542  ,   26.25378 ],\n",
       "       [ 112.2675  ,   17.86063 ],\n",
       "       [ 110.807   ,   33.00238 ],\n",
       "       [ 103.905   ,   21.23128 ],\n",
       "       [  90.3683  ,   68.59468 ],\n",
       "       [  50.96725 ,   20.15513 ],\n",
       "       [  51.56605 ,   14.95048 ],\n",
       "       [  85.91775 ,   19.83553 ],\n",
       "       [  69.17075 ,   25.80438 ],\n",
       "       [  57.11145 ,   25.56808 ],\n",
       "       [  51.81145 ,   14.65778 ],\n",
       "       [ 140.141   ,   26.74803 ],\n",
       "       [  21.2339  ,   22.21688 ],\n",
       "       [ 102.8534  ,   29.45778 ],\n",
       "       [ 124.216   ,   45.18743 ],\n",
       "       [  83.0242  ,   32.74058 ],\n",
       "       [  99.117   ,   44.08128 ],\n",
       "       [  46.20455 ,   17.79895 ],\n",
       "       [  81.437   ,   20.63738 ],\n",
       "       [  72.8377  ,   26.46088 ],\n",
       "       [  91.9245  ,   61.15983 ],\n",
       "       [  59.2982  ,   67.50788 ],\n",
       "       [  90.4316  ,   68.13193 ],\n",
       "       [ 119.2625  ,   42.13023 ],\n",
       "       [ 122.296   ,   29.88188 ],\n",
       "       [ 103.4365  ,   29.20103 ],\n",
       "       [  94.289   ,   25.03413 ],\n",
       "       [  99.1482  ,   29.65538 ],\n",
       "       [  61.4386  ,   22.38018 ],\n",
       "       [  93.1145  ,   57.14578 ],\n",
       "       [  55.9602  ,   17.34358 ],\n",
       "       [  85.5945  ,   18.38433 ],\n",
       "       [ 122.318   ,   29.87048 ],\n",
       "       [  82.6736  ,   20.26928 ],\n",
       "       [  79.49675 ,   32.30193 ],\n",
       "       [  91.5585  ,   66.45903 ],\n",
       "       [  60.2948  ,   18.87308 ],\n",
       "       [  66.907   ,   28.55158 ],\n",
       "       [  83.50605 ,   20.60228 ],\n",
       "       [  72.4497  ,   39.50318 ],\n",
       "       [  61.77735 ,   60.62503 ],\n",
       "       [ 132.351   ,   27.52883 ],\n",
       "       [  51.30545 ,   16.42518 ],\n",
       "       [ 120.923   ,   66.09168 ],\n",
       "       [  86.7462  ,   27.84933 ],\n",
       "       [  90.40345 ,   67.69078 ],\n",
       "       [  39.5354  ,   24.29188 ],\n",
       "       [  58.0907  ,   41.98333 ],\n",
       "       [  73.2407  ,   32.74508 ],\n",
       "       [  81.10355 ,   16.51768 ],\n",
       "       [ 120.263   ,   68.37703 ],\n",
       "       [  92.301   ,   65.01288 ],\n",
       "       [ 100.10025 ,   29.43583 ],\n",
       "       [  95.81445 ,   36.44608 ],\n",
       "       [  86.65475 ,   18.96673 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4764.9415 - val_loss: 4734.2809\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4602.8617 - val_loss: 4452.7651\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4130.4952 - val_loss: 3746.9824\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3156.5258 - val_loss: 2539.7597\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1870.3809 - val_loss: 1395.2769\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1107.0069 - val_loss: 943.0773\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 799.0540 - val_loss: 717.6469\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 613.6683 - val_loss: 577.2179\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 505.1938 - val_loss: 475.1050\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 431.0485 - val_loss: 407.2954\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 379.7606 - val_loss: 363.0769\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 341.2388 - val_loss: 330.8733\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 313.6836 - val_loss: 306.9300\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 304.0631 - val_loss: 289.0259\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 275.9918 - val_loss: 272.1235\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 272.2087 - val_loss: 259.7661\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 277.8930 - val_loss: 250.4127\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 262.8773 - val_loss: 237.8646\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 258.5583 - val_loss: 227.7535\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 240.7308 - val_loss: 216.4869\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 231.8215 - val_loss: 207.5906\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 218.4619 - val_loss: 198.1606\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 219.5331 - val_loss: 189.1511\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 206.4047 - val_loss: 179.2401\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 199.7377 - val_loss: 169.7806\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 190.8515 - val_loss: 164.0912\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 192.2337 - val_loss: 157.9340\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 183.6117 - val_loss: 151.6317\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 174.3906 - val_loss: 146.2277\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 175.8917 - val_loss: 140.5817\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 165.3678 - val_loss: 137.0573\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 162.9185 - val_loss: 132.8331\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 163.1174 - val_loss: 131.1886\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 158.3570 - val_loss: 128.7923\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 157.4943 - val_loss: 125.5765\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 158.8715 - val_loss: 126.5497\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 155.1440 - val_loss: 122.3858\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 151.8994 - val_loss: 121.0761\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 139.9611 - val_loss: 119.0745\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 143.2082 - val_loss: 120.4680\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 154.0260 - val_loss: 116.3193\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 144.0350 - val_loss: 117.2865\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 145.1541 - val_loss: 116.2103\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 142.1589 - val_loss: 112.2263\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 146.3508 - val_loss: 112.9993\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 132.7585 - val_loss: 111.1732\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 133.4581 - val_loss: 110.6631\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 137.2742 - val_loss: 112.5805\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 137.6576 - val_loss: 108.4550\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 134.9062 - val_loss: 108.7883\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 132.4527 - val_loss: 106.9043\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 132.5356 - val_loss: 107.5188\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 125.6318 - val_loss: 104.5966\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 134.7205 - val_loss: 105.7862\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 122.2234 - val_loss: 103.4573\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 131.0660 - val_loss: 105.3706\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 130.0709 - val_loss: 104.9918\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 134.9429 - val_loss: 102.8670\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 125.6889 - val_loss: 102.3078\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 132.3185 - val_loss: 101.2876\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 123.1938 - val_loss: 99.1951\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 123.8677 - val_loss: 100.5518\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 120.3629 - val_loss: 98.8422\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 114.8412 - val_loss: 98.5263\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 122.6558 - val_loss: 97.1265\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 113.9331 - val_loss: 95.9708\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 123.2419 - val_loss: 96.3127\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 123.0522 - val_loss: 97.4322\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 113.0899 - val_loss: 95.9615\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 112.7293 - val_loss: 94.4805\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 111.9468 - val_loss: 94.2758\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 114.7479 - val_loss: 93.8796\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 120.6677 - val_loss: 94.0241\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 116.9513 - val_loss: 93.2442\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 115.2592 - val_loss: 93.4641\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 116.6050 - val_loss: 92.5268\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 111.8188 - val_loss: 91.2138\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 112.8497 - val_loss: 90.6600\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 105.0840 - val_loss: 91.0293\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 112.8674 - val_loss: 90.4864\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 117.5780 - val_loss: 89.8234\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 112.7777 - val_loss: 89.6274\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 115.7195 - val_loss: 89.2871\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 113.4569 - val_loss: 88.4437\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 107.3739 - val_loss: 89.5250\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 108.6804 - val_loss: 88.5985\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 105.8598 - val_loss: 88.7133\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 105.6127 - val_loss: 88.9284\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 104.8442 - val_loss: 86.7888\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 108.9796 - val_loss: 86.7821\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 106.9363 - val_loss: 86.4967\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 108.6865 - val_loss: 86.9302\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 109.5251 - val_loss: 85.5545\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 110.6464 - val_loss: 85.4082\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 107.0681 - val_loss: 86.5836\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 106.0776 - val_loss: 85.7091\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 106.6646 - val_loss: 85.0657\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 104.1791 - val_loss: 89.1913\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 110.6429 - val_loss: 84.9019\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 105.6234 - val_loss: 85.2870\n",
      "Fold 1\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4694.1876 - val_loss: 4652.7213\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4524.9481 - val_loss: 4359.9836\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4050.7712 - val_loss: 3635.1093\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3089.1886 - val_loss: 2415.0511\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1861.5511 - val_loss: 1305.0963\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1111.7667 - val_loss: 863.8604\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 783.4172 - val_loss: 653.1946\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 624.1841 - val_loss: 519.7304\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 504.5498 - val_loss: 423.7699\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 435.4283 - val_loss: 365.7221\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 372.9685 - val_loss: 328.4450\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 354.1605 - val_loss: 305.1797\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 324.2270 - val_loss: 285.7404\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 296.2043 - val_loss: 271.8514\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 294.4068 - val_loss: 260.8459\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 281.7336 - val_loss: 251.1916\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 265.2576 - val_loss: 244.2095\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 260.9097 - val_loss: 234.0466\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 247.5439 - val_loss: 224.1731\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 231.2627 - val_loss: 212.3977\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 236.9881 - val_loss: 200.8948\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 222.2228 - val_loss: 189.1959\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 211.0328 - val_loss: 178.1824\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 207.2474 - val_loss: 169.6159\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 197.2927 - val_loss: 161.1925\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 185.2130 - val_loss: 155.6816\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 181.5501 - val_loss: 150.0463\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 166.7676 - val_loss: 146.9771\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 179.1769 - val_loss: 146.0042\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 163.1200 - val_loss: 142.9295\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 163.3899 - val_loss: 142.9207\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 163.6178 - val_loss: 138.7376\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 166.0263 - val_loss: 138.4402\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 152.2128 - val_loss: 134.8287\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 153.4908 - val_loss: 136.7831\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 143.9294 - val_loss: 133.5852\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 155.2331 - val_loss: 132.4177\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 151.0280 - val_loss: 133.0046\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 152.1833 - val_loss: 129.8780\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 143.5237 - val_loss: 128.9225\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 141.4805 - val_loss: 127.7199\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 143.5769 - val_loss: 127.0518\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 141.2823 - val_loss: 125.9180\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 144.2242 - val_loss: 125.8853\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 140.2179 - val_loss: 124.6628\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 145.4417 - val_loss: 125.3387\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 147.4527 - val_loss: 122.2815\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 137.0478 - val_loss: 123.4872\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 133.1337 - val_loss: 120.5004\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 141.2739 - val_loss: 118.7831\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 124.7003 - val_loss: 118.3556\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 129.2979 - val_loss: 119.8968\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 123.0905 - val_loss: 118.1685\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 128.7480 - val_loss: 117.5991\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 131.9628 - val_loss: 117.0463\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 125.7295 - val_loss: 114.6751\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 125.7324 - val_loss: 112.9554\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 122.5350 - val_loss: 111.5167\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 119.8393 - val_loss: 113.2770\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 124.8669 - val_loss: 110.6498\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 127.3824 - val_loss: 110.8013\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 118.6892 - val_loss: 110.5466\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 122.4327 - val_loss: 109.0437\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 122.6849 - val_loss: 107.8220\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 125.6976 - val_loss: 106.5357\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 122.7431 - val_loss: 106.8203\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 118.4548 - val_loss: 107.5707\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 116.5550 - val_loss: 107.2697\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 119.5721 - val_loss: 106.0809\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 113.4876 - val_loss: 105.1986\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 118.8995 - val_loss: 105.2796\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 117.1571 - val_loss: 108.5537\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 120.6781 - val_loss: 104.4485\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 123.1865 - val_loss: 104.4096\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 110.2422 - val_loss: 103.0423\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 116.6082 - val_loss: 102.3629\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 115.4474 - val_loss: 101.7348\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 111.6333 - val_loss: 100.6715\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 113.3594 - val_loss: 100.6666\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 106.5175 - val_loss: 100.7377\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 109.7412 - val_loss: 100.3847\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 107.9097 - val_loss: 99.9703\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 111.8140 - val_loss: 100.4659\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 106.6360 - val_loss: 98.3146\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 111.3258 - val_loss: 98.2571\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 112.5342 - val_loss: 96.3905\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 105.4178 - val_loss: 99.3430\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 103.8172 - val_loss: 97.1481\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 108.5401 - val_loss: 99.2587\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 101.1084 - val_loss: 97.3049\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 103.7164 - val_loss: 98.7249\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 105.3411 - val_loss: 97.0595\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 104.9147 - val_loss: 96.0181\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 99.6221 - val_loss: 97.2722\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 106.7128 - val_loss: 97.6652\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 106.0706 - val_loss: 97.1546\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 103.9632 - val_loss: 94.4253\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 104.6602 - val_loss: 97.8792\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 101.8184 - val_loss: 94.0535\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 105.3151 - val_loss: 95.2857\n",
      "Fold 2\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4649.6202 - val_loss: 4394.4273\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4479.8071 - val_loss: 4093.4016\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4001.1422 - val_loss: 3366.5430\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3047.8677 - val_loss: 2181.0832\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1845.2047 - val_loss: 1190.2431\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1124.2197 - val_loss: 814.8246\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 814.1218 - val_loss: 596.8593\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 634.3489 - val_loss: 463.1058\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 490.6241 - val_loss: 375.0802\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 429.3999 - val_loss: 322.6471\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 377.6253 - val_loss: 289.2316\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 361.6163 - val_loss: 266.1151\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 331.5071 - val_loss: 249.9053\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 300.0406 - val_loss: 236.7834\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 292.9433 - val_loss: 227.0044\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 280.1227 - val_loss: 219.3938\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 265.0838 - val_loss: 212.3573\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 258.0632 - val_loss: 204.5579\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 249.1349 - val_loss: 196.5446\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 245.5606 - val_loss: 188.4128\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 237.8999 - val_loss: 180.3356\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 211.9150 - val_loss: 172.8979\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 207.9348 - val_loss: 162.1474\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 202.5283 - val_loss: 154.5564\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 200.2955 - val_loss: 145.6971\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 185.3862 - val_loss: 139.1932\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 181.9202 - val_loss: 133.5511\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 174.4856 - val_loss: 130.0965\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 175.6301 - val_loss: 125.9937\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 168.7912 - val_loss: 124.3839\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 164.8706 - val_loss: 121.3955\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 158.0459 - val_loss: 119.9173\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 157.6293 - val_loss: 117.6623\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 152.7683 - val_loss: 116.0138\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 152.7001 - val_loss: 114.4547\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 152.5741 - val_loss: 112.6837\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 146.2739 - val_loss: 112.1141\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 143.7339 - val_loss: 109.2437\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 147.9110 - val_loss: 108.8689\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 141.0084 - val_loss: 107.5214\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 140.6285 - val_loss: 105.8575\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 143.4967 - val_loss: 105.7806\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 145.3548 - val_loss: 105.3755\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 139.4668 - val_loss: 103.2082\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 137.1271 - val_loss: 102.6733\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 140.7486 - val_loss: 103.1092\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 127.7082 - val_loss: 100.6196\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 134.0066 - val_loss: 100.7636\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 135.8922 - val_loss: 98.8192\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 132.5709 - val_loss: 98.0143\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 130.8385 - val_loss: 97.1030\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 133.2049 - val_loss: 95.7882\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 127.0397 - val_loss: 94.3115\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 125.0222 - val_loss: 94.3000\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 128.4769 - val_loss: 94.7719\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 130.6773 - val_loss: 94.1442\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 130.8733 - val_loss: 92.7600\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 125.1742 - val_loss: 91.8861\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 124.5953 - val_loss: 91.8721\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 119.4474 - val_loss: 90.9294\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 120.1203 - val_loss: 89.7741\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 126.2983 - val_loss: 91.4841\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 127.0985 - val_loss: 90.3228\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 119.2654 - val_loss: 89.9115\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 118.7137 - val_loss: 88.8340\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 120.9219 - val_loss: 87.9894\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 113.7473 - val_loss: 88.9253\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 116.9103 - val_loss: 87.1737\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 111.8953 - val_loss: 86.6384\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 111.9445 - val_loss: 86.8126\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 114.6901 - val_loss: 85.4307\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 116.3671 - val_loss: 86.0219\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 118.6618 - val_loss: 86.2627\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 113.7403 - val_loss: 85.0379\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 110.7106 - val_loss: 84.6528\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 112.0692 - val_loss: 83.1066\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 111.3734 - val_loss: 83.9844\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 115.9286 - val_loss: 84.1553\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 108.2694 - val_loss: 82.7111\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 107.7136 - val_loss: 84.3525\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 107.1361 - val_loss: 83.1836\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 111.8101 - val_loss: 84.4267\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 109.4081 - val_loss: 80.2016\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 108.3086 - val_loss: 80.5428\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 107.6807 - val_loss: 79.9769\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 102.6276 - val_loss: 81.0674\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 107.3982 - val_loss: 79.5872\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 105.8551 - val_loss: 78.6926\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 104.5617 - val_loss: 79.6866\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 108.5545 - val_loss: 77.9295\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 100.3564 - val_loss: 77.6190\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 102.4578 - val_loss: 77.6032\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 105.6774 - val_loss: 78.1605\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 101.2518 - val_loss: 77.1921\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 103.3361 - val_loss: 76.6272\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 103.3465 - val_loss: 77.2320\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 99.3005 - val_loss: 76.2782\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 105.6657 - val_loss: 79.6350\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 104.1639 - val_loss: 75.8707\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 95.4160 - val_loss: 76.6441\n",
      "Fold 3\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4601.5869 - val_loss: 4715.8496\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4436.9309 - val_loss: 4425.7300\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3989.5146 - val_loss: 3719.6332\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3063.3743 - val_loss: 2516.9726\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1856.6242 - val_loss: 1368.5692\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1085.8951 - val_loss: 927.3659\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 813.3828 - val_loss: 716.9061\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 638.2276 - val_loss: 588.9494\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 513.9178 - val_loss: 503.8013\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 445.8079 - val_loss: 441.7475\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 419.0312 - val_loss: 397.8804\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 361.4038 - val_loss: 363.6633\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 329.6758 - val_loss: 337.4553\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 320.6787 - val_loss: 317.0787\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 307.1281 - val_loss: 296.8287\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 281.2733 - val_loss: 276.9849\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 276.9574 - val_loss: 264.8928\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 263.8295 - val_loss: 250.0074\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 266.5832 - val_loss: 239.3209\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 241.1953 - val_loss: 226.0816\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 243.8556 - val_loss: 212.5614\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 219.3033 - val_loss: 201.3586\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 217.1930 - val_loss: 189.3181\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 209.1647 - val_loss: 176.6429\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 199.9206 - val_loss: 171.1261\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 192.4494 - val_loss: 157.8191\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 180.5271 - val_loss: 151.1032\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 172.6698 - val_loss: 146.2512\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 178.4582 - val_loss: 142.8761\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 173.8232 - val_loss: 136.6748\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 169.3909 - val_loss: 135.4366\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 161.9329 - val_loss: 130.4861\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 156.5315 - val_loss: 128.4654\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 165.4023 - val_loss: 129.0177\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 153.7732 - val_loss: 125.5472\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 160.0634 - val_loss: 126.0146\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 159.0173 - val_loss: 121.3375\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 155.0784 - val_loss: 121.4790\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 149.7880 - val_loss: 117.5231\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 149.5087 - val_loss: 117.8349\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 142.0610 - val_loss: 115.4698\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 139.0250 - val_loss: 115.5494\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 145.0947 - val_loss: 112.9812\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 139.6791 - val_loss: 114.3214\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 138.4483 - val_loss: 111.9724\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 146.4737 - val_loss: 114.0426\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 143.1299 - val_loss: 109.8273\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 129.3287 - val_loss: 115.2684\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 137.7984 - val_loss: 107.5099\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 134.6884 - val_loss: 109.9735\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 134.1222 - val_loss: 106.0981\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 128.9702 - val_loss: 108.1768\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 130.1252 - val_loss: 105.6868\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 127.8652 - val_loss: 103.8479\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 131.4425 - val_loss: 102.7339\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 120.2610 - val_loss: 102.9044\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 123.6103 - val_loss: 100.8699\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 123.0899 - val_loss: 100.7965\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 123.7918 - val_loss: 101.1606\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 119.2050 - val_loss: 99.1480\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 117.2131 - val_loss: 99.2306\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 119.1774 - val_loss: 99.9766\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 123.4081 - val_loss: 98.8183\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 116.2369 - val_loss: 97.8425\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 115.1242 - val_loss: 99.0577\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 119.3199 - val_loss: 98.5364\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 115.1185 - val_loss: 96.3466\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 116.3008 - val_loss: 101.6042\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 119.4068 - val_loss: 94.5859\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 111.8135 - val_loss: 95.3675\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.8470 - val_loss: 93.0609\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 112.0216 - val_loss: 95.1547\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 114.9391 - val_loss: 96.7501\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 120.3380 - val_loss: 95.4316\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 117.1345 - val_loss: 92.9364\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 117.4182 - val_loss: 99.8917\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 109.7505 - val_loss: 90.6474\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 108.6763 - val_loss: 91.8849\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 109.7451 - val_loss: 91.6991\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 104.1884 - val_loss: 90.0463\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 109.4697 - val_loss: 91.6989\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 113.0445 - val_loss: 90.5008\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 116.5277 - val_loss: 88.0594\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 111.0317 - val_loss: 88.2703\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 102.7450 - val_loss: 89.9208\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 110.4339 - val_loss: 89.9190\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 107.5576 - val_loss: 86.6092\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 108.2905 - val_loss: 94.0566\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 108.9338 - val_loss: 83.8355\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 109.0243 - val_loss: 86.6324\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 112.4438 - val_loss: 91.9555\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 106.7024 - val_loss: 86.6103\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 101.0890 - val_loss: 91.8446\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 106.1796 - val_loss: 85.8915\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 101.2687 - val_loss: 87.4491\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 102.8942 - val_loss: 88.3228\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 98.4197 - val_loss: 87.4849\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 101.0013 - val_loss: 84.3679\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 107.4917 - val_loss: 83.6125\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 99.1618 - val_loss: 86.1318\n",
      "Fold 4\n",
      "Train on 809 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 1s - loss: 4686.4994 - val_loss: 4402.1486\n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 4509.7202 - val_loss: 4102.1387\n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 4022.3501 - val_loss: 3377.3071\n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 3038.9477 - val_loss: 2195.5042\n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 1797.5487 - val_loss: 1175.5440\n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 1053.0728 - val_loss: 833.0363\n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 809.1074 - val_loss: 610.8606\n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 647.4276 - val_loss: 480.7353\n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 519.2558 - val_loss: 393.8466\n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 452.6934 - val_loss: 339.1328\n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 394.9793 - val_loss: 302.5993\n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 358.5127 - val_loss: 278.4853\n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 338.2549 - val_loss: 261.1796\n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 317.8662 - val_loss: 247.7248\n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 299.6603 - val_loss: 237.9637\n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 283.2157 - val_loss: 229.3737\n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 279.2672 - val_loss: 218.7438\n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 272.4669 - val_loss: 210.4667\n",
      "Epoch 19/100\n",
      "809/809 [==============================] - 0s - loss: 257.9353 - val_loss: 202.3642\n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 238.0774 - val_loss: 192.3689\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809/809 [==============================] - 0s - loss: 237.2151 - val_loss: 183.2243\n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 233.2119 - val_loss: 171.7347\n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 214.0835 - val_loss: 162.6221\n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 205.0510 - val_loss: 155.1267\n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 197.3416 - val_loss: 146.4802\n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 192.8422 - val_loss: 139.5160\n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 191.1672 - val_loss: 134.4103\n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 183.0720 - val_loss: 129.8396\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 164.1953 - val_loss: 126.7311\n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 178.2335 - val_loss: 124.6085\n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 163.8053 - val_loss: 122.1371\n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 169.8462 - val_loss: 120.2206\n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 161.0609 - val_loss: 118.9397\n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 157.0510 - val_loss: 118.0806\n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 159.2139 - val_loss: 116.4717\n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 158.4524 - val_loss: 115.3695\n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 149.4158 - val_loss: 114.1105\n",
      "Epoch 38/100\n",
      "809/809 [==============================] - 0s - loss: 153.6712 - val_loss: 112.0765\n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 152.0206 - val_loss: 110.9364\n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 147.9235 - val_loss: 110.1182: 141.9\n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 145.6062 - val_loss: 110.5058\n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 154.8200 - val_loss: 108.6357\n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 139.7285 - val_loss: 108.3145\n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 145.2773 - val_loss: 106.5713\n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 139.2483 - val_loss: 105.5781\n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 136.4293 - val_loss: 104.4873\n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 140.0673 - val_loss: 103.8959\n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 140.6798 - val_loss: 103.4574\n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 139.2136 - val_loss: 102.9530\n",
      "Epoch 50/100\n",
      "809/809 [==============================] - ETA: 0s - loss: 137.539 - 0s - loss: 138.7725 - val_loss: 103.9745\n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 136.1127 - val_loss: 101.0049\n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 135.5666 - val_loss: 100.9505\n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 136.5563 - val_loss: 100.9100\n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 138.4856 - val_loss: 100.0296\n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 132.6105 - val_loss: 98.2242\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 124.0713 - val_loss: 98.0594\n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 131.2130 - val_loss: 96.4295\n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 123.7254 - val_loss: 96.1416\n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 125.0191 - val_loss: 96.6320\n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 124.4788 - val_loss: 95.5614\n",
      "Epoch 61/100\n",
      "809/809 [==============================] - 0s - loss: 122.5688 - val_loss: 95.1245\n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 120.7880 - val_loss: 94.3060\n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 124.2444 - val_loss: 94.1734\n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 128.4298 - val_loss: 93.8176\n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 126.9855 - val_loss: 94.6861\n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 123.2195 - val_loss: 93.6598\n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 118.5971 - val_loss: 91.4302\n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 120.1955 - val_loss: 90.7411\n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 118.0702 - val_loss: 91.2742\n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 123.0152 - val_loss: 92.2898\n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 116.7730 - val_loss: 92.0880\n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 120.2326 - val_loss: 91.7077\n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 117.9200 - val_loss: 88.9425\n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 113.6569 - val_loss: 88.8054\n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 120.1777 - val_loss: 87.4497\n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 115.7926 - val_loss: 86.8505\n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 112.8331 - val_loss: 86.6154\n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 113.0581 - val_loss: 87.0911\n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 111.7928 - val_loss: 87.8389\n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 108.7612 - val_loss: 87.9607\n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 112.6122 - val_loss: 87.7315\n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 104.5334 - val_loss: 86.3147\n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 110.3624 - val_loss: 86.1877\n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 109.5660 - val_loss: 85.6822\n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 108.7485 - val_loss: 85.1996\n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 110.2709 - val_loss: 84.2541\n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 107.9517 - val_loss: 84.8072\n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 106.2669 - val_loss: 85.5160\n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 111.2378 - val_loss: 84.6961\n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 110.9902 - val_loss: 84.1382\n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 106.1006 - val_loss: 83.9615\n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 108.1495 - val_loss: 82.5070\n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 107.3302 - val_loss: 80.8219\n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 104.8548 - val_loss: 80.9793\n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 108.7390 - val_loss: 81.5737\n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 105.5238 - val_loss: 80.3374\n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 101.3731 - val_loss: 80.8731\n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 108.3741 - val_loss: 80.1954\n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 101.8370 - val_loss: 80.8153\n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 101.0769 - val_loss: 80.3343\n",
      "el_mean:  9.91815720185\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=992, activation=\"relu\", use_bias=True)`\n",
      "  model.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"relu\", use_bias=True)`\n",
      "  model.add(Dense(256, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=256, activation=\"relu\", use_bias=True)`\n",
      "  e.add(Dense(512, input_dim=256, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(992, activation=\"relu\", use_bias=True)`\n",
      "  e.add(Dense(input_size, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 0.0010     \n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 2s - loss: 4708.5773 - val_loss: 4516.2845\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 3059.2120 - val_loss: 1133.7566\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 974.6416 - val_loss: 579.4251\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 474.9449 - val_loss: 272.2552\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 337.9427 - val_loss: 222.7174\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 316.9792 - val_loss: 205.4299\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 285.6034 - val_loss: 205.4097\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 280.6260 - val_loss: 191.1677\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 264.2371 - val_loss: 181.8954\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 267.0028 - val_loss: 179.7673\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 234.1486 - val_loss: 175.2455\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 241.9888 - val_loss: 177.5499\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 252.2925 - val_loss: 176.5213\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 230.7924 - val_loss: 174.4730\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 218.5506 - val_loss: 168.1458\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 223.1668 - val_loss: 166.1360\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 218.1405 - val_loss: 159.8484\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 197.8085 - val_loss: 149.6928\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 199.7135 - val_loss: 140.5875\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 184.1608 - val_loss: 111.1102\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 165.0767 - val_loss: 82.2663\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 138.1304 - val_loss: 76.9675\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 138.6957 - val_loss: 96.8944\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 136.7146 - val_loss: 77.0778\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 118.5060 - val_loss: 73.4797\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 119.5557 - val_loss: 65.0358\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 121.0266 - val_loss: 76.8010\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 127.2422 - val_loss: 72.1664\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 122.8642 - val_loss: 71.0047\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 128.9598 - val_loss: 80.5650\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 131.2834 - val_loss: 75.5902\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 120.5969 - val_loss: 67.1148\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 106.3076 - val_loss: 65.1241\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 118.9691 - val_loss: 72.9930\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 113.6987 - val_loss: 64.0158\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 115.9968 - val_loss: 65.2645\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 107.2134 - val_loss: 84.7484\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 108.3141 - val_loss: 59.7081\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 113.4770 - val_loss: 61.2436\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 107.8088 - val_loss: 89.1322\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 113.8872 - val_loss: 62.9678\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 101.8599 - val_loss: 68.2372\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 113.1498 - val_loss: 61.9318\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 106.6266 - val_loss: 60.4080\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 105.6484 - val_loss: 62.5854\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 101.4451 - val_loss: 70.0847\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 103.0347 - val_loss: 56.1527\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 104.9421 - val_loss: 57.1322\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 102.5375 - val_loss: 65.1141\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 100.1542 - val_loss: 65.8612\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 105.7524 - val_loss: 62.3051\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 93.7628 - val_loss: 65.5265\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 96.8761 - val_loss: 66.3217\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 100.7837 - val_loss: 69.0437\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 109.3840 - val_loss: 71.1961\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 98.0128 - val_loss: 62.2732\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 103.2098 - val_loss: 66.8182\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 89.5272 - val_loss: 65.2764\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 87.6817 - val_loss: 56.7708\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 93.9889 - val_loss: 75.8502\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 91.3583 - val_loss: 58.5116\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 86.5708 - val_loss: 61.0437\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 93.6742 - val_loss: 59.0664\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 89.2498 - val_loss: 65.8165\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 92.9876 - val_loss: 61.1129\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 96.5387 - val_loss: 60.7026\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 92.8507 - val_loss: 59.0641\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 93.4357 - val_loss: 58.9373\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 90.8322 - val_loss: 65.6083\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 88.5307 - val_loss: 57.8773\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 93.7680 - val_loss: 60.9451\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 86.2734 - val_loss: 59.2820\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 90.6424 - val_loss: 57.2852\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 88.8630 - val_loss: 81.1007\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 90.3657 - val_loss: 60.1418\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 88.4097 - val_loss: 67.1169\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 92.4970 - val_loss: 59.1382\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 90.6916 - val_loss: 62.4069\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 88.1859 - val_loss: 71.4635\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 93.3392 - val_loss: 61.9652\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 99.0712 - val_loss: 60.3999\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 86.0211 - val_loss: 52.0383\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 92.4261 - val_loss: 64.4502\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 94.1486 - val_loss: 53.2668\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 88.9131 - val_loss: 58.4175\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 88.7350 - val_loss: 72.3835\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 92.1939 - val_loss: 55.2018\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 88.3830 - val_loss: 55.7394\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 88.5731 - val_loss: 62.8026\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 85.0620 - val_loss: 71.2535\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 89.9481 - val_loss: 60.8539\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 94.6703 - val_loss: 81.8972\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 86.0246 - val_loss: 62.9443\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 84.5994 - val_loss: 83.2607\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 90.8419 - val_loss: 59.5840\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 86.1689 - val_loss: 65.0368\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 82.0417 - val_loss: 66.9774\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 86.3261 - val_loss: 59.1179\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 85.8681 - val_loss: 67.6540\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 92.2012 - val_loss: 62.9784\n",
      "Fold 1\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 0.0010     \n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 2s - loss: 4666.8636 - val_loss: 4141.6456\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 2697.4299 - val_loss: 1337.8843\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 921.0164 - val_loss: 498.1486\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 458.7713 - val_loss: 249.8179\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 348.1177 - val_loss: 203.5908\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 312.7283 - val_loss: 191.8607\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 295.6663 - val_loss: 188.4012\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 283.3871 - val_loss: 180.7576\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 267.5412 - val_loss: 166.7669\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 272.0960 - val_loss: 160.9648\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 235.3155 - val_loss: 157.6096\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 242.2033 - val_loss: 164.4890\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 231.6843 - val_loss: 164.1610\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 232.5230 - val_loss: 178.6373\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 243.1639 - val_loss: 169.7822\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 216.6580 - val_loss: 137.5363\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 193.9568 - val_loss: 110.8809\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 166.3589 - val_loss: 83.1351\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 161.2656 - val_loss: 72.3403\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 139.0367 - val_loss: 90.6339\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 151.7520 - val_loss: 78.9928\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 131.0697 - val_loss: 72.8533\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 141.0915 - val_loss: 63.4408\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 127.8028 - val_loss: 63.6554\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 123.7409 - val_loss: 62.0736\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 135.2687 - val_loss: 56.7181\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 128.7300 - val_loss: 58.3190\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 116.9913 - val_loss: 54.4423\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 110.2374 - val_loss: 51.4207\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 113.1772 - val_loss: 53.9560\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 108.8224 - val_loss: 55.5035\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 109.2729 - val_loss: 51.7109\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 113.8606 - val_loss: 62.1250\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 110.9751 - val_loss: 66.0207\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 109.1990 - val_loss: 52.0287\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 105.4603 - val_loss: 50.7751\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 99.2256 - val_loss: 49.4246\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 98.4964 - val_loss: 56.1888\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 98.5029 - val_loss: 52.2558ss: 97\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 104.8047 - val_loss: 49.1879\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 97.0137 - val_loss: 53.5633\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 106.3996 - val_loss: 53.7579\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 97.9870 - val_loss: 53.3218\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 99.6624 - val_loss: 71.9904\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 108.3990 - val_loss: 48.7423\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 103.4043 - val_loss: 49.7200\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 102.5549 - val_loss: 66.2618\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 93.4163 - val_loss: 49.7372\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 106.8567 - val_loss: 62.7191\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 98.7681 - val_loss: 54.8035\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 98.6000 - val_loss: 48.2358\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 92.6480 - val_loss: 46.3779\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 94.0726 - val_loss: 44.3813\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 92.5746 - val_loss: 59.4119\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 95.9411 - val_loss: 47.9608\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 94.8250 - val_loss: 54.6054\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 92.5343 - val_loss: 53.5694\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 90.9363 - val_loss: 59.2922\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 97.3516 - val_loss: 46.7733\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 93.5763 - val_loss: 45.2304\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 92.3370 - val_loss: 55.9263\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 94.2043 - val_loss: 46.3101\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 100.4902 - val_loss: 48.9742\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 92.2527 - val_loss: 53.1850\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 87.8946 - val_loss: 53.4982\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 82.5981 - val_loss: 53.1358\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 88.0142 - val_loss: 48.9370\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 94.4296 - val_loss: 89.6002\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 105.9709 - val_loss: 49.6691\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 102.8314 - val_loss: 59.1723\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 94.5399 - val_loss: 51.6129\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 91.3436 - val_loss: 42.7367\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 85.5473 - val_loss: 49.9190\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 86.2206 - val_loss: 44.9278\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 99.3123 - val_loss: 46.8868\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 98.8301 - val_loss: 47.2451\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 88.2320 - val_loss: 48.3888\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 86.8553 - val_loss: 58.6579\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 94.5467 - val_loss: 44.8906\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 87.8335 - val_loss: 48.4916\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 93.5502 - val_loss: 45.8255\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 92.0353 - val_loss: 46.6909\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 79.6489 - val_loss: 49.8244\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 85.6699 - val_loss: 65.0770\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 96.8678 - val_loss: 48.6486\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 91.8987 - val_loss: 47.7762\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 85.7091 - val_loss: 53.6953\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 89.7693 - val_loss: 53.8172\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 82.7286 - val_loss: 48.4519\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 90.5677 - val_loss: 52.8888\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 85.6794 - val_loss: 47.7904\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 83.3264 - val_loss: 49.1061\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 78.9426 - val_loss: 44.0105\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 81.7993 - val_loss: 43.9943\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 91.5394 - val_loss: 50.8076\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 82.2433 - val_loss: 47.2765\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 87.2607 - val_loss: 44.7405\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 83.3681 - val_loss: 46.1743\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 77.8606 - val_loss: 42.8262\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 80.7149 - val_loss: 45.6873\n",
      "Fold 2\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 0.0010     \n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 2s - loss: 4564.7665 - val_loss: 4056.9368\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 2638.8881 - val_loss: 1333.3119\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 759.7947 - val_loss: 361.5183\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 417.8740 - val_loss: 272.2437\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 330.1876 - val_loss: 201.1595\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 316.3272 - val_loss: 198.3226\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 282.5508 - val_loss: 186.7085\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 279.7905 - val_loss: 175.9468\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 246.9353 - val_loss: 168.7018\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 229.3155 - val_loss: 164.7711\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 248.9137 - val_loss: 162.2296\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 231.1754 - val_loss: 158.4577\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 233.7527 - val_loss: 164.8662\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 214.7860 - val_loss: 142.0477\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 194.8723 - val_loss: 133.1564\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 185.2665 - val_loss: 105.4163\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 164.0325 - val_loss: 79.4066\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 156.0382 - val_loss: 68.2783\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 131.3796 - val_loss: 68.7656\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 129.0643 - val_loss: 62.2700\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 127.2512 - val_loss: 68.3904\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 123.4115 - val_loss: 60.0994\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 120.8751 - val_loss: 53.3711\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 124.1175 - val_loss: 52.6086\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 111.2795 - val_loss: 51.6110\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 119.2998 - val_loss: 59.2545\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 114.0745 - val_loss: 64.3002\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 116.8973 - val_loss: 52.9910\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 113.3041 - val_loss: 48.8875\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 110.4448 - val_loss: 46.6854\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 108.8601 - val_loss: 47.2219\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 98.9946 - val_loss: 52.9790\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 105.0982 - val_loss: 48.0244\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 109.5683 - val_loss: 46.9554\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 108.9270 - val_loss: 48.1771\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 104.8285 - val_loss: 49.4373\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 104.1949 - val_loss: 45.6348\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 99.3032 - val_loss: 51.0728\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 96.9744 - val_loss: 62.0477\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 103.9602 - val_loss: 44.1794\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 98.9237 - val_loss: 42.9511\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 99.5693 - val_loss: 41.1900\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 97.9937 - val_loss: 42.4006\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 98.7686 - val_loss: 53.6811\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 108.2601 - val_loss: 39.9822\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 95.3385 - val_loss: 44.9964\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 98.8719 - val_loss: 42.5898\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 105.7381 - val_loss: 56.2718\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 95.3992 - val_loss: 61.1056\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 99.8165 - val_loss: 49.1051\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 89.3431 - val_loss: 47.7183\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 94.2351 - val_loss: 44.8618\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 89.0189 - val_loss: 46.8653\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 90.1360 - val_loss: 40.1085\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 95.6928 - val_loss: 40.7306\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 89.6561 - val_loss: 47.9697\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 93.2406 - val_loss: 53.5583\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 88.2044 - val_loss: 45.9650\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 90.5709 - val_loss: 49.9102\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 89.9603 - val_loss: 44.3910\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 103.4409 - val_loss: 63.1185\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 101.5312 - val_loss: 55.6761\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 98.4215 - val_loss: 41.1801\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 88.2772 - val_loss: 39.8493\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 99.4047 - val_loss: 41.2486\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 88.2434 - val_loss: 43.9499\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 88.8809 - val_loss: 40.4480\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 79.7110 - val_loss: 41.6000\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 83.5237 - val_loss: 38.9385\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 81.6415 - val_loss: 50.3171\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 81.4150 - val_loss: 43.8745\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 91.5443 - val_loss: 46.9102\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 91.6763 - val_loss: 66.1693\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 94.0999 - val_loss: 45.8677\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 88.5781 - val_loss: 39.6012\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 85.7217 - val_loss: 49.4396\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 83.8868 - val_loss: 42.1681\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 83.9929 - val_loss: 39.5897\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 88.7896 - val_loss: 42.7658\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 92.6916 - val_loss: 60.7796\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 86.0396 - val_loss: 47.6219\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 87.0906 - val_loss: 49.2980\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 84.0647 - val_loss: 41.2982\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 81.5455 - val_loss: 36.8143\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 80.4729 - val_loss: 35.9773\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 81.3771 - val_loss: 44.5561\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 75.2994 - val_loss: 38.5452\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 81.4762 - val_loss: 38.4685\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 84.6584 - val_loss: 42.9084\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 90.6060 - val_loss: 37.5721\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 87.7330 - val_loss: 51.1543\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 85.0681 - val_loss: 38.2800\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 80.6415 - val_loss: 40.0388\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 85.0431 - val_loss: 44.4178\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 87.1824 - val_loss: 40.1992\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 79.2661 - val_loss: 44.2315\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 77.3441 - val_loss: 42.9432\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 85.4710 - val_loss: 38.8865\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 87.2697 - val_loss: 40.6914\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 90.1187 - val_loss: 50.2012\n",
      "Fold 3\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 0.0010     \n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     - ETA: 0s - loss: 0.00\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 0.0010     \n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 2s - loss: 4488.6859 - val_loss: 4566.7757\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 2688.0559 - val_loss: 1223.5077\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 879.5371 - val_loss: 552.4673\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 460.7734 - val_loss: 256.0237\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 323.4324 - val_loss: 206.9727\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 299.5075 - val_loss: 203.6219\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 274.3671 - val_loss: 187.8645\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 254.6691 - val_loss: 185.6844\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 253.1433 - val_loss: 175.6028\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 236.2354 - val_loss: 171.4680\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 224.8229 - val_loss: 173.9474\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 226.3918 - val_loss: 161.8538\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 219.2001 - val_loss: 165.0632\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 195.0404 - val_loss: 153.4726\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 188.2662 - val_loss: 125.7953\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 155.8427 - val_loss: 96.8092\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 147.0389 - val_loss: 88.8518\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 136.9068 - val_loss: 75.7890\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 127.0749 - val_loss: 69.0590\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 131.6508 - val_loss: 77.1345\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 116.6961 - val_loss: 75.3260\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 122.1066 - val_loss: 75.7388\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 117.0342 - val_loss: 64.5656\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 112.6735 - val_loss: 66.3418\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 111.5221 - val_loss: 64.6296\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 122.0526 - val_loss: 78.0352\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 106.1944 - val_loss: 72.5632\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 102.4550 - val_loss: 64.8866\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 102.8017 - val_loss: 62.0452\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 108.1456 - val_loss: 76.5681\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 109.9373 - val_loss: 65.0002\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 100.6327 - val_loss: 55.7265\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 104.0508 - val_loss: 54.8989\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 105.9464 - val_loss: 111.9375\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 111.6630 - val_loss: 83.6330\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 115.4560 - val_loss: 61.9332\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 100.9565 - val_loss: 80.8855\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 94.8424 - val_loss: 64.9780\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 93.9662 - val_loss: 57.9187\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 98.1043 - val_loss: 56.9290\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 98.2725 - val_loss: 70.6280\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 106.3345 - val_loss: 69.4615\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 90.1848 - val_loss: 64.5810\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 95.7062 - val_loss: 57.9938\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 100.7883 - val_loss: 74.7056\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 100.9840 - val_loss: 88.9766\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 101.5420 - val_loss: 56.8478\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 92.4755 - val_loss: 60.3825\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 91.0645 - val_loss: 56.0321\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 80.8850 - val_loss: 52.9299\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 92.4444 - val_loss: 56.6380\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 91.1492 - val_loss: 62.9663\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 89.8743 - val_loss: 56.5778\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 94.3306 - val_loss: 54.1101\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 95.1451 - val_loss: 87.7168\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 90.4871 - val_loss: 67.2485\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 90.0853 - val_loss: 51.5654\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 92.6459 - val_loss: 60.6328\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 87.3507 - val_loss: 69.6607\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 81.5649 - val_loss: 56.0901\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 85.1998 - val_loss: 63.9612\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 80.7032 - val_loss: 60.1805\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 87.7114 - val_loss: 60.3824\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 86.6201 - val_loss: 63.1175\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 82.8730 - val_loss: 52.3871\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 80.6664 - val_loss: 66.5434\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 92.1982 - val_loss: 70.0417\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 91.8096 - val_loss: 55.2835\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 85.4526 - val_loss: 70.5245\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 84.3878 - val_loss: 54.8906\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 82.0196 - val_loss: 69.8020\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 84.6556 - val_loss: 78.3738\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 84.4507 - val_loss: 52.3332\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 80.2725 - val_loss: 52.1628\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 95.5686 - val_loss: 58.3751\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 87.8353 - val_loss: 53.4471\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 82.3835 - val_loss: 54.6628\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 78.1424 - val_loss: 52.4322\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 78.6328 - val_loss: 60.8097\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 85.4631 - val_loss: 54.8410\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 84.9798 - val_loss: 56.0001\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 84.2773 - val_loss: 63.0686\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 80.6232 - val_loss: 51.8214\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 92.0996 - val_loss: 61.0001\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 83.9101 - val_loss: 55.3230\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 83.2882 - val_loss: 60.1137\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 76.7192 - val_loss: 59.7333\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 84.5873 - val_loss: 65.9077\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 86.1014 - val_loss: 71.8584\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 79.1228 - val_loss: 52.9833\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 79.0533 - val_loss: 66.0885\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 75.1775 - val_loss: 60.3605\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 80.2876 - val_loss: 51.5735\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 82.2056 - val_loss: 60.9937\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 79.0871 - val_loss: 47.3957\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 78.8146 - val_loss: 67.6577\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 83.0107 - val_loss: 57.1101\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 78.8823 - val_loss: 79.5243\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 83.8952 - val_loss: 55.6651\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 80.1562 - val_loss: 61.8491\n",
      "Fold 4\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 1s - loss: 0.0010     \n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 19/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 21/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     - ETA: 0s - loss: 0.0\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 38/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 50/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     - ETA: 0s - loss: 0.0\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 61/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 0.0010     \n",
      "Train on 809 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 2s - loss: 4644.3654 - val_loss: 3968.2624\n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 2684.9063 - val_loss: 1540.7260\n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 884.5436 - val_loss: 467.7061\n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 435.9827 - val_loss: 273.4783\n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 325.4473 - val_loss: 216.6843\n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 302.5057 - val_loss: 206.1015\n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 290.5185 - val_loss: 188.6791\n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 271.7262 - val_loss: 190.6713\n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 268.2172 - val_loss: 199.9859\n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 261.2030 - val_loss: 181.4940\n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 238.6273 - val_loss: 174.4627\n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 225.6616 - val_loss: 166.3292\n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 229.5397 - val_loss: 166.6943\n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 224.5003 - val_loss: 164.6092\n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 213.4589 - val_loss: 162.5375\n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 221.3390 - val_loss: 171.0383\n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 222.2747 - val_loss: 154.0080\n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 201.5254 - val_loss: 137.6079\n",
      "Epoch 19/100\n",
      "809/809 [==============================] - 0s - loss: 175.6343 - val_loss: 125.4120\n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 164.3926 - val_loss: 81.8843\n",
      "Epoch 21/100\n",
      "809/809 [==============================] - 0s - loss: 147.0441 - val_loss: 74.5278\n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 129.6504 - val_loss: 72.5365\n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 127.8695 - val_loss: 71.0811\n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 123.1158 - val_loss: 68.5861\n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 125.7403 - val_loss: 69.5710\n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 126.3399 - val_loss: 72.9264\n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 113.9662 - val_loss: 65.7354\n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 118.4104 - val_loss: 67.0253\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 120.9354 - val_loss: 77.1883\n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 116.4228 - val_loss: 65.5448\n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 113.3849 - val_loss: 65.7511\n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 112.7968 - val_loss: 74.1803\n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 108.6714 - val_loss: 76.0609\n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 115.8576 - val_loss: 74.5623\n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 112.7456 - val_loss: 60.1652\n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 110.3346 - val_loss: 73.9786\n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 123.2055 - val_loss: 62.3006\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809/809 [==============================] - 0s - loss: 113.1614 - val_loss: 64.8897\n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 107.0754 - val_loss: 60.7190\n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 97.9684 - val_loss: 59.3200\n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 110.3263 - val_loss: 68.3498\n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 102.5303 - val_loss: 70.2424\n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 101.8685 - val_loss: 60.4173\n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 98.6862 - val_loss: 59.4488\n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 99.8898 - val_loss: 59.8021\n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 106.1179 - val_loss: 61.5185\n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 102.8827 - val_loss: 66.6673\n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 98.5991 - val_loss: 62.5921\n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 92.9685 - val_loss: 58.2520\n",
      "Epoch 50/100\n",
      "809/809 [==============================] - 0s - loss: 93.4998 - val_loss: 56.1239\n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 95.4643 - val_loss: 57.2782\n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 93.0919 - val_loss: 58.6185\n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 99.9377 - val_loss: 80.4036\n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 99.1044 - val_loss: 62.0108\n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 88.7283 - val_loss: 57.0579\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 114.0538 - val_loss: 80.8476\n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 101.3697 - val_loss: 62.7466\n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 93.7093 - val_loss: 66.1127\n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 91.3509 - val_loss: 57.1783\n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 92.6920 - val_loss: 56.4881\n",
      "Epoch 61/100\n",
      "809/809 [==============================] - 0s - loss: 88.8719 - val_loss: 60.1121\n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 96.3761 - val_loss: 67.3258\n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 94.7663 - val_loss: 60.4760\n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 92.4919 - val_loss: 60.8681\n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 88.8781 - val_loss: 60.4067\n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 96.6414 - val_loss: 64.4135\n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 92.3671 - val_loss: 57.6188\n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 92.2626 - val_loss: 65.1451\n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 91.1897 - val_loss: 63.7664\n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 99.5911 - val_loss: 57.6024\n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 95.8925 - val_loss: 91.7438\n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 96.2395 - val_loss: 55.7036\n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 89.0797 - val_loss: 60.8605\n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 96.2762 - val_loss: 63.0168\n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 96.1544 - val_loss: 58.4337\n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 96.0403 - val_loss: 66.5011\n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 89.0190 - val_loss: 59.4959\n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 92.1827 - val_loss: 54.7986\n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 91.9226 - val_loss: 57.2584\n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 85.2598 - val_loss: 55.1513\n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 86.2750 - val_loss: 56.2019\n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 80.3826 - val_loss: 58.8594\n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 89.5098 - val_loss: 58.7249\n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 96.8110 - val_loss: 56.4274\n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 87.0658 - val_loss: 57.8058\n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 97.8698 - val_loss: 54.8674\n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 97.0070 - val_loss: 55.0023\n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 85.1532 - val_loss: 51.6206\n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 85.8926 - val_loss: 64.9244\n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 83.5427 - val_loss: 54.5686\n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 94.7938 - val_loss: 58.9688\n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 87.4124 - val_loss: 60.5116\n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 89.6707 - val_loss: 54.1517\n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 86.7866 - val_loss: 53.4066\n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 84.5991 - val_loss: 53.9795\n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 88.5075 - val_loss: 56.4881\n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 90.7983 - val_loss: 59.6941\n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 88.8567 - val_loss: 59.8466\n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 86.0440 - val_loss: 60.8999\n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 86.3696 - val_loss: 53.9402\n",
      "e2_mean:  8.23822020047\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\regular_regression.py:66: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=992, activation=\"relu\", use_bias=True)`\n",
      "  model.add(Dense(128, input_dim=992, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\regular_regression.py:68: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", use_bias=True)`\n",
      "  model.add(Dense(128, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\regular_regression.py:70: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"linear\", use_bias=True)`\n",
      "  model.add(Dense(2, activation='linear', bias=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4736.9292 - acc: 0.8552 - val_loss: 4696.6758 - val_acc: 0.9655\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4586.1268 - acc: 0.9418 - val_loss: 4424.5890 - val_acc: 0.9655\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4141.7962 - acc: 0.9431 - val_loss: 3736.2607 - val_acc: 0.9655\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3216.9630 - acc: 0.9443 - val_loss: 2548.6752 - val_acc: 0.9655\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1958.2895 - acc: 0.9443 - val_loss: 1379.1486 - val_acc: 0.9655\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1135.4064 - acc: 0.9443 - val_loss: 943.6562 - val_acc: 0.9655\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 844.2136 - acc: 0.9443 - val_loss: 705.9240 - val_acc: 0.9655\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 676.6398 - acc: 0.9443 - val_loss: 556.2347 - val_acc: 0.9655\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 571.9251 - acc: 0.9443 - val_loss: 451.8616 - val_acc: 0.9655\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 454.9944 - acc: 0.9443 - val_loss: 385.2742 - val_acc: 0.9655\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 413.8709 - acc: 0.9443 - val_loss: 338.9944 - val_acc: 0.9655\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 396.5625 - acc: 0.9443 - val_loss: 302.4048 - val_acc: 0.9655\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 342.7476 - acc: 0.9443 - val_loss: 277.5069 - val_acc: 0.9655\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 349.3854 - acc: 0.9443 - val_loss: 258.9507 - val_acc: 0.9655\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 328.6757 - acc: 0.9443 - val_loss: 241.3917 - val_acc: 0.9655\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 316.9467 - acc: 0.9443 - val_loss: 231.0746 - val_acc: 0.9655\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 304.9364 - acc: 0.9443 - val_loss: 220.3698 - val_acc: 0.9655\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 296.6153 - acc: 0.9443 - val_loss: 208.3987 - val_acc: 0.9655\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 310.2259 - acc: 0.9443 - val_loss: 204.8765 - val_acc: 0.9655\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 293.9212 - acc: 0.9443 - val_loss: 199.7049 - val_acc: 0.9655\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 289.7898 - acc: 0.9443 - val_loss: 193.1592 - val_acc: 0.9655\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 263.5128 - acc: 0.9443 - val_loss: 185.8335 - val_acc: 0.9655\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 283.3254 - acc: 0.9443 - val_loss: 185.6599 - val_acc: 0.9655\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 261.4996 - acc: 0.9443 - val_loss: 174.6431 - val_acc: 0.9655\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 280.0828 - acc: 0.9443 - val_loss: 171.6362 - val_acc: 0.9655\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 256.9063 - acc: 0.9443 - val_loss: 163.8058 - val_acc: 0.9655\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 254.9039 - acc: 0.9443 - val_loss: 156.2621 - val_acc: 0.9655\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 242.1133 - acc: 0.9455 - val_loss: 151.2941 - val_acc: 0.9655\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 231.1686 - acc: 0.9493 - val_loss: 148.2571 - val_acc: 0.9655\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 241.3241 - acc: 0.9468 - val_loss: 137.8429 - val_acc: 0.9655\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 219.4454 - acc: 0.9468 - val_loss: 131.6321 - val_acc: 0.9655\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 222.0781 - acc: 0.9542 - val_loss: 126.2970 - val_acc: 0.9655\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 217.1946 - acc: 0.9493 - val_loss: 127.4549 - val_acc: 0.9655\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 215.6379 - acc: 0.9493 - val_loss: 117.6592 - val_acc: 0.9655\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 206.3329 - acc: 0.9443 - val_loss: 117.2184 - val_acc: 0.9704\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 208.4496 - acc: 0.9493 - val_loss: 119.0396 - val_acc: 0.9704\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 200.8019 - acc: 0.9530 - val_loss: 109.6795 - val_acc: 0.9704\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 200.2237 - acc: 0.9480 - val_loss: 106.7745 - val_acc: 0.9704\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 198.4822 - acc: 0.9455 - val_loss: 114.0837 - val_acc: 0.9704\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 200.6034 - acc: 0.9431 - val_loss: 102.4445 - val_acc: 0.9704\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 181.3023 - acc: 0.9480 - val_loss: 105.5879 - val_acc: 0.9704\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 190.7286 - acc: 0.9455 - val_loss: 101.0164 - val_acc: 0.9704\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 197.3574 - acc: 0.9517 - val_loss: 99.7893 - val_acc: 0.9704\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 187.5412 - acc: 0.9505 - val_loss: 96.8611 - val_acc: 0.9704\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 191.1203 - acc: 0.9567 - val_loss: 97.7119 - val_acc: 0.9704\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 198.2244 - acc: 0.9480 - val_loss: 100.1326 - val_acc: 0.9704\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 181.3119 - acc: 0.9542 - val_loss: 95.0544 - val_acc: 0.9704\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 182.4729 - acc: 0.9480 - val_loss: 91.7516 - val_acc: 0.9704\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 190.0420 - acc: 0.9455 - val_loss: 96.3769 - val_acc: 0.9704\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 175.0072 - acc: 0.9394 - val_loss: 90.9078 - val_acc: 0.9704\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 189.3486 - acc: 0.9493 - val_loss: 96.4144 - val_acc: 0.9704\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 177.5718 - acc: 0.9468 - val_loss: 93.7995 - val_acc: 0.9704\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 169.8501 - acc: 0.9468 - val_loss: 89.8094 - val_acc: 0.9704\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 183.6412 - acc: 0.9468 - val_loss: 96.2361 - val_acc: 0.9704\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 182.8149 - acc: 0.9505 - val_loss: 85.7006 - val_acc: 0.9704\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 162.5101 - acc: 0.9542 - val_loss: 90.0937 - val_acc: 0.9704\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 163.8829 - acc: 0.9493 - val_loss: 85.3772 - val_acc: 0.9704\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 157.9021 - acc: 0.9542 - val_loss: 88.5381 - val_acc: 0.9704\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 184.8979 - acc: 0.9517 - val_loss: 82.6370 - val_acc: 0.9704\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 165.9992 - acc: 0.9554 - val_loss: 85.9518 - val_acc: 0.9704\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 160.4682 - acc: 0.9517 - val_loss: 79.7441 - val_acc: 0.9704\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 159.8396 - acc: 0.9542 - val_loss: 86.8147 - val_acc: 0.9704\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 156.9932 - acc: 0.9530 - val_loss: 82.9467 - val_acc: 0.9704\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 183.3089 - acc: 0.9530 - val_loss: 86.8046 - val_acc: 0.9704\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 167.6208 - acc: 0.9505 - val_loss: 77.8457 - val_acc: 0.9704\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 157.7547 - acc: 0.9480 - val_loss: 75.5787 - val_acc: 0.9704\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 146.9709 - acc: 0.9530 - val_loss: 75.6482 - val_acc: 0.9704\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 170.5701 - acc: 0.9530 - val_loss: 78.4436 - val_acc: 0.9704\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 169.1576 - acc: 0.9530 - val_loss: 74.9509 - val_acc: 0.9704\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 164.0708 - acc: 0.9517 - val_loss: 80.6668 - val_acc: 0.9704\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 159.2500 - acc: 0.9530 - val_loss: 79.3110 - val_acc: 0.9704\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 160.2394 - acc: 0.9517 - val_loss: 74.2623 - val_acc: 0.9704\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 157.8299 - acc: 0.9530 - val_loss: 72.8337 - val_acc: 0.9704\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 157.4279 - acc: 0.9592 - val_loss: 78.2391 - val_acc: 0.9704\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 173.7320 - acc: 0.9604 - val_loss: 73.9347 - val_acc: 0.9704\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 151.3305 - acc: 0.9554 - val_loss: 76.9122 - val_acc: 0.9704\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 152.3019 - acc: 0.9542 - val_loss: 83.0297 - val_acc: 0.9704\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 150.2910 - acc: 0.9542 - val_loss: 76.3064 - val_acc: 0.9704\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 157.5463 - acc: 0.9567 - val_loss: 79.8012 - val_acc: 0.9704\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 151.0589 - acc: 0.9579 - val_loss: 77.9875 - val_acc: 0.9704\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 156.4915 - acc: 0.9554 - val_loss: 74.8721 - val_acc: 0.9704\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 161.7143 - acc: 0.9530 - val_loss: 79.1978 - val_acc: 0.9704\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 160.3223 - acc: 0.9542 - val_loss: 75.6636 - val_acc: 0.9704\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 153.0099 - acc: 0.9579 - val_loss: 76.4317 - val_acc: 0.9704\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 153.8907 - acc: 0.9554 - val_loss: 80.4363 - val_acc: 0.9704\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 146.3880 - acc: 0.9530 - val_loss: 71.5679 - val_acc: 0.9704\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 151.7848 - acc: 0.9567 - val_loss: 75.2358 - val_acc: 0.9704\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 150.7931 - acc: 0.9629 - val_loss: 78.8311 - val_acc: 0.9704\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 159.0871 - acc: 0.9542 - val_loss: 72.1978 - val_acc: 0.9704\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 161.8784 - acc: 0.9530 - val_loss: 73.3742 - val_acc: 0.9704\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 142.7427 - acc: 0.9592 - val_loss: 71.3269 - val_acc: 0.9704\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 150.4489 - acc: 0.9604 - val_loss: 79.4330 - val_acc: 0.9655\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 153.5276 - acc: 0.9567 - val_loss: 75.9397 - val_acc: 0.9606\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 153.6975 - acc: 0.9554 - val_loss: 75.2886 - val_acc: 0.9606\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 153.0054 - acc: 0.9517 - val_loss: 74.2032 - val_acc: 0.9704\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 152.2824 - acc: 0.9567 - val_loss: 69.3675 - val_acc: 0.9704\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 149.4875 - acc: 0.9530 - val_loss: 77.9413 - val_acc: 0.9606\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 142.1722 - acc: 0.9604 - val_loss: 71.7450 - val_acc: 0.9704\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 138.9361 - acc: 0.9579 - val_loss: 77.0684 - val_acc: 0.9704\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 148.4858 - acc: 0.9542 - val_loss: 71.5605 - val_acc: 0.9704\n",
      "Fold 1\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4727.7359 - acc: 0.7203 - val_loss: 4482.9776 - val_acc: 0.9310\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4622.5393 - acc: 0.9455 - val_loss: 4300.3397 - val_acc: 0.9310\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4305.1839 - acc: 0.9455 - val_loss: 3824.5931 - val_acc: 0.9310\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3591.7326 - acc: 0.9455 - val_loss: 2912.9996 - val_acc: 0.9310\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 2474.6371 - acc: 0.9443 - val_loss: 1734.3935 - val_acc: 0.9310\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1448.7397 - acc: 0.9443 - val_loss: 1025.6398 - val_acc: 0.9310\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 952.5586 - acc: 0.9455 - val_loss: 773.0225 - val_acc: 0.9310\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 799.8288 - acc: 0.9455 - val_loss: 598.6998 - val_acc: 0.9310\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 638.3889 - acc: 0.9455 - val_loss: 472.8620 - val_acc: 0.9310\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 539.5176 - acc: 0.9455 - val_loss: 383.6861 - val_acc: 0.9310\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 502.3913 - acc: 0.9455 - val_loss: 323.2462 - val_acc: 0.9310\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 425.7843 - acc: 0.9455 - val_loss: 284.4324 - val_acc: 0.9310\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 395.2983 - acc: 0.9455 - val_loss: 256.1773 - val_acc: 0.9310\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 394.2363 - acc: 0.9455 - val_loss: 239.7584 - val_acc: 0.9310\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 370.5886 - acc: 0.9455 - val_loss: 226.9922 - val_acc: 0.9310\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 360.1028 - acc: 0.9455 - val_loss: 214.8971 - val_acc: 0.9310\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 331.2160 - acc: 0.9455 - val_loss: 205.2912 - val_acc: 0.9310\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 323.5774 - acc: 0.9455 - val_loss: 197.3942 - val_acc: 0.9310\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 338.7541 - acc: 0.9455 - val_loss: 192.1126 - val_acc: 0.9310\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 320.8562 - acc: 0.9455 - val_loss: 187.2099 - val_acc: 0.9310\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 304.3299 - acc: 0.9455 - val_loss: 179.9444 - val_acc: 0.9310\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 293.1834 - acc: 0.9455 - val_loss: 173.0059 - val_acc: 0.9310\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 292.6612 - acc: 0.9455 - val_loss: 167.3829 - val_acc: 0.9310\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 294.4105 - acc: 0.9455 - val_loss: 163.3040 - val_acc: 0.9310\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 292.1922 - acc: 0.9455 - val_loss: 159.1512 - val_acc: 0.9310\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 261.4964 - acc: 0.9455 - val_loss: 148.5694 - val_acc: 0.9310\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 281.3859 - acc: 0.9468 - val_loss: 140.2366 - val_acc: 0.9310\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 259.6287 - acc: 0.9493 - val_loss: 141.0919 - val_acc: 0.9310\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 240.0376 - acc: 0.9505 - val_loss: 125.4511 - val_acc: 0.9310\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 241.6156 - acc: 0.9542 - val_loss: 123.8238 - val_acc: 0.9310\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 241.3983 - acc: 0.9567 - val_loss: 118.1953 - val_acc: 0.9360\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 215.1904 - acc: 0.9505 - val_loss: 108.3929 - val_acc: 0.9360\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 238.9689 - acc: 0.9579 - val_loss: 105.3806 - val_acc: 0.9360\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 205.4210 - acc: 0.9579 - val_loss: 103.1841 - val_acc: 0.9310\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 207.7624 - acc: 0.9554 - val_loss: 96.3720 - val_acc: 0.9360\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 224.6899 - acc: 0.9579 - val_loss: 98.1722 - val_acc: 0.9360\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 215.2901 - acc: 0.9517 - val_loss: 92.6197 - val_acc: 0.9360\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 207.1673 - acc: 0.9554 - val_loss: 89.9283 - val_acc: 0.9360\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 219.6933 - acc: 0.9579 - val_loss: 92.6745 - val_acc: 0.9409\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 223.4204 - acc: 0.9530 - val_loss: 84.3109 - val_acc: 0.9409\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 201.3652 - acc: 0.9592 - val_loss: 87.4617 - val_acc: 0.9409\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 208.7054 - acc: 0.9579 - val_loss: 91.8925 - val_acc: 0.9409\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - ETA: 0s - loss: 203.4315 - acc: 0.944 - 0s - loss: 202.4911 - acc: 0.9493 - val_loss: 82.8308 - val_acc: 0.9409\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 212.5495 - acc: 0.9604 - val_loss: 86.2023 - val_acc: 0.9409\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 194.7111 - acc: 0.9592 - val_loss: 84.9015 - val_acc: 0.9409\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 205.3012 - acc: 0.9567 - val_loss: 79.3847 - val_acc: 0.9360\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 198.7827 - acc: 0.9592 - val_loss: 79.2688 - val_acc: 0.9409\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 199.4028 - acc: 0.9554 - val_loss: 80.3035 - val_acc: 0.9458\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 195.4141 - acc: 0.9616 - val_loss: 82.7630 - val_acc: 0.9458\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 180.1478 - acc: 0.9567 - val_loss: 79.5737 - val_acc: 0.9458\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 203.6198 - acc: 0.9579 - val_loss: 76.3154 - val_acc: 0.9458\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 191.5594 - acc: 0.9592 - val_loss: 75.3253 - val_acc: 0.9458\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 212.0687 - acc: 0.9604 - val_loss: 79.0740 - val_acc: 0.9458\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 188.2462 - acc: 0.9629 - val_loss: 75.6748 - val_acc: 0.9458\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 169.9296 - acc: 0.9641 - val_loss: 73.7359 - val_acc: 0.9458\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 193.2245 - acc: 0.9678 - val_loss: 73.4602 - val_acc: 0.9458\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 186.2674 - acc: 0.9641 - val_loss: 68.7358 - val_acc: 0.9458\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 191.5190 - acc: 0.9629 - val_loss: 78.8643 - val_acc: 0.9409\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 178.0700 - acc: 0.9554 - val_loss: 72.2879 - val_acc: 0.9458\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 179.0062 - acc: 0.9616 - val_loss: 69.2808 - val_acc: 0.9409\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 188.6696 - acc: 0.9542 - val_loss: 72.0361 - val_acc: 0.9458\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 183.1594 - acc: 0.9616 - val_loss: 70.7253 - val_acc: 0.9409\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 160.5146 - acc: 0.9554 - val_loss: 68.8401 - val_acc: 0.9409\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 174.4864 - acc: 0.9592 - val_loss: 67.9371 - val_acc: 0.9458\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 172.2713 - acc: 0.9641 - val_loss: 65.7526 - val_acc: 0.9409\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 190.6301 - acc: 0.9666 - val_loss: 75.2745 - val_acc: 0.9409\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 176.2080 - acc: 0.9653 - val_loss: 64.9223 - val_acc: 0.9507\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 179.0270 - acc: 0.9641 - val_loss: 71.9815 - val_acc: 0.9507\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 186.7644 - acc: 0.9641 - val_loss: 65.2283 - val_acc: 0.9507\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 168.0401 - acc: 0.9666 - val_loss: 64.4884 - val_acc: 0.9507\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 180.9737 - acc: 0.9629 - val_loss: 66.3592 - val_acc: 0.9507\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 176.9847 - acc: 0.9653 - val_loss: 63.1411 - val_acc: 0.9507\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 151.4027 - acc: 0.9592 - val_loss: 64.6918 - val_acc: 0.9507\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 167.7535 - acc: 0.9592 - val_loss: 64.7604 - val_acc: 0.9507\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 166.6710 - acc: 0.9715 - val_loss: 64.8182 - val_acc: 0.9507\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 168.6314 - acc: 0.9678 - val_loss: 65.7077 - val_acc: 0.9507\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 182.7999 - acc: 0.9604 - val_loss: 66.4571 - val_acc: 0.9507\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 173.6890 - acc: 0.9641 - val_loss: 64.7750 - val_acc: 0.9507\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 155.9423 - acc: 0.9517 - val_loss: 66.0578 - val_acc: 0.9507\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 190.1706 - acc: 0.9604 - val_loss: 62.0589 - val_acc: 0.9507\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 167.1140 - acc: 0.9703 - val_loss: 67.1289 - val_acc: 0.9507\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 167.5761 - acc: 0.9629 - val_loss: 60.0618 - val_acc: 0.9507\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 158.9140 - acc: 0.9629 - val_loss: 63.0518 - val_acc: 0.9507\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 158.3024 - acc: 0.9666 - val_loss: 62.6635 - val_acc: 0.9507\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 159.5328 - acc: 0.9678 - val_loss: 61.4040 - val_acc: 0.9507\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 168.0035 - acc: 0.9691 - val_loss: 60.5001 - val_acc: 0.9507\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 168.9083 - acc: 0.9703 - val_loss: 64.8839 - val_acc: 0.9507\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 166.7184 - acc: 0.9653 - val_loss: 59.3091 - val_acc: 0.9507\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 157.1780 - acc: 0.9641 - val_loss: 59.7346 - val_acc: 0.9507\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 153.3429 - acc: 0.9653 - val_loss: 60.0668 - val_acc: 0.9507\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 166.0961 - acc: 0.9629 - val_loss: 59.1902 - val_acc: 0.9507\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 154.9521 - acc: 0.9666 - val_loss: 59.3817 - val_acc: 0.9507\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 153.2327 - acc: 0.9629 - val_loss: 59.2855 - val_acc: 0.9507\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 148.2339 - acc: 0.9641 - val_loss: 57.6806 - val_acc: 0.9507\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 154.2491 - acc: 0.9666 - val_loss: 57.4399 - val_acc: 0.9507\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 142.8142 - acc: 0.9678 - val_loss: 61.5488 - val_acc: 0.9507\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 153.0149 - acc: 0.9678 - val_loss: 57.0777 - val_acc: 0.9507\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 136.8843 - acc: 0.9728 - val_loss: 59.0571 - val_acc: 0.9507\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 156.5584 - acc: 0.9641 - val_loss: 58.4528 - val_acc: 0.9507\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 153.7872 - acc: 0.9629 - val_loss: 58.2644 - val_acc: 0.9507\n",
      "Fold 2\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4612.2224 - acc: 0.8218 - val_loss: 4432.9931 - val_acc: 0.9458\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4457.4842 - acc: 0.9344 - val_loss: 4164.7352 - val_acc: 0.9458\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4025.4722 - acc: 0.9431 - val_loss: 3507.4478 - val_acc: 0.9458\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3140.6444 - acc: 0.9431 - val_loss: 2388.5347 - val_acc: 0.9458\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1922.6347 - acc: 0.9431 - val_loss: 1302.2976 - val_acc: 0.9458\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1144.2810 - acc: 0.9431 - val_loss: 867.4508 - val_acc: 0.9458\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 864.7824 - acc: 0.9443 - val_loss: 651.9561 - val_acc: 0.9458\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 661.9034 - acc: 0.9443 - val_loss: 514.1401 - val_acc: 0.9458\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 554.4239 - acc: 0.9443 - val_loss: 413.0092 - val_acc: 0.9458\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 466.6256 - acc: 0.9443 - val_loss: 346.6211 - val_acc: 0.9458\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 394.3176 - acc: 0.9443 - val_loss: 306.1673 - val_acc: 0.9458\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 388.9594 - acc: 0.9443 - val_loss: 278.1856 - val_acc: 0.9458\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 357.8000 - acc: 0.9443 - val_loss: 253.1638 - val_acc: 0.9458\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 344.2107 - acc: 0.9443 - val_loss: 240.8944 - val_acc: 0.9458\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 336.1906 - acc: 0.9443 - val_loss: 226.6848 - val_acc: 0.9458\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 317.6002 - acc: 0.9443 - val_loss: 213.4812 - val_acc: 0.9458\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 309.5850 - acc: 0.9443 - val_loss: 208.8908 - val_acc: 0.9458\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 310.8316 - acc: 0.9443 - val_loss: 198.1021 - val_acc: 0.9458\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 293.3748 - acc: 0.9443 - val_loss: 189.9434 - val_acc: 0.9458\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 287.0609 - acc: 0.9443 - val_loss: 184.2907 - val_acc: 0.9458\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 273.3929 - acc: 0.9443 - val_loss: 176.6758 - val_acc: 0.9458\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 271.6779 - acc: 0.9443 - val_loss: 170.8366 - val_acc: 0.9458\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 263.4268 - acc: 0.9443 - val_loss: 168.9712 - val_acc: 0.9458\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 252.2721 - acc: 0.9443 - val_loss: 155.7962 - val_acc: 0.9458\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 266.4787 - acc: 0.9443 - val_loss: 153.1505 - val_acc: 0.9458\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 244.9518 - acc: 0.9443 - val_loss: 149.6122 - val_acc: 0.9458\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 235.3238 - acc: 0.9455 - val_loss: 138.1378 - val_acc: 0.9458\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 241.1891 - acc: 0.9468 - val_loss: 141.7601 - val_acc: 0.9458\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 243.2190 - acc: 0.9468 - val_loss: 129.7886 - val_acc: 0.9458\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 235.8013 - acc: 0.9480 - val_loss: 122.5054 - val_acc: 0.9458\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 213.6767 - acc: 0.9480 - val_loss: 125.5966 - val_acc: 0.9458\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 206.0488 - acc: 0.9505 - val_loss: 113.5132 - val_acc: 0.9458\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 205.8817 - acc: 0.9530 - val_loss: 111.7009 - val_acc: 0.9507\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 215.3306 - acc: 0.9542 - val_loss: 106.0752 - val_acc: 0.9458\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 209.8359 - acc: 0.9530 - val_loss: 105.5759 - val_acc: 0.9458\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 209.6873 - acc: 0.9554 - val_loss: 104.2841 - val_acc: 0.9458\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 199.1633 - acc: 0.9455 - val_loss: 100.0087 - val_acc: 0.9458\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 195.9282 - acc: 0.9505 - val_loss: 96.2803 - val_acc: 0.9458\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 196.4611 - acc: 0.9493 - val_loss: 94.5553 - val_acc: 0.9458\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 200.9420 - acc: 0.9530 - val_loss: 93.1092 - val_acc: 0.9507\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 179.8441 - acc: 0.9542 - val_loss: 90.1178 - val_acc: 0.9507\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 182.9629 - acc: 0.9542 - val_loss: 88.4856 - val_acc: 0.9507\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 185.1006 - acc: 0.9455 - val_loss: 88.4279 - val_acc: 0.9507\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 184.7943 - acc: 0.9542 - val_loss: 87.2483 - val_acc: 0.9507\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 185.2000 - acc: 0.9579 - val_loss: 83.0846 - val_acc: 0.9507\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 184.7435 - acc: 0.9666 - val_loss: 88.6326 - val_acc: 0.9507\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 181.1713 - acc: 0.9567 - val_loss: 84.3994 - val_acc: 0.9557\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 180.4037 - acc: 0.9579 - val_loss: 82.2565 - val_acc: 0.9557\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 174.0085 - acc: 0.9505 - val_loss: 84.1676 - val_acc: 0.9507\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 180.3208 - acc: 0.9629 - val_loss: 79.1729 - val_acc: 0.9507\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 168.2776 - acc: 0.9579 - val_loss: 80.4013 - val_acc: 0.9507\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 176.0305 - acc: 0.9678 - val_loss: 79.4198 - val_acc: 0.9507\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 173.5775 - acc: 0.9530 - val_loss: 79.0089 - val_acc: 0.9507\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 172.0968 - acc: 0.9592 - val_loss: 81.0256 - val_acc: 0.9507\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 162.7645 - acc: 0.9629 - val_loss: 75.5143 - val_acc: 0.9507\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 173.0526 - acc: 0.9579 - val_loss: 81.7301 - val_acc: 0.9507\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 168.3676 - acc: 0.9604 - val_loss: 74.7288 - val_acc: 0.9507\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 179.0706 - acc: 0.9579 - val_loss: 77.9432 - val_acc: 0.9507\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 154.5376 - acc: 0.9641 - val_loss: 74.4066 - val_acc: 0.9507\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 168.3335 - acc: 0.9604 - val_loss: 76.5554 - val_acc: 0.9507\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 173.0252 - acc: 0.9604 - val_loss: 78.7013 - val_acc: 0.9507\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 163.8190 - acc: 0.9629 - val_loss: 72.8657 - val_acc: 0.9507\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 154.2793 - acc: 0.9666 - val_loss: 77.3733 - val_acc: 0.9507\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 167.0382 - acc: 0.9678 - val_loss: 74.3790 - val_acc: 0.9507\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 160.2569 - acc: 0.9678 - val_loss: 69.8409 - val_acc: 0.9507\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 169.9105 - acc: 0.9666 - val_loss: 74.4756 - val_acc: 0.9507\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 158.7344 - acc: 0.9592 - val_loss: 69.2192 - val_acc: 0.9507\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 148.2425 - acc: 0.9604 - val_loss: 82.6348 - val_acc: 0.9557\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 161.2359 - acc: 0.9678 - val_loss: 71.9995 - val_acc: 0.9557\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 153.2718 - acc: 0.9641 - val_loss: 68.7439 - val_acc: 0.9606\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 157.3419 - acc: 0.9579 - val_loss: 72.3821 - val_acc: 0.9606\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 142.1838 - acc: 0.9678 - val_loss: 67.9059 - val_acc: 0.9606\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 147.2775 - acc: 0.9579 - val_loss: 69.7333 - val_acc: 0.9606\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 160.2352 - acc: 0.9691 - val_loss: 69.1308 - val_acc: 0.9557\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 145.4377 - acc: 0.9728 - val_loss: 70.8098 - val_acc: 0.9557\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 152.3728 - acc: 0.9641 - val_loss: 72.7176 - val_acc: 0.9557\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 133.1010 - acc: 0.9715 - val_loss: 65.8013 - val_acc: 0.9606\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 149.5559 - acc: 0.9641 - val_loss: 71.9759 - val_acc: 0.9606\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 151.8812 - acc: 0.9616 - val_loss: 67.5918 - val_acc: 0.9606\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 151.1695 - acc: 0.9641 - val_loss: 70.7541 - val_acc: 0.9606\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 145.2133 - acc: 0.9616 - val_loss: 66.5806 - val_acc: 0.9606\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 152.0550 - acc: 0.9641 - val_loss: 67.0508 - val_acc: 0.9606\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 135.1363 - acc: 0.9641 - val_loss: 65.4838 - val_acc: 0.9606\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 152.8455 - acc: 0.9740 - val_loss: 66.4079 - val_acc: 0.9606\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 145.3675 - acc: 0.9641 - val_loss: 68.0593 - val_acc: 0.9557\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 150.7563 - acc: 0.9629 - val_loss: 66.3943 - val_acc: 0.9557\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 149.8204 - acc: 0.9629 - val_loss: 65.9351 - val_acc: 0.9557\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 154.9749 - acc: 0.9629 - val_loss: 67.9049 - val_acc: 0.9557\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 145.4833 - acc: 0.9604 - val_loss: 64.9240 - val_acc: 0.9606\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 150.2516 - acc: 0.9703 - val_loss: 76.9574 - val_acc: 0.9606\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 155.7281 - acc: 0.9666 - val_loss: 62.2072 - val_acc: 0.9606\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 155.3097 - acc: 0.9691 - val_loss: 63.5346 - val_acc: 0.9606\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 149.5706 - acc: 0.9653 - val_loss: 64.6978 - val_acc: 0.9606\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 148.9772 - acc: 0.9715 - val_loss: 59.2694 - val_acc: 0.9606\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 147.1182 - acc: 0.9641 - val_loss: 64.2709 - val_acc: 0.9606\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 137.5759 - acc: 0.9678 - val_loss: 62.7689 - val_acc: 0.9606\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 142.7089 - acc: 0.9752 - val_loss: 62.4893 - val_acc: 0.9606\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 133.1747 - acc: 0.9715 - val_loss: 70.7640 - val_acc: 0.9606\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 136.7324 - acc: 0.9678 - val_loss: 69.2695 - val_acc: 0.9606\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 147.4042 - acc: 0.9641 - val_loss: 61.1201 - val_acc: 0.9606\n",
      "Fold 3\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 1s - loss: 4630.1065 - acc: 0.8564 - val_loss: 4529.6917 - val_acc: 0.9557\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4520.2301 - acc: 0.9394 - val_loss: 4327.3067 - val_acc: 0.9557\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4185.0521 - acc: 0.9394 - val_loss: 3792.5573 - val_acc: 0.9557\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3451.1835 - acc: 0.9394 - val_loss: 2796.8175 - val_acc: 0.9557\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 2334.4622 - acc: 0.9394 - val_loss: 1585.0760 - val_acc: 0.9557\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1290.7822 - acc: 0.9394 - val_loss: 887.6206 - val_acc: 0.9557\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 959.4125 - acc: 0.9394 - val_loss: 687.8334 - val_acc: 0.9557\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 763.7507 - acc: 0.9394 - val_loss: 534.2258 - val_acc: 0.9557\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 629.7761 - acc: 0.9394 - val_loss: 431.8005 - val_acc: 0.9557\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 548.6840 - acc: 0.9394 - val_loss: 354.1605 - val_acc: 0.9557\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 454.4022 - acc: 0.9394 - val_loss: 302.3106 - val_acc: 0.9557\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 394.4899 - acc: 0.9394 - val_loss: 273.1807 - val_acc: 0.9557\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 390.2341 - acc: 0.9394 - val_loss: 251.0985 - val_acc: 0.9557\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 357.2929 - acc: 0.9394 - val_loss: 232.2162 - val_acc: 0.9557\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 361.2055 - acc: 0.9394 - val_loss: 227.4157 - val_acc: 0.9557\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 333.7383 - acc: 0.9394 - val_loss: 214.7693 - val_acc: 0.9557\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 307.8657 - acc: 0.9394 - val_loss: 204.2048 - val_acc: 0.9557\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 317.1951 - acc: 0.9394 - val_loss: 190.1690 - val_acc: 0.9557\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 299.7419 - acc: 0.9394 - val_loss: 191.0707 - val_acc: 0.9557\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 284.4806 - acc: 0.9394 - val_loss: 179.0429 - val_acc: 0.9557\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 278.8194 - acc: 0.9394 - val_loss: 174.8531 - val_acc: 0.9557\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 275.0809 - acc: 0.9394 - val_loss: 170.0709 - val_acc: 0.9557\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 265.8462 - acc: 0.9394 - val_loss: 162.3246 - val_acc: 0.9557\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 266.3611 - acc: 0.9406 - val_loss: 147.0959 - val_acc: 0.9557\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 257.1306 - acc: 0.9394 - val_loss: 141.6770 - val_acc: 0.9557\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 248.6471 - acc: 0.9406 - val_loss: 137.0194 - val_acc: 0.9557\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 248.2373 - acc: 0.9394 - val_loss: 122.8369 - val_acc: 0.9557\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 225.9703 - acc: 0.9443 - val_loss: 115.3508 - val_acc: 0.9606\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 228.2510 - acc: 0.9443 - val_loss: 120.7543 - val_acc: 0.9655\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 224.3621 - acc: 0.9418 - val_loss: 103.4345 - val_acc: 0.9606\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 216.7250 - acc: 0.9455 - val_loss: 103.2982 - val_acc: 0.9557\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 207.3303 - acc: 0.9431 - val_loss: 96.2390 - val_acc: 0.9557\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 194.9221 - acc: 0.9480 - val_loss: 87.6789 - val_acc: 0.9557\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 206.3792 - acc: 0.9431 - val_loss: 93.5370 - val_acc: 0.9557\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 197.5515 - acc: 0.9443 - val_loss: 84.4917 - val_acc: 0.9557\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 185.8978 - acc: 0.9455 - val_loss: 84.5592 - val_acc: 0.9606\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 205.6520 - acc: 0.9443 - val_loss: 90.0706 - val_acc: 0.9557\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 201.1537 - acc: 0.9493 - val_loss: 79.3337 - val_acc: 0.9606\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 165.9756 - acc: 0.9443 - val_loss: 85.6618 - val_acc: 0.9606\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 182.7559 - acc: 0.9505 - val_loss: 89.9863 - val_acc: 0.9606\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 188.7163 - acc: 0.9505 - val_loss: 75.7729 - val_acc: 0.9606\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 190.3287 - acc: 0.9493 - val_loss: 85.2083 - val_acc: 0.9606\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 195.3651 - acc: 0.9505 - val_loss: 82.4165 - val_acc: 0.9655\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 183.9675 - acc: 0.9443 - val_loss: 75.2387 - val_acc: 0.9606\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 189.5899 - acc: 0.9455 - val_loss: 82.6576 - val_acc: 0.9655\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 189.2092 - acc: 0.9455 - val_loss: 76.9865 - val_acc: 0.9655\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 175.1028 - acc: 0.9468 - val_loss: 74.8933 - val_acc: 0.9655\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 179.9856 - acc: 0.9517 - val_loss: 78.0170 - val_acc: 0.9655\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 190.9548 - acc: 0.9530 - val_loss: 75.5805 - val_acc: 0.9655\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 183.4661 - acc: 0.9431 - val_loss: 71.1895 - val_acc: 0.9606\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 182.8654 - acc: 0.9418 - val_loss: 71.2342 - val_acc: 0.9655\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 179.4001 - acc: 0.9455 - val_loss: 66.6584 - val_acc: 0.9655\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 168.7169 - acc: 0.9530 - val_loss: 68.3906 - val_acc: 0.9704\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 171.4136 - acc: 0.9604 - val_loss: 72.2997 - val_acc: 0.9704\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 164.6482 - acc: 0.9480 - val_loss: 66.2162 - val_acc: 0.9704\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 179.3169 - acc: 0.9517 - val_loss: 76.8786 - val_acc: 0.9754\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 166.0837 - acc: 0.9480 - val_loss: 66.2825 - val_acc: 0.9704\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 165.8885 - acc: 0.9431 - val_loss: 72.3834 - val_acc: 0.9754\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 168.6125 - acc: 0.9542 - val_loss: 63.7294 - val_acc: 0.9754\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 180.6275 - acc: 0.9567 - val_loss: 74.0950 - val_acc: 0.9754\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 161.4775 - acc: 0.9505 - val_loss: 64.0422 - val_acc: 0.9754\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 146.4643 - acc: 0.9530 - val_loss: 74.7354 - val_acc: 0.9754\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 163.2699 - acc: 0.9443 - val_loss: 60.3473 - val_acc: 0.9754\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 157.4835 - acc: 0.9493 - val_loss: 75.1441 - val_acc: 0.9754\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 171.1351 - acc: 0.9493 - val_loss: 66.2975 - val_acc: 0.9754\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 159.4686 - acc: 0.9554 - val_loss: 64.7614 - val_acc: 0.9754\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 163.1742 - acc: 0.9567 - val_loss: 71.2895 - val_acc: 0.9754\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 166.8718 - acc: 0.9629 - val_loss: 58.8833 - val_acc: 0.9754\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 158.5317 - acc: 0.9579 - val_loss: 61.1314 - val_acc: 0.9754\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 155.6574 - acc: 0.9468 - val_loss: 61.3805 - val_acc: 0.9754\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 155.3680 - acc: 0.9505 - val_loss: 68.0000 - val_acc: 0.9754\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 156.6968 - acc: 0.9493 - val_loss: 58.9992 - val_acc: 0.9754\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 161.2189 - acc: 0.9579 - val_loss: 59.8573 - val_acc: 0.9754\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 162.7540 - acc: 0.9542 - val_loss: 67.7180 - val_acc: 0.9754\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 147.0226 - acc: 0.9579 - val_loss: 60.8108 - val_acc: 0.9754\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 151.1548 - acc: 0.9517 - val_loss: 60.4136 - val_acc: 0.9754\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 149.8918 - acc: 0.9505 - val_loss: 61.7866 - val_acc: 0.9754\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 158.3642 - acc: 0.9542 - val_loss: 59.6723 - val_acc: 0.9754\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 145.8433 - acc: 0.9567 - val_loss: 60.7118 - val_acc: 0.9754\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 146.0834 - acc: 0.9517 - val_loss: 61.3081 - val_acc: 0.9754\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 145.6550 - acc: 0.9604 - val_loss: 61.9795 - val_acc: 0.9754\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 147.1929 - acc: 0.9505 - val_loss: 62.5482 - val_acc: 0.9754\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 160.6539 - acc: 0.9517 - val_loss: 59.5111 - val_acc: 0.9754\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 159.0293 - acc: 0.9616 - val_loss: 59.6996 - val_acc: 0.9754\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 142.3761 - acc: 0.9567 - val_loss: 59.5349 - val_acc: 0.9754\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 139.9905 - acc: 0.9567 - val_loss: 63.9903 - val_acc: 0.9754\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 144.1657 - acc: 0.9517 - val_loss: 56.2955 - val_acc: 0.9754\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 155.0758 - acc: 0.9554 - val_loss: 58.9008 - val_acc: 0.9754\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 149.3938 - acc: 0.9530 - val_loss: 64.2181 - val_acc: 0.9704\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 137.1781 - acc: 0.9554 - val_loss: 58.6785 - val_acc: 0.9704\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 146.8675 - acc: 0.9517 - val_loss: 60.2585 - val_acc: 0.9704\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 144.2838 - acc: 0.9517 - val_loss: 59.9279 - val_acc: 0.9704\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 139.1158 - acc: 0.9542 - val_loss: 59.1558 - val_acc: 0.9704\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 153.7973 - acc: 0.9542 - val_loss: 62.6560 - val_acc: 0.9704\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 143.4194 - acc: 0.9554 - val_loss: 56.3386 - val_acc: 0.9704\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 154.5896 - acc: 0.9604 - val_loss: 58.3482 - val_acc: 0.9704\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 155.2749 - acc: 0.9579 - val_loss: 58.2318 - val_acc: 0.9704\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 151.0020 - acc: 0.9604 - val_loss: 66.6096 - val_acc: 0.9704\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 150.8933 - acc: 0.9592 - val_loss: 57.7745 - val_acc: 0.9704\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 145.1624 - acc: 0.9629 - val_loss: 58.5786 - val_acc: 0.9704\n",
      "Fold 4\n",
      "Train on 809 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 1s - loss: 4528.7870 - acc: 0.8702 - val_loss: 4791.9352 - val_acc: 0.9507\n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 4311.2194 - acc: 0.9382 - val_loss: 4385.0461 - val_acc: 0.9507\n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 3726.9260 - acc: 0.9382 - val_loss: 3483.6747 - val_acc: 0.9507\n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 2646.6195 - acc: 0.9382 - val_loss: 2129.1818 - val_acc: 0.9507\n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 1502.4035 - acc: 0.9382 - val_loss: 1062.1663 - val_acc: 0.9507\n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 976.1739 - acc: 0.9382 - val_loss: 724.4202 - val_acc: 0.9507\n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 769.3061 - acc: 0.9382 - val_loss: 558.7014 - val_acc: 0.9507\n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 634.1468 - acc: 0.9382 - val_loss: 448.7099 - val_acc: 0.9507\n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 528.3290 - acc: 0.9382 - val_loss: 359.6328 - val_acc: 0.9507\n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 442.6603 - acc: 0.9382 - val_loss: 309.6921 - val_acc: 0.9507\n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 412.1085 - acc: 0.9382 - val_loss: 284.9851 - val_acc: 0.9507\n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 367.9904 - acc: 0.9382 - val_loss: 251.7009 - val_acc: 0.9507\n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 366.5836 - acc: 0.9382 - val_loss: 235.4339 - val_acc: 0.9507\n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 330.2406 - acc: 0.9382 - val_loss: 220.0649 - val_acc: 0.9507\n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 340.5683 - acc: 0.9382 - val_loss: 212.7146 - val_acc: 0.9507\n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 313.7888 - acc: 0.9382 - val_loss: 207.8882 - val_acc: 0.9507\n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 292.6946 - acc: 0.9382 - val_loss: 195.6837 - val_acc: 0.9507\n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 287.7847 - acc: 0.9382 - val_loss: 194.8850 - val_acc: 0.9507\n",
      "Epoch 19/100\n",
      "809/809 [==============================] - 0s - loss: 291.0965 - acc: 0.9382 - val_loss: 182.2291 - val_acc: 0.9507\n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 280.1962 - acc: 0.9382 - val_loss: 181.7442 - val_acc: 0.9507\n",
      "Epoch 21/100\n",
      "809/809 [==============================] - 0s - loss: 263.8851 - acc: 0.9382 - val_loss: 175.8602 - val_acc: 0.9507\n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 251.0380 - acc: 0.9382 - val_loss: 180.1410 - val_acc: 0.9507\n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 271.6382 - acc: 0.9394 - val_loss: 161.5969 - val_acc: 0.9507\n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 245.5359 - acc: 0.9382 - val_loss: 157.9214 - val_acc: 0.9507\n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 250.7816 - acc: 0.9382 - val_loss: 156.6747 - val_acc: 0.9507\n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 241.9397 - acc: 0.9419 - val_loss: 146.5548 - val_acc: 0.9507\n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 229.4671 - acc: 0.9468 - val_loss: 137.3510 - val_acc: 0.9507\n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 238.3280 - acc: 0.9431 - val_loss: 137.8812 - val_acc: 0.9507\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 235.8246 - acc: 0.9468 - val_loss: 130.5336 - val_acc: 0.9606\n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 207.6419 - acc: 0.9468 - val_loss: 120.4077 - val_acc: 0.9606\n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 213.2851 - acc: 0.9518 - val_loss: 118.1806 - val_acc: 0.9606\n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 210.0592 - acc: 0.9481 - val_loss: 108.9372 - val_acc: 0.9606\n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 197.9300 - acc: 0.9468 - val_loss: 108.1994 - val_acc: 0.9606\n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 200.0485 - acc: 0.9543 - val_loss: 105.7013 - val_acc: 0.9606\n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 203.0277 - acc: 0.9407 - val_loss: 106.3676 - val_acc: 0.9606\n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 197.9004 - acc: 0.9481 - val_loss: 102.5186 - val_acc: 0.9606\n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 194.6666 - acc: 0.9567 - val_loss: 100.2616 - val_acc: 0.9606\n",
      "Epoch 38/100\n",
      "809/809 [==============================] - 0s - loss: 196.5783 - acc: 0.9431 - val_loss: 101.9060 - val_acc: 0.9655\n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 188.5844 - acc: 0.9444 - val_loss: 96.7115 - val_acc: 0.9655\n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 187.1632 - acc: 0.9481 - val_loss: 95.8464 - val_acc: 0.9655\n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 181.0678 - acc: 0.9530 - val_loss: 92.0054 - val_acc: 0.9655\n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 178.7350 - acc: 0.9481 - val_loss: 93.8241 - val_acc: 0.9655\n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 177.2113 - acc: 0.9493 - val_loss: 95.9738 - val_acc: 0.9655\n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 195.3591 - acc: 0.9468 - val_loss: 90.8753 - val_acc: 0.9655\n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 188.0713 - acc: 0.9543 - val_loss: 92.2141 - val_acc: 0.9655\n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 176.2996 - acc: 0.9506 - val_loss: 91.8797 - val_acc: 0.9655\n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 184.0918 - acc: 0.9580 - val_loss: 88.4145 - val_acc: 0.9655\n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 171.8056 - acc: 0.9555 - val_loss: 91.5351 - val_acc: 0.9655\n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 172.8182 - acc: 0.9567 - val_loss: 87.3788 - val_acc: 0.9655\n",
      "Epoch 50/100\n",
      "809/809 [==============================] - 0s - loss: 171.5752 - acc: 0.9543 - val_loss: 87.2275 - val_acc: 0.9655\n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 175.9906 - acc: 0.9518 - val_loss: 89.8248 - val_acc: 0.9655\n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 171.4358 - acc: 0.9481 - val_loss: 89.3042 - val_acc: 0.9655\n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 160.7346 - acc: 0.9468 - val_loss: 85.6733 - val_acc: 0.9655\n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 159.9709 - acc: 0.9604 - val_loss: 90.3146 - val_acc: 0.9655\n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 169.0020 - acc: 0.9543 - val_loss: 84.9335 - val_acc: 0.9655\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 171.1289 - acc: 0.9444 - val_loss: 87.8183 - val_acc: 0.9655\n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 178.6755 - acc: 0.9555 - val_loss: 81.7631 - val_acc: 0.9655\n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 161.6672 - acc: 0.9481 - val_loss: 90.2740 - val_acc: 0.9655\n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 165.3013 - acc: 0.9468 - val_loss: 85.2756 - val_acc: 0.9655\n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 154.0527 - acc: 0.9580 - val_loss: 80.0229 - val_acc: 0.9655\n",
      "Epoch 61/100\n",
      "809/809 [==============================] - 0s - loss: 162.9398 - acc: 0.9555 - val_loss: 84.5282 - val_acc: 0.9655\n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 154.7515 - acc: 0.9543 - val_loss: 89.5082 - val_acc: 0.9655\n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 153.6464 - acc: 0.9543 - val_loss: 82.9544 - val_acc: 0.9655\n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 165.2312 - acc: 0.9530 - val_loss: 85.6946 - val_acc: 0.9655\n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 162.3721 - acc: 0.9592 - val_loss: 80.0863 - val_acc: 0.9655\n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 160.4986 - acc: 0.9580 - val_loss: 82.6310 - val_acc: 0.9655\n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 164.1771 - acc: 0.9555 - val_loss: 81.6447 - val_acc: 0.9655\n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 153.2936 - acc: 0.9567 - val_loss: 83.1516 - val_acc: 0.9655\n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 151.0259 - acc: 0.9617 - val_loss: 81.0566 - val_acc: 0.9655\n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 132.5427 - acc: 0.9567 - val_loss: 81.2885 - val_acc: 0.9655\n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 149.2543 - acc: 0.9617 - val_loss: 78.3166 - val_acc: 0.9655\n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 147.2098 - acc: 0.9580 - val_loss: 83.2980 - val_acc: 0.9655\n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 149.7481 - acc: 0.9629 - val_loss: 75.8858 - val_acc: 0.9655\n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 156.7873 - acc: 0.9592 - val_loss: 83.1503 - val_acc: 0.9655\n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 149.1112 - acc: 0.9617 - val_loss: 77.4708 - val_acc: 0.9655\n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 159.7050 - acc: 0.9654 - val_loss: 83.7919 - val_acc: 0.9655\n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 160.7712 - acc: 0.9580 - val_loss: 78.4876 - val_acc: 0.9655\n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 144.1065 - acc: 0.9604 - val_loss: 78.9257 - val_acc: 0.9655\n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 152.9736 - acc: 0.9530 - val_loss: 79.1384 - val_acc: 0.9655\n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 155.6831 - acc: 0.9580 - val_loss: 84.3932 - val_acc: 0.9655\n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 144.3587 - acc: 0.9617 - val_loss: 79.7482 - val_acc: 0.9655\n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 145.9289 - acc: 0.9555 - val_loss: 81.7343 - val_acc: 0.9655\n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 144.5409 - acc: 0.9629 - val_loss: 75.2504 - val_acc: 0.9655\n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 153.2565 - acc: 0.9592 - val_loss: 81.3043 - val_acc: 0.9655\n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 145.0673 - acc: 0.9567 - val_loss: 76.9590 - val_acc: 0.9655\n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 141.0564 - acc: 0.9555 - val_loss: 75.4726 - val_acc: 0.9655\n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 134.2912 - acc: 0.9629 - val_loss: 84.6467 - val_acc: 0.9655\n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 152.5357 - acc: 0.9604 - val_loss: 75.2221 - val_acc: 0.9655\n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 155.1660 - acc: 0.9604 - val_loss: 87.0907 - val_acc: 0.9655\n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 139.3064 - acc: 0.9629 - val_loss: 76.7990 - val_acc: 0.9655\n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 136.9408 - acc: 0.9629 - val_loss: 79.7037 - val_acc: 0.9655\n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 142.3100 - acc: 0.9567 - val_loss: 80.0653 - val_acc: 0.9655\n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 131.7149 - acc: 0.9617 - val_loss: 73.4742 - val_acc: 0.9655\n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 158.4602 - acc: 0.9567 - val_loss: 79.2945 - val_acc: 0.9655\n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 141.0414 - acc: 0.9592 - val_loss: 74.8419 - val_acc: 0.9655\n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 155.0786 - acc: 0.9592 - val_loss: 80.1926 - val_acc: 0.9655\n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 145.7891 - acc: 0.9666 - val_loss: 75.8213 - val_acc: 0.9655\n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 133.3021 - acc: 0.9617 - val_loss: 73.9388 - val_acc: 0.9655\n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 130.2368 - acc: 0.9592 - val_loss: 78.3456 - val_acc: 0.9655\n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 139.6044 - acc: 0.9567 - val_loss: 75.1687 - val_acc: 0.9655\n",
      "e4_mean 10.3310501084\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "e5_mean:  10.4781034752\n"
     ]
    }
   ],
   "source": [
    "blend_train1, blend_test1 = get_oof(clf1, trainX, trainY, testX)\n",
    "e1, e1_mean = accuracy(blend_test1, testY)\n",
    "print(\"el_mean: \", e1_mean)\n",
    "blend_train2, blend_test2 = get_oof(clf2, trainX, trainY, testX)\n",
    "e2, e2_mean = accuracy(blend_test2, testY)\n",
    "print(\"e2_mean: \", e2_mean)\n",
    "blend_train4, blend_test4 = get_oof(clf4, trainX, trainY, testX)\n",
    "e4, e4_mean = accuracy(blend_test4, testY)\n",
    "print(\"e4_mean\", e4_mean)\n",
    "blend_train5, blend_test5 = get_oof(clf5, trainX, trainY, testX)\n",
    "e5, e5_mean = accuracy(blend_test5, testY)\n",
    "print(\"e5_mean: \", e5_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  84.69   ,   41.00268],\n",
       "       [  37.90275,   24.49393],\n",
       "       [  89.848  ,   25.67468],\n",
       "       ..., \n",
       "       [ 109.105  ,   19.13443],\n",
       "       [  34.87425,   22.97593],\n",
       "       [ 113.3825 ,   49.53018]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  91.03826904,   42.48535919,   91.89848328,   42.30059433,\n",
       "          82.58936364,   39.91518   ],\n",
       "       [  49.87475204,   22.85513496,   42.19886398,   23.0267601 ,\n",
       "          67.3305    ,   25.39368   ],\n",
       "       [  88.31018066,   23.92281532,   89.99443817,   21.04453468,\n",
       "          88.07856604,   29.96919887],\n",
       "       ..., \n",
       "       [ 103.12555695,   22.16657639,  103.0127182 ,   18.89170456,\n",
       "         105.637     ,   22.45004   ],\n",
       "       [  33.55236816,   24.23777771,   31.53597641,   19.9478054 ,\n",
       "          26.242     ,   23.06689429],\n",
       "       [ 118.52603912,   55.6455574 ,  111.94852448,   53.22294998,\n",
       "          78.42725   ,   39.60768   ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((blend_train1,blend_train2,blend_train5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, input_dim=10, activation=\"linear\", use_bias=True)`\n",
      "  \"\"\"\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(16, activation=\"linear\", use_bias=True)`\n",
      "  \n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"linear\", use_bias=True)`\n",
      "  import sys\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1264/1264 [==============================] - 1s - loss: 359.0960 - acc: 0.9446      \n",
      "Epoch 2/50\n",
      "1264/1264 [==============================] - 0s - loss: 76.1884 - acc: 0.9517     \n",
      "Epoch 3/50\n",
      "1264/1264 [==============================] - 0s - loss: 50.2868 - acc: 0.9699     \n",
      "Epoch 4/50\n",
      "1264/1264 [==============================] - 0s - loss: 46.7998 - acc: 0.9676     \n",
      "Epoch 5/50\n",
      "1264/1264 [==============================] - 0s - loss: 41.7778 - acc: 0.9684     \n",
      "Epoch 6/50\n",
      "1264/1264 [==============================] - 0s - loss: 39.7484 - acc: 0.9676     \n",
      "Epoch 7/50\n",
      "1264/1264 [==============================] - 0s - loss: 37.9357 - acc: 0.9691     \n",
      "Epoch 8/50\n",
      "1264/1264 [==============================] - 0s - loss: 37.7385 - acc: 0.9715     \n",
      "Epoch 9/50\n",
      "1264/1264 [==============================] - 0s - loss: 38.4449 - acc: 0.9684     \n",
      "Epoch 10/50\n",
      "1264/1264 [==============================] - 0s - loss: 36.1700 - acc: 0.9676     \n",
      "Epoch 11/50\n",
      "1264/1264 [==============================] - 0s - loss: 35.7788 - acc: 0.9684     \n",
      "Epoch 12/50\n",
      "1264/1264 [==============================] - 0s - loss: 37.6540 - acc: 0.9691     \n",
      "Epoch 13/50\n",
      "1264/1264 [==============================] - 0s - loss: 35.5119 - acc: 0.9691     \n",
      "Epoch 14/50\n",
      "1264/1264 [==============================] - 0s - loss: 35.0907 - acc: 0.9691     \n",
      "Epoch 15/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.6872 - acc: 0.9691     \n",
      "Epoch 16/50\n",
      "1264/1264 [==============================] - 0s - loss: 35.6035 - acc: 0.9699     \n",
      "Epoch 17/50\n",
      "1264/1264 [==============================] - 0s - loss: 35.2504 - acc: 0.9684     \n",
      "Epoch 18/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.4399 - acc: 0.9707     \n",
      "Epoch 19/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.5325 - acc: 0.9676     \n",
      "Epoch 20/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.9445 - acc: 0.9707     \n",
      "Epoch 21/50\n",
      "1264/1264 [==============================] - 0s - loss: 35.3306 - acc: 0.9684     \n",
      "Epoch 22/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.3174 - acc: 0.9684     \n",
      "Epoch 23/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.0435 - acc: 0.9699     \n",
      "Epoch 24/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.2385 - acc: 0.9684     \n",
      "Epoch 25/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.9807 - acc: 0.9715     \n",
      "Epoch 26/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.9723 - acc: 0.9723     \n",
      "Epoch 27/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.2220 - acc: 0.9707     \n",
      "Epoch 28/50\n",
      "1264/1264 [==============================] - 0s - loss: 36.7781 - acc: 0.9660     \n",
      "Epoch 29/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.4630 - acc: 0.9691     \n",
      "Epoch 30/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.9057 - acc: 0.9707     \n",
      "Epoch 31/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.1201 - acc: 0.9684     \n",
      "Epoch 32/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.6574 - acc: 0.9691     \n",
      "Epoch 33/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.3172 - acc: 0.9691     \n",
      "Epoch 34/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.8840 - acc: 0.9691     \n",
      "Epoch 35/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.3920 - acc: 0.9691     \n",
      "Epoch 36/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.1982 - acc: 0.9707     \n",
      "Epoch 37/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.9309 - acc: 0.9699     \n",
      "Epoch 38/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.2435 - acc: 0.9715     \n",
      "Epoch 39/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.7876 - acc: 0.9715     \n",
      "Epoch 40/50\n",
      "1264/1264 [==============================] - 0s - loss: 34.4310 - acc: 0.9723     \n",
      "Epoch 41/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.9529 - acc: 0.9699     \n",
      "Epoch 42/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.5108 - acc: 0.9699     \n",
      "Epoch 43/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.5502 - acc: 0.9715     \n",
      "Epoch 44/50\n",
      "1264/1264 [==============================] - 0s - loss: 33.2535 - acc: 0.9707     \n",
      "Epoch 45/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.2443 - acc: 0.9699     \n",
      "Epoch 46/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.6447 - acc: 0.9715     \n",
      "Epoch 47/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.5811 - acc: 0.9691     \n",
      "Epoch 48/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.4924 - acc: 0.9676     \n",
      "Epoch 49/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.5852 - acc: 0.9707     \n",
      "Epoch 50/50\n",
      "1264/1264 [==============================] - 0s - loss: 32.4005 - acc: 0.9699     \n",
      "7.19210026607\n"
     ]
    }
   ],
   "source": [
    "blend_trainX = np.hstack((blend_train1,blend_train2, blend_train3,blend_train4,blend_train5))\n",
    "blend_testX = np.hstack((blend_test1, blend_test2, blend_test3,blend_test4,blend_test5))\n",
    "\n",
    "model_stack = Sequential()\n",
    "model_stack.add(Dense(32, input_dim=10, activation='linear', bias=True))\n",
    "model_stack.add(Dense(16, activation='linear', bias=True))\n",
    "model_stack.add(Dense(2, activation='linear', bias=True))\n",
    "model_stack.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
    "model_stack.fit(blend_trainX,trainY,nb_epoch=50)\n",
    "predict_1 = model_stack.predict(blend_testX)\n",
    "e_stack, e_stack_mean = accuracy(predict_1, testY)\n",
    "print(e_stack_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum error: 0.261665060526 maximum error: 95.2823280652 variance: 56.7659859261 median:  5.69344048531\n"
     ]
    }
   ],
   "source": [
    "print('minimum error:', np.amin(e_stack), 'maximum error:', np.amax(e_stack), 'variance:', np.var(e_stack),\"median: \", np.median(e_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfRGRGPn/BHzsUtagRCkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28924fb5b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_sorted= np.sort(e_stack)\n",
    "p = 1. *np.arange(len(e_stack))/(len(e_stack)-1)\n",
    "plt.plot(error_sorted, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf1aTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x289267c5d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(e_stack, sym=\"o\", whis = 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data to one dimension. above method don't need to reshape\n",
    "\n",
    "blend_train11 =reshaped(blend_train1)\n",
    "blend_test11 = reshaped(blend_test1)\n",
    "blend_train21 =reshaped(blend_train2)\n",
    "blend_test21 = reshaped(blend_test2)\n",
    "blend_train31 =reshaped(blend_train3)\n",
    "blend_test31 = reshaped(blend_test3)\n",
    "blend_train41 =reshaped(blend_train4)\n",
    "blend_test41 = reshaped(blend_test4)\n",
    "blend_train51 =reshaped(blend_train5)\n",
    "blend_test51 = reshaped(blend_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_trainX = np.column_stack((blend_train11,blend_train21, blend_train31,blend_train41,blend_train51))\n",
    "blend_testX = np.column_stack((blend_test11, blend_test21, blend_test31,blend_test41,blend_test51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_s = reshaped(trainY)\n",
    "testY_s = reshaped(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.2503328222\n"
     ]
    }
   ],
   "source": [
    "# stacking model 1\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(blend_trainX, trainY_s)\n",
    "pred1 = lr.predict(blend_testX)\n",
    "e, a = accuracy(pred1.reshape((226,2)), testY)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 103.47663688]\n",
      " [  37.47861771]\n",
      " [  82.00413568]\n",
      " [  27.40981271]\n",
      " [ 119.90302098]\n",
      " [  37.01663983]\n",
      " [ 134.87327909]\n",
      " [  29.05646716]\n",
      " [  55.67037546]\n",
      " [  15.52685932]\n",
      " [  92.60774227]\n",
      " [  49.67480668]\n",
      " [  37.85590219]\n",
      " [  18.68615024]\n",
      " [ 107.28076036]\n",
      " [  24.4511424 ]\n",
      " [  53.86225261]\n",
      " [  16.93309304]\n",
      " [  67.52831905]\n",
      " [  31.36265746]\n",
      " [  77.13317386]\n",
      " [  22.24668159]\n",
      " [  65.64913199]\n",
      " [  23.28191938]\n",
      " [  70.62993459]\n",
      " [  24.46005207]\n",
      " [  54.89730386]\n",
      " [  25.31444603]\n",
      " [ 129.01578486]\n",
      " [  37.68272022]\n",
      " [ 120.19577752]\n",
      " [  69.86745901]\n",
      " [ 133.53783118]\n",
      " [  52.2806416 ]\n",
      " [  95.39668439]\n",
      " [  33.97619493]\n",
      " [ 139.57374803]\n",
      " [  28.12332227]\n",
      " [  28.89421458]\n",
      " [  23.77531956]\n",
      " [  60.70322035]\n",
      " [  36.27920879]\n",
      " [  79.16346358]\n",
      " [  20.32584167]\n",
      " [ 108.50780537]\n",
      " [  19.51286435]\n",
      " [  92.91108788]\n",
      " [  61.42871882]\n",
      " [ 122.5088731 ]\n",
      " [  65.88268005]\n",
      " [  51.84422344]\n",
      " [  24.6755312 ]\n",
      " [  55.62580173]\n",
      " [  17.43746962]\n",
      " [  98.87552637]\n",
      " [  34.94322239]\n",
      " [  55.6342252 ]\n",
      " [  14.96473749]\n",
      " [  79.96109001]\n",
      " [  35.48716522]\n",
      " [ 101.16823102]\n",
      " [  31.03063253]\n",
      " [  94.07482551]\n",
      " [  55.46313583]\n",
      " [ 141.78920038]\n",
      " [  33.05939784]\n",
      " [  86.97854201]\n",
      " [  28.23998955]\n",
      " [ 100.42420388]\n",
      " [  27.91040722]\n",
      " [  91.63378698]\n",
      " [  63.42199889]\n",
      " [ 106.4614706 ]\n",
      " [  20.37976224]\n",
      " [  58.86494943]\n",
      " [  17.01794495]\n",
      " [  93.62079139]\n",
      " [  28.03949893]\n",
      " [  85.39449135]\n",
      " [  27.03453402]\n",
      " [  59.71455396]\n",
      " [  65.86593031]\n",
      " [  81.74735014]\n",
      " [  24.7492132 ]\n",
      " [ 117.05506757]\n",
      " [  17.22954213]\n",
      " [ 122.37174678]\n",
      " [  71.09444847]\n",
      " [  78.21862098]\n",
      " [  25.11902872]\n",
      " [ 123.22068479]\n",
      " [  67.97240067]\n",
      " [  55.08109116]\n",
      " [  32.12424987]\n",
      " [  24.41176525]\n",
      " [  21.6436566 ]\n",
      " [ 105.39883587]\n",
      " [  19.89251078]\n",
      " [  84.89406749]\n",
      " [  18.55857819]\n",
      " [ 172.5811753 ]\n",
      " [  54.5766512 ]\n",
      " [  66.70999215]\n",
      " [  28.69101817]\n",
      " [  62.2078457 ]\n",
      " [  61.61459939]\n",
      " [  62.31691699]\n",
      " [  61.29813566]\n",
      " [  65.219249  ]\n",
      " [  54.80575748]\n",
      " [ 137.22227651]\n",
      " [  35.57883474]\n",
      " [  64.88532758]\n",
      " [  41.93419929]\n",
      " [ 112.17358796]\n",
      " [  16.94344369]\n",
      " [ 131.0249323 ]\n",
      " [  37.57728881]\n",
      " [  65.56417449]\n",
      " [  25.83261013]\n",
      " [ 123.42089613]\n",
      " [  29.21230489]\n",
      " [  93.60034305]\n",
      " [  36.3759209 ]\n",
      " [ 107.30290133]\n",
      " [  21.45616357]\n",
      " [  52.20947216]\n",
      " [  16.15561015]\n",
      " [  72.73643199]\n",
      " [  25.03525046]\n",
      " [  93.50490409]\n",
      " [  34.60804311]\n",
      " [  97.22327572]\n",
      " [  26.21344687]\n",
      " [  67.1634712 ]\n",
      " [  38.81646322]\n",
      " [  40.82032862]\n",
      " [  25.73966575]\n",
      " [  76.34741382]\n",
      " [  27.22853142]\n",
      " [  94.67222539]\n",
      " [  29.31538144]\n",
      " [  99.98988489]\n",
      " [  27.9908601 ]\n",
      " [  83.06440291]\n",
      " [  19.3285973 ]\n",
      " [ 152.39313306]\n",
      " [  46.65755953]\n",
      " [  81.86059821]\n",
      " [  37.05871538]\n",
      " [  81.88608533]\n",
      " [  24.24640282]\n",
      " [  32.11427159]\n",
      " [  23.51934695]\n",
      " [ 110.79818275]\n",
      " [  22.38623035]\n",
      " [ 105.11057422]\n",
      " [  21.94141773]\n",
      " [ 175.90118878]\n",
      " [  55.85845461]\n",
      " [ 141.46679852]\n",
      " [  29.96845752]\n",
      " [  72.57079435]\n",
      " [  26.35820806]\n",
      " [  34.49150162]\n",
      " [  52.05253118]\n",
      " [  77.77313344]\n",
      " [  27.11730146]\n",
      " [ 103.13970847]\n",
      " [  20.92650247]\n",
      " [  98.63857294]\n",
      " [  36.08429465]\n",
      " [  92.13278054]\n",
      " [  43.11519469]\n",
      " [ 112.8761651 ]\n",
      " [  16.25658374]\n",
      " [  60.34274001]\n",
      " [  66.13211724]\n",
      " [  57.69133455]\n",
      " [  17.05002124]\n",
      " [  64.43548456]\n",
      " [  29.6679049 ]\n",
      " [  26.14195358]\n",
      " [  22.5079224 ]\n",
      " [  95.24314422]\n",
      " [  54.3830885 ]\n",
      " [  25.69709152]\n",
      " [  22.37698828]\n",
      " [ 107.41024403]\n",
      " [  45.80940556]\n",
      " [  60.85466531]\n",
      " [  25.07367682]\n",
      " [  83.46413145]\n",
      " [  39.77079324]\n",
      " [  48.00590418]\n",
      " [  22.92011538]\n",
      " [  22.16309208]\n",
      " [  21.41345495]\n",
      " [  93.49568842]\n",
      " [  34.80634846]\n",
      " [  64.0734465 ]\n",
      " [  27.57988253]\n",
      " [  67.48395058]\n",
      " [  38.26305795]\n",
      " [  95.24844684]\n",
      " [  55.16955591]\n",
      " [  99.12645403]\n",
      " [  37.1612442 ]\n",
      " [  19.23056014]\n",
      " [  20.4395335 ]\n",
      " [ 126.61450784]\n",
      " [  35.24964605]\n",
      " [ 100.83931432]\n",
      " [  48.77027509]\n",
      " [  63.3250888 ]\n",
      " [  22.63204176]\n",
      " [  51.66699068]\n",
      " [  15.54576388]\n",
      " [  84.86487047]\n",
      " [  21.25478671]\n",
      " [ 114.00993995]\n",
      " [  33.12802295]\n",
      " [  94.74798795]\n",
      " [  54.37189518]\n",
      " [  62.48448352]\n",
      " [  19.94185476]\n",
      " [ 117.30967101]\n",
      " [  46.4569658 ]\n",
      " [  95.77917528]\n",
      " [  53.76603787]\n",
      " [ 107.61434466]\n",
      " [  32.08150086]\n",
      " [  75.06406281]\n",
      " [  42.13022082]\n",
      " [  98.45796122]\n",
      " [  28.90883711]\n",
      " [ 104.12055807]\n",
      " [  18.72065045]\n",
      " [  64.75457374]\n",
      " [  57.09936305]\n",
      " [  31.1049742 ]\n",
      " [  21.75185724]\n",
      " [ 103.90881394]\n",
      " [  20.26914839]\n",
      " [  66.86228766]\n",
      " [  25.6177488 ]\n",
      " [  81.94743994]\n",
      " [  20.09765163]\n",
      " [  45.02917357]\n",
      " [  24.57899066]\n",
      " [  88.02089132]\n",
      " [  20.28156764]\n",
      " [  43.5491681 ]\n",
      " [  28.60528445]\n",
      " [ 136.27736345]\n",
      " [  39.75353808]\n",
      " [ 115.60927269]\n",
      " [  35.06186198]\n",
      " [  77.49480653]\n",
      " [  31.32911783]\n",
      " [  54.12999994]\n",
      " [  16.39109158]\n",
      " [  68.19388825]\n",
      " [  33.35535385]\n",
      " [  88.44669476]\n",
      " [  37.98997961]\n",
      " [  81.39488093]\n",
      " [  23.35690135]\n",
      " [  88.34634118]\n",
      " [  23.11946181]\n",
      " [  99.59624851]\n",
      " [  31.21874124]\n",
      " [  84.2885554 ]\n",
      " [  19.10559426]\n",
      " [ 115.32366201]\n",
      " [  39.14772525]\n",
      " [ 115.8411074 ]\n",
      " [  37.15349115]\n",
      " [  65.0111413 ]\n",
      " [  31.72916901]\n",
      " [  60.32890218]\n",
      " [  24.11787646]\n",
      " [  95.28129577]\n",
      " [  25.99180462]\n",
      " [ 101.29076503]\n",
      " [  48.55264975]\n",
      " [  62.46118986]\n",
      " [  60.15932609]\n",
      " [  62.57750857]\n",
      " [  39.21558807]\n",
      " [ 102.70238078]\n",
      " [  43.58670502]\n",
      " [  93.84236881]\n",
      " [  42.75831613]\n",
      " [  24.84185879]\n",
      " [  22.49239283]\n",
      " [  32.96924652]\n",
      " [  46.57093431]\n",
      " [ 121.01289808]\n",
      " [  73.2325904 ]\n",
      " [ 106.88354806]\n",
      " [  32.84670766]\n",
      " [  23.61723664]\n",
      " [  21.56972452]\n",
      " [ 112.47460469]\n",
      " [  38.56080613]\n",
      " [  15.76617597]\n",
      " [  46.27408753]\n",
      " [  63.25656288]\n",
      " [  60.57430852]\n",
      " [ 123.31422421]\n",
      " [  28.99506418]\n",
      " [  93.04051682]\n",
      " [  33.65149909]\n",
      " [  99.82069533]\n",
      " [  38.72142986]\n",
      " [  45.43148125]\n",
      " [  24.67749483]\n",
      " [ 120.88926656]\n",
      " [  32.41078706]\n",
      " [ 111.88372604]\n",
      " [  28.98200569]\n",
      " [ 125.08593745]\n",
      " [  22.56976851]\n",
      " [ 113.8509986 ]\n",
      " [  18.60621039]\n",
      " [  73.48986806]\n",
      " [  26.98846388]\n",
      " [  81.61727541]\n",
      " [  20.99698722]\n",
      " [  80.65829749]\n",
      " [  34.30960851]\n",
      " [  85.68085124]\n",
      " [  16.34764812]\n",
      " [  67.23911097]\n",
      " [  53.28428277]\n",
      " [  92.4124073 ]\n",
      " [  31.04271113]\n",
      " [ 149.90589805]\n",
      " [  45.05673627]\n",
      " [  54.12736725]\n",
      " [  14.61328468]\n",
      " [  69.23989494]\n",
      " [  26.78737189]\n",
      " [ 109.94982744]\n",
      " [  19.33946011]\n",
      " [ 111.51233487]\n",
      " [  34.13752366]\n",
      " [ 105.48035186]\n",
      " [  20.84974361]\n",
      " [  88.88910539]\n",
      " [  65.96361432]\n",
      " [  50.10403023]\n",
      " [  20.72183021]\n",
      " [  51.51272895]\n",
      " [  14.79681864]\n",
      " [  86.38487899]\n",
      " [  21.64399933]\n",
      " [  68.98705026]\n",
      " [  25.67991165]\n",
      " [  59.64622491]\n",
      " [  25.58431647]\n",
      " [  50.91849054]\n",
      " [  15.36781745]\n",
      " [ 142.97762067]\n",
      " [  31.62111662]\n",
      " [  23.79526133]\n",
      " [  21.81382308]\n",
      " [ 107.43996758]\n",
      " [  30.34586774]\n",
      " [ 123.58458517]\n",
      " [  48.86025758]\n",
      " [  83.31835023]\n",
      " [  32.82481979]\n",
      " [  93.57984827]\n",
      " [  43.66121659]\n",
      " [  44.87391091]\n",
      " [  17.23590363]\n",
      " [  83.2879478 ]\n",
      " [  20.40756197]\n",
      " [  74.39316061]\n",
      " [  26.70779196]\n",
      " [  93.74298186]\n",
      " [  55.87902403]\n",
      " [  63.45021748]\n",
      " [  62.35521751]\n",
      " [  88.90722636]\n",
      " [  66.48362552]\n",
      " [ 118.78647669]\n",
      " [  43.07350746]\n",
      " [ 125.7462858 ]\n",
      " [  29.76692358]\n",
      " [ 101.91420907]\n",
      " [  29.22513197]\n",
      " [  95.2718752 ]\n",
      " [  25.98928907]\n",
      " [ 102.23923812]\n",
      " [  28.96184296]\n",
      " [  63.53639055]\n",
      " [  22.64152556]\n",
      " [  90.69975939]\n",
      " [  53.91752628]\n",
      " [  56.97702425]\n",
      " [  17.93903368]\n",
      " [  86.92244044]\n",
      " [  19.2026559 ]\n",
      " [ 124.50917008]\n",
      " [  29.72085252]\n",
      " [  83.66668941]\n",
      " [  19.68970582]\n",
      " [  80.20598586]\n",
      " [  31.60897975]\n",
      " [  91.75949105]\n",
      " [  65.9865388 ]\n",
      " [  59.76619205]\n",
      " [  19.57829015]\n",
      " [  69.03505231]\n",
      " [  29.12144735]\n",
      " [  85.50660569]\n",
      " [  20.59425524]\n",
      " [  71.96842431]\n",
      " [  38.28586055]\n",
      " [  61.48217105]\n",
      " [  59.88943336]\n",
      " [ 133.85612776]\n",
      " [  27.97545053]\n",
      " [  51.7494332 ]\n",
      " [  17.64502142]\n",
      " [ 122.76874397]\n",
      " [  65.81688888]\n",
      " [  85.61412821]\n",
      " [  27.60855739]\n",
      " [  87.77496352]\n",
      " [  66.00553591]\n",
      " [  37.86248306]\n",
      " [  23.70825444]\n",
      " [  59.28370442]\n",
      " [  39.87862595]\n",
      " [  73.42537369]\n",
      " [  31.05723451]\n",
      " [  81.40547275]\n",
      " [  18.29215699]\n",
      " [ 123.33449038]\n",
      " [  68.65655481]\n",
      " [  91.67599683]\n",
      " [  65.13621113]\n",
      " [ 101.36013468]\n",
      " [  29.07319913]\n",
      " [  97.25909664]\n",
      " [  36.11483056]\n",
      " [  88.58049349]\n",
      " [  22.45602195]]\n"
     ]
    }
   ],
   "source": [
    "print(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.69701627213\n"
     ]
    }
   ],
   "source": [
    "# stacking model 3\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=4)\n",
    "neigh.fit(blend_trainX, trainY_s)\n",
    "pred3 = neigh.predict(blend_testX)\n",
    "e3, a3 = accuracy(pred3.reshape((226,2)), testY)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <function train_model at 0x0000016E1020A400>\n",
      "Fold 0\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4772.4495 - val_loss: 4640.5937\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4580.5283 - val_loss: 4301.8972\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4048.7716 - val_loss: 3501.5784\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3026.3709 - val_loss: 2262.0923\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1826.8906 - val_loss: 1311.9731\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1118.8428 - val_loss: 884.2793\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 783.5000 - val_loss: 644.1486\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 613.6579 - val_loss: 505.8835\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 510.0662 - val_loss: 413.3073\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 442.2402 - val_loss: 357.3569\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 395.9252 - val_loss: 316.3412\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 359.7618 - val_loss: 289.3404\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 332.4323 - val_loss: 270.8572\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 306.5305 - val_loss: 252.9656\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 305.8137 - val_loss: 245.6081\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 292.1738 - val_loss: 232.9654\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 275.9443 - val_loss: 222.5486\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 275.3529 - val_loss: 216.8672\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 254.4975 - val_loss: 206.6164\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 242.2780 - val_loss: 199.9100\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 240.0937 - val_loss: 193.4837\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 218.2296 - val_loss: 183.5675\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 222.3459 - val_loss: 176.3101\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 210.5334 - val_loss: 167.7171\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 209.3445 - val_loss: 161.6821\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 189.3433 - val_loss: 154.2148\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 182.6669 - val_loss: 146.9419\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 182.3186 - val_loss: 145.4749\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 173.2343 - val_loss: 140.4699\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 167.5253 - val_loss: 138.0908\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 162.6557 - val_loss: 134.7784\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 170.9693 - val_loss: 132.7964\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 158.8453 - val_loss: 130.8291\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 155.2656 - val_loss: 127.6101\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 151.1276 - val_loss: 126.5657\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 149.7685 - val_loss: 124.1619\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 154.6614 - val_loss: 126.1565\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 157.5312 - val_loss: 120.2600\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 149.9497 - val_loss: 122.1297\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 143.2656 - val_loss: 119.9666\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 147.8321 - val_loss: 119.6749\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 145.2302 - val_loss: 117.8620\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 141.6195 - val_loss: 115.8962\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 138.3438 - val_loss: 115.5766\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 152.9143 - val_loss: 114.0976\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 137.1759 - val_loss: 116.1346\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 145.6930 - val_loss: 111.0805\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 142.1944 - val_loss: 114.4875\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 136.1762 - val_loss: 111.8426\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 129.9450 - val_loss: 109.3137\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 134.4404 - val_loss: 109.4950\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 126.5326 - val_loss: 107.8841\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 126.5332 - val_loss: 108.6250\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 137.1883 - val_loss: 106.4717\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 121.3208 - val_loss: 109.0949\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 121.8152 - val_loss: 105.7763\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 125.0661 - val_loss: 105.4347\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 126.3727 - val_loss: 104.3404\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 120.6149 - val_loss: 107.0916\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 126.7808 - val_loss: 103.0236\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 123.0569 - val_loss: 103.3583\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 120.6675 - val_loss: 100.2906\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 111.8366 - val_loss: 100.9787\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 123.5391 - val_loss: 101.5257\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 115.1529 - val_loss: 102.2054\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 115.9208 - val_loss: 101.0450\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 117.3973 - val_loss: 99.3585\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 114.4905 - val_loss: 99.9687\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 126.7847 - val_loss: 96.9263\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 112.1303 - val_loss: 101.6268\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 119.7820 - val_loss: 98.1758\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 115.1963 - val_loss: 99.3716\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 106.2991 - val_loss: 94.8566\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 116.1861 - val_loss: 94.0770\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 110.9481 - val_loss: 94.6215\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 111.2224 - val_loss: 92.8053\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 115.2153 - val_loss: 92.3511\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 118.5890 - val_loss: 93.8975\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 106.2785 - val_loss: 92.8464\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 110.7265 - val_loss: 92.8594\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 107.8529 - val_loss: 92.2557\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 111.4620 - val_loss: 92.9608\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 111.1207 - val_loss: 95.4839\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 112.1969 - val_loss: 92.3370\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 108.2908 - val_loss: 90.4180\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 108.6955 - val_loss: 90.5200\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 107.2822 - val_loss: 89.8054\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 109.6533 - val_loss: 89.9213\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 102.1250 - val_loss: 89.6816\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.7372 - val_loss: 91.9743\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 105.7120 - val_loss: 87.9380\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 106.4221 - val_loss: 91.4754\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 103.8876 - val_loss: 86.8150\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 104.1479 - val_loss: 84.9420\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 103.8890 - val_loss: 84.2190\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 105.5086 - val_loss: 90.0635\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 104.2573 - val_loss: 85.1193\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 104.9343 - val_loss: 85.8087\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 102.0560 - val_loss: 87.2996\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 101.2841 - val_loss: 88.5339\n",
      "Fold 1\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4667.7040 - val_loss: 4703.0165\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4465.3607 - val_loss: 4351.8697\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3930.3594 - val_loss: 3535.6204\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 2893.9457 - val_loss: 2248.7289\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1647.0550 - val_loss: 1199.9655\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1024.2495 - val_loss: 865.5656\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 767.8163 - val_loss: 654.8630\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 601.5732 - val_loss: 522.0843\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 482.5824 - val_loss: 420.5019\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 428.4722 - val_loss: 357.7609\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 387.5897 - val_loss: 318.0688\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 357.9018 - val_loss: 289.0264\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 330.3932 - val_loss: 267.2426\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 310.2478 - val_loss: 252.8560\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 296.4046 - val_loss: 242.0721\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 285.2358 - val_loss: 230.7206\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 278.6968 - val_loss: 222.7336\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 273.7382 - val_loss: 212.8038\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 253.9556 - val_loss: 204.2475\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 235.6870 - val_loss: 197.4845\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 241.2457 - val_loss: 187.1947\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 230.3985 - val_loss: 178.5693\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 219.9770 - val_loss: 169.2788\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 213.5371 - val_loss: 159.9852\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 199.2237 - val_loss: 150.8307\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 195.7340 - val_loss: 144.2900\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 183.8086 - val_loss: 137.4144\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 179.4054 - val_loss: 134.0998\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 178.9019 - val_loss: 129.2801\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 160.7407 - val_loss: 126.4414\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 174.9727 - val_loss: 124.9730\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 162.1593 - val_loss: 123.1290\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 164.3505 - val_loss: 121.2228\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 157.8778 - val_loss: 118.9478\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 153.6529 - val_loss: 117.3267\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 147.7329 - val_loss: 115.0530\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 159.6833 - val_loss: 113.6392\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 148.1676 - val_loss: 112.5898\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 142.5814 - val_loss: 111.9792\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 146.3120 - val_loss: 109.8455\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 146.7289 - val_loss: 109.6402\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 139.4747 - val_loss: 108.8552\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 135.3921 - val_loss: 108.1405\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 140.9861 - val_loss: 106.8532\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 133.5303 - val_loss: 105.7965\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 140.8455 - val_loss: 105.1964\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 125.8469 - val_loss: 104.8869\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 137.4922 - val_loss: 105.3078\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 134.5365 - val_loss: 105.4887\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 135.2666 - val_loss: 106.4181\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 131.2552 - val_loss: 105.3327\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 125.4470 - val_loss: 103.9191\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 130.4294 - val_loss: 102.9629\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 129.3433 - val_loss: 102.1881\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 124.6093 - val_loss: 100.8968\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 131.2524 - val_loss: 100.5565\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 129.6080 - val_loss: 100.9115\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 122.8859 - val_loss: 101.0672\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 122.0423 - val_loss: 99.4859\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 127.9640 - val_loss: 98.2821\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 119.8024 - val_loss: 99.4905\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 119.9744 - val_loss: 97.9032\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 120.8982 - val_loss: 97.8996\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 114.8312 - val_loss: 96.7076\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 120.6393 - val_loss: 97.4722\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 118.5525 - val_loss: 96.9715\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 119.8882 - val_loss: 96.2566\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 116.0455 - val_loss: 96.9169\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 110.3638 - val_loss: 96.5894\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 119.3247 - val_loss: 94.0773\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.0894 - val_loss: 93.1979\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 114.1282 - val_loss: 93.8057\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 115.5467 - val_loss: 91.9086\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 114.4644 - val_loss: 92.6957\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 115.7478 - val_loss: 91.9154\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 111.0712 - val_loss: 92.5960\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 113.1139 - val_loss: 92.5831\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 114.2932 - val_loss: 91.6535\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 106.5626 - val_loss: 91.4921\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 111.0587 - val_loss: 91.6757\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 107.8344 - val_loss: 91.5114\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 111.7787 - val_loss: 91.5896\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 104.7042 - val_loss: 91.0780\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 106.1407 - val_loss: 91.7440\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 108.7030 - val_loss: 90.9294\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 106.5040 - val_loss: 91.5978\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 113.4763 - val_loss: 89.4336\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 107.3287 - val_loss: 91.4039\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 108.0307 - val_loss: 89.9113\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 109.1342 - val_loss: 89.0300\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 111.6898 - val_loss: 90.6646\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 102.1665 - val_loss: 89.1648\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 100.3341 - val_loss: 89.3472\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 101.8794 - val_loss: 90.6743\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 103.3956 - val_loss: 88.1019\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 110.4989 - val_loss: 90.7075\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 104.1985 - val_loss: 88.7325\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 98.2038 - val_loss: 88.0959\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 101.2244 - val_loss: 87.8376\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 100.7835 - val_loss: 86.6219\n",
      "Fold 2\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4561.4532 - val_loss: 4750.5541\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4392.2985 - val_loss: 4460.7461\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3938.0222 - val_loss: 3757.0508\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3005.9386 - val_loss: 2565.0992\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1821.2718 - val_loss: 1430.13382044.\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1104.9248 - val_loss: 941.0561\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 803.7285 - val_loss: 724.3488\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 613.2543 - val_loss: 594.1773\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 524.9068 - val_loss: 481.5924\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 437.9370 - val_loss: 416.8867\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 393.8429 - val_loss: 369.8317\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 366.1070 - val_loss: 337.7902\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 333.9424 - val_loss: 303.6076\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 318.5348 - val_loss: 290.5592\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 290.8840 - val_loss: 267.6768\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 280.4235 - val_loss: 252.0221\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 265.6561 - val_loss: 245.0255\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 268.9152 - val_loss: 231.2394\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 253.1333 - val_loss: 220.8123\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 238.2172 - val_loss: 207.6477\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 228.8043 - val_loss: 194.8839\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 228.8907 - val_loss: 186.1930\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 197.5616 - val_loss: 174.7377\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 189.6827 - val_loss: 164.5114\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 192.3108 - val_loss: 162.3670\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 190.4593 - val_loss: 155.9794\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 175.4367 - val_loss: 151.7502\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 183.7709 - val_loss: 146.3706\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 173.4491 - val_loss: 144.2147\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 167.3455 - val_loss: 141.4658\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 164.0585 - val_loss: 138.5176\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 162.7091 - val_loss: 136.6150\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 159.2024 - val_loss: 139.9483\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 155.4444 - val_loss: 132.2293\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 155.2941 - val_loss: 132.5453\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 144.2525 - val_loss: 130.5052\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 148.2325 - val_loss: 130.0585\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 146.4149 - val_loss: 129.1485\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 147.8810 - val_loss: 128.1575\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 142.1051 - val_loss: 124.6401\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 144.7665 - val_loss: 127.9571\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 153.0161 - val_loss: 122.0400\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 146.0846 - val_loss: 124.4153\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 139.7934 - val_loss: 121.0209\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 134.0579 - val_loss: 125.7947\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 134.4913 - val_loss: 118.8272\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 142.6298 - val_loss: 121.0540\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 133.6099 - val_loss: 123.2915\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 130.2103 - val_loss: 118.9706\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 126.8065 - val_loss: 119.6721\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 130.3092 - val_loss: 120.4533\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 130.7512 - val_loss: 115.7854\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 120.5768 - val_loss: 117.2649\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 128.6807 - val_loss: 115.8351\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 127.7445 - val_loss: 113.6157\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 130.4411 - val_loss: 114.6029\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 124.0644 - val_loss: 114.1236\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 120.8488 - val_loss: 112.9613\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 119.9951 - val_loss: 112.3705\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 127.2782 - val_loss: 113.8438\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 122.1985 - val_loss: 114.3501\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 122.8530 - val_loss: 112.8491\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 123.2283 - val_loss: 111.6730\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 117.1869 - val_loss: 111.9229\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 109.7939 - val_loss: 112.3414\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 112.0619 - val_loss: 108.8717\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 111.9464 - val_loss: 110.7687\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 117.7913 - val_loss: 109.7366\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 117.7553 - val_loss: 110.6776\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 110.8256 - val_loss: 108.8364\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.2831 - val_loss: 108.8778\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 117.0628 - val_loss: 106.6300\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 115.2681 - val_loss: 107.3921\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 114.3448 - val_loss: 106.4201\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 105.2188 - val_loss: 105.7909\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 109.0645 - val_loss: 105.0423\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 111.4414 - val_loss: 106.6953\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 115.5860 - val_loss: 105.6495\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 110.8976 - val_loss: 104.7265\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 111.1662 - val_loss: 105.9267\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 109.3507 - val_loss: 103.7907\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 107.2699 - val_loss: 106.5498\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 108.2479 - val_loss: 108.5365\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 113.8969 - val_loss: 102.3650\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 103.6714 - val_loss: 103.5086\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 100.1145 - val_loss: 102.3107\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 107.6916 - val_loss: 103.1564\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 101.9172 - val_loss: 104.3078\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 102.4762 - val_loss: 102.4571\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.6236 - val_loss: 102.3928\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 103.2239 - val_loss: 103.7197\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 104.1986 - val_loss: 102.5619\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 99.8103 - val_loss: 101.2557\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 104.9861 - val_loss: 102.6072\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 99.9725 - val_loss: 102.9607\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 105.6094 - val_loss: 99.5945\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 107.3039 - val_loss: 103.6705\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 101.2857 - val_loss: 100.9786\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 101.6940 - val_loss: 101.3194\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 101.5891 - val_loss: 99.3880\n",
      "Fold 3\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4659.6294 - val_loss: 4517.3850\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4504.3169 - val_loss: 4256.7005\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4066.7032 - val_loss: 3599.1973\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3145.2579 - val_loss: 2462.0438\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1924.0442 - val_loss: 1363.5372\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1135.2838 - val_loss: 910.5405\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 812.2250 - val_loss: 696.6948\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 648.1728 - val_loss: 563.7726\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 535.6470 - val_loss: 457.1715\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 452.1432 - val_loss: 385.3471\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 404.8978 - val_loss: 339.7868\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 360.6287 - val_loss: 307.8616\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 339.9667 - val_loss: 281.7125\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 328.0847 - val_loss: 262.0331\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 305.9513 - val_loss: 241.8550\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 297.7659 - val_loss: 230.4316\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 273.6988 - val_loss: 216.2693\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 256.7508 - val_loss: 207.9887\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 255.0968 - val_loss: 198.9484\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 241.5962 - val_loss: 189.8802\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 231.3172 - val_loss: 179.4572\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 226.9230 - val_loss: 173.6735\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 212.9660 - val_loss: 164.6026\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 202.2192 - val_loss: 157.4175\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 198.9920 - val_loss: 148.1431\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 182.9888 - val_loss: 141.6970\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 182.3726 - val_loss: 137.9063\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 179.8147 - val_loss: 137.3593\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 162.8733 - val_loss: 129.8383\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 168.9573 - val_loss: 128.2208: 173.95 - ETA: 0s - loss: 171.222\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 166.8833 - val_loss: 126.7105\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 166.9809 - val_loss: 125.3909\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 163.0879 - val_loss: 124.6523\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 158.2623 - val_loss: 120.2279\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 157.6064 - val_loss: 122.5402\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 151.4503 - val_loss: 117.0512\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 143.3432 - val_loss: 116.4519\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 145.5523 - val_loss: 115.8898\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 152.8682 - val_loss: 114.5363\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 152.9077 - val_loss: 111.6395\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 144.2571 - val_loss: 112.1791\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 142.9322 - val_loss: 110.6159\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 141.3125 - val_loss: 110.0932\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 135.3025 - val_loss: 109.7531\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 139.0961 - val_loss: 110.5549\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 139.8776 - val_loss: 108.2969\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 136.7304 - val_loss: 113.5132\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 127.3299 - val_loss: 106.3025\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 136.3339 - val_loss: 108.1835\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 133.3990 - val_loss: 103.7366\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 129.6637 - val_loss: 106.5759\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 127.5254 - val_loss: 104.7675\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 132.2351 - val_loss: 103.3879\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 134.5150 - val_loss: 103.2043\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 126.1098 - val_loss: 101.8478\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 117.5494 - val_loss: 101.7829\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 133.8056 - val_loss: 101.9528\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 125.3172 - val_loss: 100.1262\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 120.7315 - val_loss: 101.1850\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 122.4761 - val_loss: 99.8698\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 125.5269 - val_loss: 99.5905\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 115.4527 - val_loss: 98.4324\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 126.2173 - val_loss: 99.6028\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 120.4664 - val_loss: 97.0634\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 119.6387 - val_loss: 98.6210\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 118.0745 - val_loss: 96.9664\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 112.5888 - val_loss: 93.8096\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 114.0672 - val_loss: 97.6500\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 114.8260 - val_loss: 95.9619\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 110.0272 - val_loss: 96.4171\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.6112 - val_loss: 96.1166\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 112.1138 - val_loss: 95.7657\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 110.3059 - val_loss: 95.1692\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 117.9345 - val_loss: 97.0277\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 114.5213 - val_loss: 96.4393\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 115.8963 - val_loss: 96.6044\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 114.3094 - val_loss: 95.6948\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 105.7911 - val_loss: 95.0974\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 109.5286 - val_loss: 93.8298\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 109.3982 - val_loss: 92.4208\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 108.3238 - val_loss: 92.8237\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 112.9447 - val_loss: 92.9711\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 113.8448 - val_loss: 94.2258\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 106.4842 - val_loss: 92.0957\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 103.4633 - val_loss: 90.7264\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 109.3379 - val_loss: 90.5027\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 102.7414 - val_loss: 91.6341\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 109.2562 - val_loss: 91.1635\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 104.1715 - val_loss: 89.5310\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.7760 - val_loss: 93.5970\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 108.4033 - val_loss: 88.1100\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 99.9842 - val_loss: 90.4759\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 105.4798 - val_loss: 90.1501\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 98.9645 - val_loss: 91.2668\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 104.9930 - val_loss: 91.5581\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 106.6334 - val_loss: 87.1761\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 99.7803 - val_loss: 87.2458\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 99.8949 - val_loss: 87.5604\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 103.8974 - val_loss: 88.7255\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 100.9595 - val_loss: 86.9961\n",
      "Fold 4\n",
      "Train on 809 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 0s - loss: 4709.8958 - val_loss: 4328.4418\n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 4545.6195 - val_loss: 4042.9103\n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 4086.0087 - val_loss: 3352.0183\n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 3159.1504 - val_loss: 2204.0706\n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 1933.6068 - val_loss: 1177.4936\n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 1158.5693 - val_loss: 775.4289\n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 814.4858 - val_loss: 546.1533\n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 658.9352 - val_loss: 422.5983\n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 527.6782 - val_loss: 345.6455\n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 434.3515 - val_loss: 300.5253\n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 382.2034 - val_loss: 272.1679\n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 363.7057 - val_loss: 250.9049\n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 333.2821 - val_loss: 236.5287\n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 322.6051 - val_loss: 223.4058\n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 301.9060 - val_loss: 213.4477\n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 303.2105 - val_loss: 205.9170\n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 281.8691 - val_loss: 196.8300\n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 269.2022 - val_loss: 190.5836\n",
      "Epoch 19/100\n",
      "809/809 [==============================] - 0s - loss: 269.3947 - val_loss: 181.6825\n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 266.2401 - val_loss: 172.6825\n",
      "Epoch 21/100\n",
      "809/809 [==============================] - 0s - loss: 246.9815 - val_loss: 167.5511\n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 237.9934 - val_loss: 156.6410\n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 227.8983 - val_loss: 147.0431\n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 209.9623 - val_loss: 137.0303\n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 202.8689 - val_loss: 129.8829\n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 208.6081 - val_loss: 120.9383\n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 193.0405 - val_loss: 115.2023\n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 185.4166 - val_loss: 111.6078\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 170.4553 - val_loss: 106.3502\n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 178.5359 - val_loss: 104.9428\n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 179.0472 - val_loss: 102.0599\n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 167.9920 - val_loss: 100.9409\n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 167.4525 - val_loss: 99.1543\n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 159.6613 - val_loss: 96.1683\n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 165.8499 - val_loss: 95.2958\n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 159.8605 - val_loss: 95.6085\n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 151.0703 - val_loss: 95.0255\n",
      "Epoch 38/100\n",
      "809/809 [==============================] - 0s - loss: 155.7174 - val_loss: 95.6901\n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 157.5469 - val_loss: 91.5722\n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 158.9979 - val_loss: 91.4098\n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 153.6677 - val_loss: 90.0183\n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 153.9337 - val_loss: 89.8406\n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 152.8657 - val_loss: 90.2600\n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 139.5044 - val_loss: 87.2658\n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 150.2569 - val_loss: 89.1828\n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 144.8902 - val_loss: 86.7195\n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 149.2905 - val_loss: 86.4434\n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 140.9986 - val_loss: 84.3948\n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 137.5397 - val_loss: 83.8774\n",
      "Epoch 50/100\n",
      "809/809 [==============================] - 0s - loss: 136.6743 - val_loss: 83.6312\n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 143.9925 - val_loss: 82.7782\n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 129.7636 - val_loss: 84.0863\n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 143.0705 - val_loss: 81.8828\n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 132.9166 - val_loss: 81.3693\n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 131.1698 - val_loss: 82.9902\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 133.0080 - val_loss: 81.3472\n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 131.6749 - val_loss: 80.6923\n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 136.5414 - val_loss: 81.2433\n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 132.1188 - val_loss: 80.9572\n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 135.4400 - val_loss: 81.4201\n",
      "Epoch 61/100\n",
      "809/809 [==============================] - 0s - loss: 131.5834 - val_loss: 78.9112\n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 127.4339 - val_loss: 79.3915\n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 128.6503 - val_loss: 78.1683\n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 120.3752 - val_loss: 76.9014\n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 124.0933 - val_loss: 78.7252\n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 120.8820 - val_loss: 77.5295\n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 126.7966 - val_loss: 76.5681\n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 126.2810 - val_loss: 77.4910\n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 125.8501 - val_loss: 77.7882\n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 116.1419 - val_loss: 77.6936\n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 121.0682 - val_loss: 78.2609\n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 114.9232 - val_loss: 75.1610\n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 116.2283 - val_loss: 75.1190\n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 111.3659 - val_loss: 79.8599\n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 119.6955 - val_loss: 74.7567\n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 115.8641 - val_loss: 75.0045\n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 111.8685 - val_loss: 73.6494\n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 122.6598 - val_loss: 73.7541\n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 119.0854 - val_loss: 73.8401\n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 112.8904 - val_loss: 74.2809\n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 113.3239 - val_loss: 73.5218\n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 112.6807 - val_loss: 72.5215\n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 111.9385 - val_loss: 72.4924\n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 115.9394 - val_loss: 72.1265\n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 118.2744 - val_loss: 72.4520\n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 111.1091 - val_loss: 71.7772\n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 112.8202 - val_loss: 72.5548\n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 117.3933 - val_loss: 70.9232\n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 113.7544 - val_loss: 72.8804\n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 112.2343 - val_loss: 71.9317\n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 112.5332 - val_loss: 70.8824\n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 108.3940 - val_loss: 72.5073\n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 111.7654 - val_loss: 69.6693\n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 112.1406 - val_loss: 71.5999\n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 106.9034 - val_loss: 69.7198\n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 106.2016 - val_loss: 71.1750\n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 112.1195 - val_loss: 69.6313\n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 105.4604 - val_loss: 70.6459\n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 101.3325 - val_loss: 69.5780\n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 109.4718 - val_loss: 68.1884\n",
      "1 <function regression at 0x0000016E1020A268>\n",
      "2 <bound method SupervisedFloatMixin.fit of KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "          weights='uniform')>\n",
      "3 <bound method DecisionTreeRegressor.fit of DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')>\n",
      "4 <bound method DecisionTreeRegressor.fit of ExtraTreeRegressor(criterion='mse', max_depth=7, max_features='auto',\n",
      "          max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "          min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, random_state=None,\n",
      "          splitter='random')>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for j, clf in enumerate(clfs):    \n",
    "    print(j, clf)\n",
    "    blend_test_j = np.zeros(shape=(5, len(testX),2))\n",
    "    for i, (train_index, test_index) in enumerate(skf):\n",
    "        print(\"Fold\", i)   \n",
    "\n",
    "        X_train = trainX[train_index]\n",
    "        y_train = trainY[train_index]\n",
    "        X_test = trainX[test_index]\n",
    "        y_test = trainY[test_index]\n",
    "\n",
    "    #     model1 = rr.train_model(X_train,y_train)\n",
    "        model = clf(X_train,y_train)\n",
    "        blend_train[j, test_index, :] =  model.predict(X_test)\n",
    "        blend_test_j[:,:,i] =   model.predict(testX)\n",
    "        \n",
    "\n",
    "    blend_test[:,:,j] = blend_test_j.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 49.99036026,   0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ 34.67944336,   0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_train[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-0851c735d657>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mneig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mneig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblend_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpred3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblend_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0me3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    739\u001b[0m         \"\"\"\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 405\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "# stacking model 3\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neig = KNeighborsRegressor(n_neighbors=4)\n",
    "neig.fit(blend_train,trainY)\n",
    "pred3 = neig.predict(blend_test)\n",
    "e3, a3 = accuracy(pred3, test_y)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((3,2,5))\n",
    "print(a)\n",
    "print(a[:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [1, 9],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.array([[2,0],[1,9],[3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
