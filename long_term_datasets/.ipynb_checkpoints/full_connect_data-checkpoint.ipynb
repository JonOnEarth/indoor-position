{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this full connected deep learning model is no difference from the short-term-data's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = 'C:/software/WinPython/notebooks/indoor position/long_term_datasets'\n",
    "train_rss = pd.read_csv(path + '/train_rss.csv',header  = None)\n",
    "train_coord = pd.read_csv(path + '/train_coords.csv', header  = None)\n",
    "\n",
    "test_rss = pd.read_csv(path + '/test_rss.csv',header  = None)\n",
    "test_coord = pd.read_csv(path + '/test_coords.csv',header  = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16704, 448)\n",
      "(46800, 448)\n"
     ]
    }
   ],
   "source": [
    "print(train_rss.shape)\n",
    "print(test_rss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-90.    -62.    -69.    ...,  12.904  29.208   3.   ]\n",
      " [-93.    -65.    -67.    ...,  12.904  29.208   3.   ]\n",
      " [-88.    -64.    -68.    ...,  12.904  29.208   3.   ]\n",
      " ..., \n",
      " [  0.      0.      0.    ...,  12.904  29.208   5.   ]\n",
      " [  0.      0.      0.    ...,  12.904  29.208   5.   ]\n",
      " [  0.      0.      0.    ...,  12.904  29.208   5.   ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.concat([train_rss, train_coord], axis=1, ignore_index=True)\n",
    "test = pd.concat([test_rss, test_coord], axis=1, ignore_index=True)\n",
    "train_replace = train.replace(100,0)\n",
    "train_ar = train_replace.values\n",
    "test_replace = test.replace(100,0)\n",
    "test_ar = test_replace.values\n",
    "print(train_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3=train_ar[train_ar[:,-1]==3]\n",
    "train3_rss = train3[:, 0:448]\n",
    "train3_coord = train3[:,448:450]\n",
    "\n",
    "test3=test_ar[test_ar[:,-1]==3]\n",
    "test3_rss = test3[0:5000, 0:448]\n",
    "test3_coord = test3[0:5000, 448:450]\n",
    "\n",
    "\n",
    "train5 = train_ar[train_ar[:,-1]==5]\n",
    "train5_rss = train5[:, 0:448]\n",
    "train5_coord = train5[:, 448:450]\n",
    "\n",
    "test5=test_ar[test_ar[:,-1]==5]\n",
    "test5_rss = test5[0:5000, 0:448]\n",
    "test5_coord = test5[0:5000, 448:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-87. -54. -57. ...,   0.   0.   0.]\n",
      " [  0. -54. -55. ...,   0.   0.   0.]\n",
      " [-88. -52. -56. ...,   0.   0.   0.]\n",
      " ..., \n",
      " [  0. -62. -66. ...,   0.   0.   0.]\n",
      " [  0. -64. -62. ...,   0.   0.   0.]\n",
      " [  0. -61. -61. ...,   0.   0.   0.]] [[ 12.904   29.208 ]\n",
      " [ 12.904   29.208 ]\n",
      " [ 12.904   29.208 ]\n",
      " ..., \n",
      " [  9.5149  27.419 ]\n",
      " [  5.1254  27.419 ]\n",
      " [  5.1254  27.419 ]]\n",
      "(8352, 448)\n"
     ]
    }
   ],
   "source": [
    "print(test3_rss, test3_coord)\n",
    "print(train5_rss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Input\n",
    "\n",
    "def predata(rss, locations):\n",
    "    # the origin of the room\n",
    "    origin = np.amin(locations,axis=0)\n",
    "    #size of the room\n",
    "    room_size = np.amax(locations, axis=0)-origin\n",
    "    # position respect to origin\n",
    "    train_Yy = locations - origin\n",
    "    train_Xx = np.asarray(rss, dtype=np.float64) \n",
    "    return train_Xx, train_Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(rss, locations):\n",
    "    train_Xx, train_Yy = predata(rss, locations)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_Xx, train_Yy, test_size=0.25)\n",
    "    return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    error = np.sqrt(np.sum((predictions - labels)**2, 1))\n",
    "    return error, np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(rss, locations, test_rss, test_locations):\n",
    "    # get train_X, val_X, train_Y, val_Y\n",
    "    train_X, val_X, train_Y, val_Y = train_val(rss, locations)\n",
    "    test_X, test_Y = predata(test_rss, test_locations)\n",
    "    \n",
    "    # parameters\n",
    "    num_input = train_X.shape[1]# input layer size\n",
    "    act_fun = 'relu'\n",
    "    regularzation_penalty = 0.03\n",
    "    initilization_method = 'he_normal' #'random_uniform' ,'random_normal','TruncatedNormal' ,'glorot_uniform', 'glorot_nomral', 'he_normal', 'he_uniform'\n",
    "    #Optimizer\n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    # define model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=act_fun, input_dim=num_input, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation=act_fun, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(128, activation=act_fun, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='linear', kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "\n",
    "    #Model compile\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=adam)\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "    model.fit(train_X, train_Y,\n",
    "              epochs=100,\n",
    "              batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n",
    "    #model evaluate\n",
    "    train_loss = model.evaluate(train_X,train_Y, batch_size=len(train_Y)) #calculate the data in test mode(Keras)\n",
    "    val_loss = model.evaluate(val_X, val_Y, batch_size=len(val_Y))\n",
    "    test_loss = model.evaluate(test_X, test_Y, batch_size=len(test_Y))\n",
    "    print(\"Loss for training data is\",train_loss)\n",
    "    print(\"Loss for validation data is\",val_loss)\n",
    "    print(\"Loss for test data is\",test_loss)\n",
    "    predict_Y = model.predict(test_X)\n",
    "    error_t, accuracy_t = accuracy(predict_Y, test_Y)\n",
    "    return predict_Y, error_t, accuracy_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6264 samples, validate on 2088 samples\n",
      "Epoch 1/100\n",
      "6264/6264 [==============================] - 0s - loss: 194.1658 - val_loss: 27.8117\n",
      "Epoch 2/100\n",
      "6264/6264 [==============================] - 0s - loss: 52.9678 - val_loss: 24.5952\n",
      "Epoch 3/100\n",
      "6264/6264 [==============================] - 0s - loss: 34.8002 - val_loss: 23.3429\n",
      "Epoch 4/100\n",
      "6264/6264 [==============================] - 0s - loss: 29.0777 - val_loss: 21.2888\n",
      "Epoch 5/100\n",
      "6264/6264 [==============================] - 0s - loss: 24.9774 - val_loss: 19.5265\n",
      "Epoch 6/100\n",
      "6264/6264 [==============================] - 0s - loss: 23.0734 - val_loss: 20.2018\n",
      "Epoch 7/100\n",
      "6264/6264 [==============================] - 0s - loss: 21.3448 - val_loss: 18.3024\n",
      "Epoch 8/100\n",
      "6264/6264 [==============================] - 0s - loss: 20.2751 - val_loss: 18.0328\n",
      "Epoch 9/100\n",
      "6264/6264 [==============================] - 0s - loss: 19.1352 - val_loss: 16.8172\n",
      "Epoch 10/100\n",
      "6264/6264 [==============================] - 0s - loss: 18.1646 - val_loss: 16.8119\n",
      "Epoch 11/100\n",
      "6264/6264 [==============================] - 0s - loss: 17.4464 - val_loss: 15.8193\n",
      "Epoch 12/100\n",
      "6264/6264 [==============================] - 0s - loss: 16.8713 - val_loss: 15.6287\n",
      "Epoch 13/100\n",
      "6264/6264 [==============================] - 0s - loss: 16.5939 - val_loss: 15.1436\n",
      "Epoch 14/100\n",
      "6264/6264 [==============================] - 0s - loss: 15.9260 - val_loss: 14.6321\n",
      "Epoch 15/100\n",
      "6264/6264 [==============================] - 0s - loss: 15.4753 - val_loss: 13.2025\n",
      "Epoch 16/100\n",
      "6264/6264 [==============================] - 0s - loss: 15.1891 - val_loss: 14.0274\n",
      "Epoch 17/100\n",
      "6264/6264 [==============================] - 0s - loss: 14.8320 - val_loss: 13.8845\n",
      "Epoch 18/100\n",
      "6264/6264 [==============================] - 0s - loss: 14.5610 - val_loss: 13.1930\n",
      "Epoch 19/100\n",
      "6264/6264 [==============================] - 0s - loss: 14.3106 - val_loss: 13.2219\n",
      "Epoch 20/100\n",
      "6264/6264 [==============================] - 0s - loss: 13.8586 - val_loss: 12.2359\n",
      "Epoch 21/100\n",
      "6264/6264 [==============================] - 0s - loss: 13.6551 - val_loss: 12.1142\n",
      "Epoch 22/100\n",
      "6264/6264 [==============================] - 0s - loss: 13.3182 - val_loss: 12.2537\n",
      "Epoch 23/100\n",
      "6264/6264 [==============================] - 0s - loss: 13.1095 - val_loss: 12.2905\n",
      "Epoch 24/100\n",
      "6264/6264 [==============================] - 0s - loss: 12.8842 - val_loss: 11.7252\n",
      "Epoch 25/100\n",
      "6264/6264 [==============================] - 0s - loss: 12.5030 - val_loss: 11.7799\n",
      "Epoch 26/100\n",
      "6264/6264 [==============================] - 0s - loss: 12.2739 - val_loss: 11.2516\n",
      "Epoch 27/100\n",
      "6264/6264 [==============================] - 0s - loss: 12.3502 - val_loss: 10.9041\n",
      "Epoch 28/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.9665 - val_loss: 10.3752\n",
      "Epoch 29/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.9253 - val_loss: 11.0053\n",
      "Epoch 30/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.7260 - val_loss: 10.0265\n",
      "Epoch 31/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.4632 - val_loss: 10.6889\n",
      "Epoch 32/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.2239 - val_loss: 9.9404\n",
      "Epoch 33/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.1260 - val_loss: 10.1819\n",
      "Epoch 34/100\n",
      "6264/6264 [==============================] - 0s - loss: 11.0611 - val_loss: 9.9490\n",
      "Epoch 35/100\n",
      "6264/6264 [==============================] - 0s - loss: 10.7918 - val_loss: 9.6991\n",
      "Epoch 36/100\n",
      "6264/6264 [==============================] - 0s - loss: 10.6260 - val_loss: 9.6771\n",
      "Epoch 37/100\n",
      "6264/6264 [==============================] - 0s - loss: 10.4815 - val_loss: 9.5854\n",
      "Epoch 38/100\n",
      "6264/6264 [==============================] - 0s - loss: 10.3178 - val_loss: 9.2972\n",
      "Epoch 39/100\n",
      "6264/6264 [==============================] - 0s - loss: 10.1317 - val_loss: 8.9968\n",
      "Epoch 40/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.8558 - val_loss: 8.8741\n",
      "Epoch 41/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.8681 - val_loss: 8.3469\n",
      "Epoch 42/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.6369 - val_loss: 8.0404\n",
      "Epoch 43/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.5376 - val_loss: 8.0777\n",
      "Epoch 44/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.2274 - val_loss: 8.3407\n",
      "Epoch 45/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.2568 - val_loss: 8.3841\n",
      "Epoch 46/100\n",
      "6264/6264 [==============================] - 0s - loss: 9.1695 - val_loss: 7.9412\n",
      "Epoch 47/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.9334 - val_loss: 7.3816\n",
      "Epoch 48/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.8185 - val_loss: 7.4667\n",
      "Epoch 49/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.8158 - val_loss: 8.1105\n",
      "Epoch 50/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.6390 - val_loss: 7.1767\n",
      "Epoch 51/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.4304 - val_loss: 7.0523\n",
      "Epoch 52/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.3146 - val_loss: 6.3588\n",
      "Epoch 53/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.1451 - val_loss: 6.7414\n",
      "Epoch 54/100\n",
      "6264/6264 [==============================] - 0s - loss: 8.1540 - val_loss: 7.2392\n",
      "Epoch 55/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.9382 - val_loss: 7.0569\n",
      "Epoch 56/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.9120 - val_loss: 6.5233\n",
      "Epoch 57/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.7253 - val_loss: 6.6346\n",
      "Epoch 58/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.6995 - val_loss: 6.6127\n",
      "Epoch 59/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.5492 - val_loss: 6.8166\n",
      "Epoch 60/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.4476 - val_loss: 5.9128\n",
      "Epoch 61/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.2790 - val_loss: 6.3839\n",
      "Epoch 62/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.2025 - val_loss: 5.9432\n",
      "Epoch 63/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.0850 - val_loss: 6.0715\n",
      "Epoch 64/100\n",
      "6264/6264 [==============================] - 0s - loss: 7.0910 - val_loss: 5.9727\n",
      "Epoch 65/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.9708 - val_loss: 5.6566\n",
      "Epoch 66/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.8167 - val_loss: 5.6165\n",
      "Epoch 67/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.8478 - val_loss: 5.3988\n",
      "Epoch 68/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.7375 - val_loss: 5.8304\n",
      "Epoch 69/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.5245 - val_loss: 5.6203\n",
      "Epoch 70/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.5262 - val_loss: 5.4700\n",
      "Epoch 71/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.4023 - val_loss: 5.0198\n",
      "Epoch 72/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.3472 - val_loss: 5.3060\n",
      "Epoch 73/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.3255 - val_loss: 5.0221\n",
      "Epoch 74/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.1245 - val_loss: 4.9883\n",
      "Epoch 75/100\n",
      "6264/6264 [==============================] - 0s - loss: 6.0710 - val_loss: 5.1579\n",
      "Epoch 76/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.9823 - val_loss: 4.8805\n",
      "Epoch 77/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.9885 - val_loss: 4.9383\n",
      "Epoch 78/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.9339 - val_loss: 4.7218\n",
      "Epoch 79/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.8243 - val_loss: 4.6294\n",
      "Epoch 80/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.8349 - val_loss: 5.1729\n",
      "Epoch 81/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.7498 - val_loss: 4.5498\n",
      "Epoch 82/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.6632 - val_loss: 4.9948\n",
      "Epoch 83/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.7091 - val_loss: 4.6839\n",
      "Epoch 84/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.6302 - val_loss: 4.7769\n",
      "Epoch 85/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.3869 - val_loss: 4.6861\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6264/6264 [==============================] - 0s - loss: 5.5821 - val_loss: 4.6284\n",
      "Epoch 87/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.4087 - val_loss: 4.3749\n",
      "Epoch 88/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.4063 - val_loss: 4.8602\n",
      "Epoch 89/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.4360 - val_loss: 4.3189\n",
      "Epoch 90/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.3037 - val_loss: 4.6819\n",
      "Epoch 91/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.3377 - val_loss: 4.3749\n",
      "Epoch 92/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.2732 - val_loss: 4.4165\n",
      "Epoch 93/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.1764 - val_loss: 4.1866\n",
      "Epoch 94/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.1634 - val_loss: 4.2528\n",
      "Epoch 95/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.1797 - val_loss: 4.3028\n",
      "Epoch 96/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.2019 - val_loss: 4.1813\n",
      "Epoch 97/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.1254 - val_loss: 4.1296\n",
      "Epoch 98/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.0836 - val_loss: 4.1393\n",
      "Epoch 99/100\n",
      "6264/6264 [==============================] - 0s - loss: 5.1101 - val_loss: 4.3182\n",
      "Epoch 100/100\n",
      "6264/6264 [==============================] - 0s - loss: 4.9715 - val_loss: 4.4452\n",
      "6264/6264 [==============================] - 0s\n",
      "2088/2088 [==============================] - 0s\n",
      "5000/5000 [==============================] - 0s\n",
      "Loss for training data is 3.714189291\n",
      "Loss for validation data is 4.44521951675\n",
      "Loss for test data is 6.88192844391\n"
     ]
    }
   ],
   "source": [
    "predict5, error5, accuracy5 = train_model(train5_rss, train5_coord, test5_rss, test5_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error: 3.30178222565\n"
     ]
    }
   ],
   "source": [
    "print('The average error:', accuracy5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum error: 0.0343681916991 maximum error: 9.24152428984 variance: 2.75592794414 75%: 4.4298916034\n"
     ]
    }
   ],
   "source": [
    "print('minimum error:', np.amin(error5), 'maximum error:', np.amax(error5), 'variance:', \n",
    "      np.var(error5), \"75%:\", np.percentile(error5, 75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcKOGkPw5jBNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x211111dbc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_sorted= np.sort(error5)\n",
    "p = 1. *np.arange(len(error5))/(len(error5)-1)\n",
    "plt.plot(error_sorted, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(129*4)\n",
    "\n",
    "x = x.reshape(129, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "a = np.vsplit(x, np.arange(6, 129, 6))\n",
    "print(a[0].shape)\n",
    "\n",
    "print(a[-1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 12)\n"
     ]
    }
   ],
   "source": [
    "print(a[1].shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
