{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#test\n",
    "#load data\n",
    "path = 'C:/Users/mys12/Desktop/northeastern/summer2018/data_Robust_Fingerprinting-master/DISTRIBUTED_OPENSOURCE/FINGERPRINTING_DB'\n",
    "\n",
    "test_rss = pd.read_csv(path + '/Training_rss_21Aug17.csv', header = 0)\n",
    "test_coord = pd.read_csv(path + '/Training_coordinates_21Aug17.csv', header = 0)\n",
    "test_rss = test_rss.replace(100, 0)\n",
    "\n",
    "train_rss = pd.read_csv(path + '/Test_rss_21Aug17.csv', header = 0)\n",
    "train_coord = pd.read_csv(path + '/Test_coordinates_21Aug17.csv', header = 0)\n",
    "train_rss = train_rss.replace(100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>100.1</th>\n",
       "      <th>100.2</th>\n",
       "      <th>100.3</th>\n",
       "      <th>100.4</th>\n",
       "      <th>100.5</th>\n",
       "      <th>100.6</th>\n",
       "      <th>100.7</th>\n",
       "      <th>100.8</th>\n",
       "      <th>100.9</th>\n",
       "      <th>...</th>\n",
       "      <th>100.939</th>\n",
       "      <th>100.940</th>\n",
       "      <th>100.941</th>\n",
       "      <th>100.942</th>\n",
       "      <th>100.943</th>\n",
       "      <th>100.944</th>\n",
       "      <th>100.945</th>\n",
       "      <th>100.946</th>\n",
       "      <th>100.947</th>\n",
       "      <th>100.948</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64</td>\n",
       "      <td>0</td>\n",
       "      <td>-64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 992 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   100  100.1  100.2  100.3  100.4  100.5  100.6  100.7  100.8  100.9  \\\n",
       "0    0      0      0      0      0      0      0    -64      0    -64   \n",
       "1    0      0      0      0      0      0      0      0      0      0   \n",
       "2    0      0      0      0      0      0      0      0      0      0   \n",
       "3    0      0      0      0      0      0      0      0      0      0   \n",
       "4    0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "    ...     100.939  100.940  100.941  100.942  100.943  100.944  100.945  \\\n",
       "0   ...           0        0        0        0        0        0        0   \n",
       "1   ...           0        0        0        0        0        0        0   \n",
       "2   ...           0        0        0        0        0        0        0   \n",
       "3   ...           0        0        0        0        0        0        0   \n",
       "4   ...           0        0        0        0        0        0        0   \n",
       "\n",
       "   100.946  100.947  100.948  \n",
       "0        0        0        0  \n",
       "1        0        0        0  \n",
       "2        0        0        0  \n",
       "3        0        0        0  \n",
       "4        0        0        0  \n",
       "\n",
       "[5 rows x 992 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_rss, train_coord], axis=1, ignore_index=True)\n",
    "test = pd.concat([test_rss, test_coord], axis=1, ignore_index=True)\n",
    "train = np.asarray(train)\n",
    "test = np.asarray(test)\n",
    "\n",
    "# first floor\n",
    "train1 = train[train[:,-1]==0.0]\n",
    "normalizer = preprocessing.Normalizer(norm='l1').fit(train1[:,:-3])\n",
    "train1_r=normalizer.transform(train1[:,:-3])\n",
    "train1_c=train1[:,-3:-1]\n",
    "train1_r.shape\n",
    "\n",
    "test1 = test[test[:,-1]==0.0]\n",
    "test1_r=normalizer.transform(test1[:,:-3])\n",
    "test1_c=test1[:,-3:-1]\n",
    "\n",
    "\n",
    "# second floor\n",
    "train2 = train[train[:,-1]==3.7]\n",
    "normalizer2 = preprocessing.Normalizer(norm='l1').fit(train2[:,:-3])\n",
    "\n",
    "train2_r=normalizer2.transform(train2[:,:-3])\n",
    "train2_c=train2[:,-3:-1]\n",
    "\n",
    "test2 = test[test[:,-1]==3.7]\n",
    "test2_r=normalizer2.transform(test2[:,:-3])\n",
    "test2_c=test2[:,-3:-1]\n",
    "\n",
    "# third floor\n",
    "train3 = train[train[:,-1]==7.4]\n",
    "normalizer3 = preprocessing.Normalizer(norm='l1').fit(train3[:,:-3])\n",
    "\n",
    "train3_r=normalizer3.transform(train3[:,:-3])\n",
    "train3_c=train3[:,-3:-1]\n",
    "\n",
    "test3 = test[test[:,-1]==7.4]\n",
    "test3_r=normalizer3.transform(test2[:,:-3])\n",
    "test3_c=test2[:,-3:-1]\n",
    "\n",
    "# forth floor\n",
    "train4 = train[train[:,-1]==11.1]\n",
    "train4_r=train4[:,:-3]\n",
    "train4_c=train4[:,-3:-1]\n",
    "\n",
    "test4 = test[test[:,-1]==11.1]\n",
    "test4_r=scale(test4[:,:-3])\n",
    "test4_c=test4[:,-3:-1]\n",
    "\n",
    "# fifth floor\n",
    "train5 = train[train[:,-1]==14.8]\n",
    "train5_r=train5[:,:-3]\n",
    "train5_c=train5[:,-3:-1]\n",
    "\n",
    "test5 = test[test[:,-1]==14.8]\n",
    "test5_r=scale(test4[:,:-3])\n",
    "test5_c=test5[:,-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "train1_c[0:5]\n",
    "num_input = train1_r.shape[1]\n",
    "print(num_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata(rss, locations):\n",
    "    # the origin of the room\n",
    "    origin = np.amin(locations,axis=0)\n",
    "    #size of the room\n",
    "    room_size = np.amax(locations, axis=0)-origin\n",
    "    # position respect to origin\n",
    "    train_Yy = locations - origin\n",
    "    train_Xx = np.asarray(rss, dtype=np.float64)\n",
    "    return train_Xx, train_Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plotpre(train_r,train_c):\n",
    "#     train1_X, train1_Y = predata(train1_r, train1_c)\n",
    "#     x,y= train1_Y.T\n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(rss, locations):\n",
    "    train_Xx, train_Yy = predata(rss, locations)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_Xx, train_Yy, test_size=0.25)\n",
    "    return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    error = np.sqrt(np.sum((predictions - labels)**2, 1))\n",
    "    return error, np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(rss, locations, test_rss, test_locations):\n",
    "    # get train_X, val_X, train_Y, val_Y\n",
    "    train_X, val_X, train_Y, val_Y = train_val(rss, locations)\n",
    "    test_X, test_Y = predata(test_rss, test_locations)\n",
    "    \n",
    "    # parameters\n",
    "    num_input = train_X.shape[1]# input layer size\n",
    "    act_fun = 'relu'\n",
    "    regularzation_penalty = 0.03\n",
    "    initilization_method = 'he_normal' #'random_uniform' ,'random_normal','TruncatedNormal' ,'glorot_uniform', 'glorot_nomral', 'he_normal', 'he_uniform'\n",
    "    #Optimizer\n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    # define model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation=act_fun, input_dim=num_input, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation=act_fun, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(64, activation=act_fun, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='linear', kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "\n",
    "    #Model compile\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=adam)\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "    model.fit(train_X, train_Y,\n",
    "              epochs=500,\n",
    "              batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n",
    "    #model evaluate\n",
    "    train_loss = model.evaluate(train_X,train_Y, batch_size=len(train_Y)) #calculate the data in test mode(Keras)\n",
    "    val_loss = model.evaluate(val_X, val_Y, batch_size=len(val_Y))\n",
    "    test_loss = model.evaluate(test_X, test_Y, batch_size=len(test_Y))\n",
    "    print(\"Loss for training data is\",train_loss)\n",
    "    print(\"Loss for validation data is\",val_loss)\n",
    "    print(\"Loss for test data is\",test_loss)\n",
    "    predict_Y = model.predict(test_X)\n",
    "    error_t, accuracy_t = accuracy(predict_Y, test_Y)\n",
    "    return predict_Y, error_t, accuracy_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 948 samples, validate on 316 samples\n",
      "Epoch 1/500\n",
      "948/948 [==============================] - 1s - loss: 4646.0534 - val_loss: 4798.2892\n",
      "Epoch 2/500\n",
      "948/948 [==============================] - 0s - loss: 4592.8598 - val_loss: 4710.6440\n",
      "Epoch 3/500\n",
      "948/948 [==============================] - 0s - loss: 4459.8824 - val_loss: 4510.5530\n",
      "Epoch 4/500\n",
      "948/948 [==============================] - 0s - loss: 4192.4433 - val_loss: 4126.0115\n",
      "Epoch 5/500\n",
      "948/948 [==============================] - 0s - loss: 3718.6821 - val_loss: 3503.7213\n",
      "Epoch 6/500\n",
      "948/948 [==============================] - 0s - loss: 3021.7814 - val_loss: 2675.3728\n",
      "Epoch 7/500\n",
      "948/948 [==============================] - 0s - loss: 2197.8825 - val_loss: 1813.2845\n",
      "Epoch 8/500\n",
      "948/948 [==============================] - 0s - loss: 1496.1897 - val_loss: 1164.8608\n",
      "Epoch 9/500\n",
      "948/948 [==============================] - 0s - loss: 1032.1607 - val_loss: 782.2942\n",
      "Epoch 10/500\n",
      "948/948 [==============================] - 0s - loss: 767.0913 - val_loss: 569.7441\n",
      "Epoch 11/500\n",
      "948/948 [==============================] - 0s - loss: 638.1903 - val_loss: 458.2170\n",
      "Epoch 12/500\n",
      "948/948 [==============================] - 0s - loss: 546.0016 - val_loss: 406.1266\n",
      "Epoch 13/500\n",
      "948/948 [==============================] - 0s - loss: 513.2038 - val_loss: 364.7676\n",
      "Epoch 14/500\n",
      "948/948 [==============================] - 0s - loss: 477.4049 - val_loss: 336.0896\n",
      "Epoch 15/500\n",
      "948/948 [==============================] - 0s - loss: 434.3434 - val_loss: 313.0829\n",
      "Epoch 16/500\n",
      "948/948 [==============================] - 0s - loss: 426.4366 - val_loss: 293.4816\n",
      "Epoch 17/500\n",
      "948/948 [==============================] - 0s - loss: 409.6989 - val_loss: 279.2095\n",
      "Epoch 18/500\n",
      "948/948 [==============================] - 0s - loss: 386.2401 - val_loss: 268.3165\n",
      "Epoch 19/500\n",
      "948/948 [==============================] - 0s - loss: 378.1373 - val_loss: 257.9800\n",
      "Epoch 20/500\n",
      "948/948 [==============================] - 0s - loss: 359.3901 - val_loss: 249.6285\n",
      "Epoch 21/500\n",
      "948/948 [==============================] - 0s - loss: 348.6908 - val_loss: 243.7717\n",
      "Epoch 22/500\n",
      "948/948 [==============================] - 0s - loss: 344.4822 - val_loss: 239.0071\n",
      "Epoch 23/500\n",
      "948/948 [==============================] - 0s - loss: 343.0591 - val_loss: 235.5233\n",
      "Epoch 24/500\n",
      "948/948 [==============================] - 0s - loss: 332.5280 - val_loss: 231.9863\n",
      "Epoch 25/500\n",
      "948/948 [==============================] - 0s - loss: 326.9477 - val_loss: 229.7514\n",
      "Epoch 26/500\n",
      "948/948 [==============================] - 0s - loss: 336.1942 - val_loss: 227.2092\n",
      "Epoch 27/500\n",
      "948/948 [==============================] - 0s - loss: 329.8930 - val_loss: 225.8863\n",
      "Epoch 28/500\n",
      "948/948 [==============================] - 0s - loss: 324.7655 - val_loss: 223.2083\n",
      "Epoch 29/500\n",
      "948/948 [==============================] - 0s - loss: 333.5236 - val_loss: 221.5412\n",
      "Epoch 30/500\n",
      "948/948 [==============================] - 0s - loss: 315.5166 - val_loss: 220.7168\n",
      "Epoch 31/500\n",
      "948/948 [==============================] - 0s - loss: 314.5841 - val_loss: 219.8411\n",
      "Epoch 32/500\n",
      "948/948 [==============================] - 0s - loss: 309.6693 - val_loss: 218.4572\n",
      "Epoch 33/500\n",
      "948/948 [==============================] - 0s - loss: 313.1565 - val_loss: 217.6443\n",
      "Epoch 34/500\n",
      "948/948 [==============================] - 0s - loss: 305.8998 - val_loss: 216.2552\n",
      "Epoch 35/500\n",
      "948/948 [==============================] - 0s - loss: 318.7956 - val_loss: 215.8645\n",
      "Epoch 36/500\n",
      "948/948 [==============================] - 0s - loss: 309.3087 - val_loss: 214.0117\n",
      "Epoch 37/500\n",
      "948/948 [==============================] - 0s - loss: 312.9993 - val_loss: 213.4231\n",
      "Epoch 38/500\n",
      "948/948 [==============================] - 0s - loss: 303.3768 - val_loss: 211.9723\n",
      "Epoch 39/500\n",
      "948/948 [==============================] - 0s - loss: 299.2333 - val_loss: 211.3553\n",
      "Epoch 40/500\n",
      "948/948 [==============================] - 0s - loss: 290.7660 - val_loss: 210.5488\n",
      "Epoch 41/500\n",
      "948/948 [==============================] - 0s - loss: 287.4841 - val_loss: 210.3711\n",
      "Epoch 42/500\n",
      "948/948 [==============================] - 0s - loss: 289.6695 - val_loss: 208.5816\n",
      "Epoch 43/500\n",
      "948/948 [==============================] - 0s - loss: 293.1316 - val_loss: 207.5372\n",
      "Epoch 44/500\n",
      "948/948 [==============================] - 0s - loss: 287.8215 - val_loss: 207.5798\n",
      "Epoch 45/500\n",
      "948/948 [==============================] - 0s - loss: 285.0073 - val_loss: 206.1611\n",
      "Epoch 46/500\n",
      "948/948 [==============================] - 0s - loss: 290.8847 - val_loss: 205.7755\n",
      "Epoch 47/500\n",
      "948/948 [==============================] - 0s - loss: 285.2041 - val_loss: 204.6973\n",
      "Epoch 48/500\n",
      "948/948 [==============================] - 0s - loss: 287.3649 - val_loss: 204.4025\n",
      "Epoch 49/500\n",
      "948/948 [==============================] - 0s - loss: 284.0760 - val_loss: 202.3593\n",
      "Epoch 50/500\n",
      "948/948 [==============================] - 0s - loss: 287.8312 - val_loss: 201.7191\n",
      "Epoch 51/500\n",
      "948/948 [==============================] - 0s - loss: 288.9011 - val_loss: 200.7430\n",
      "Epoch 52/500\n",
      "948/948 [==============================] - 0s - loss: 286.0952 - val_loss: 199.8810\n",
      "Epoch 53/500\n",
      "948/948 [==============================] - 0s - loss: 283.2266 - val_loss: 199.2587\n",
      "Epoch 54/500\n",
      "948/948 [==============================] - 0s - loss: 285.1731 - val_loss: 198.8862\n",
      "Epoch 55/500\n",
      "948/948 [==============================] - 0s - loss: 277.0453 - val_loss: 197.8437\n",
      "Epoch 56/500\n",
      "948/948 [==============================] - 0s - loss: 276.0327 - val_loss: 196.9845\n",
      "Epoch 57/500\n",
      "948/948 [==============================] - 0s - loss: 266.9563 - val_loss: 195.7585\n",
      "Epoch 58/500\n",
      "948/948 [==============================] - 0s - loss: 273.2159 - val_loss: 194.6717\n",
      "Epoch 59/500\n",
      "948/948 [==============================] - 0s - loss: 271.4050 - val_loss: 194.1265\n",
      "Epoch 60/500\n",
      "948/948 [==============================] - 0s - loss: 268.3617 - val_loss: 193.5067\n",
      "Epoch 61/500\n",
      "948/948 [==============================] - 0s - loss: 263.4715 - val_loss: 192.3780\n",
      "Epoch 62/500\n",
      "948/948 [==============================] - 0s - loss: 274.3442 - val_loss: 191.4455\n",
      "Epoch 63/500\n",
      "948/948 [==============================] - 0s - loss: 263.1608 - val_loss: 190.5083\n",
      "Epoch 64/500\n",
      "948/948 [==============================] - 0s - loss: 265.2326 - val_loss: 189.5032\n",
      "Epoch 65/500\n",
      "948/948 [==============================] - 0s - loss: 270.9443 - val_loss: 189.1649\n",
      "Epoch 66/500\n",
      "948/948 [==============================] - 0s - loss: 265.5343 - val_loss: 188.8586\n",
      "Epoch 67/500\n",
      "948/948 [==============================] - 0s - loss: 261.5149 - val_loss: 186.6159\n",
      "Epoch 68/500\n",
      "948/948 [==============================] - 0s - loss: 263.0535 - val_loss: 186.6783\n",
      "Epoch 69/500\n",
      "948/948 [==============================] - 0s - loss: 264.8389 - val_loss: 185.7399\n",
      "Epoch 70/500\n",
      "948/948 [==============================] - 0s - loss: 267.7897 - val_loss: 183.5494\n",
      "Epoch 71/500\n",
      "948/948 [==============================] - 0s - loss: 259.9169 - val_loss: 182.5970: 261.03\n",
      "Epoch 72/500\n",
      "948/948 [==============================] - 0s - loss: 261.2379 - val_loss: 181.6473\n",
      "Epoch 73/500\n",
      "948/948 [==============================] - 0s - loss: 253.8660 - val_loss: 180.3082\n",
      "Epoch 74/500\n",
      "948/948 [==============================] - 0s - loss: 259.2111 - val_loss: 179.8582\n",
      "Epoch 75/500\n",
      "948/948 [==============================] - 0s - loss: 256.1888 - val_loss: 179.6430\n",
      "Epoch 76/500\n",
      "948/948 [==============================] - 0s - loss: 254.7881 - val_loss: 177.8169\n",
      "Epoch 77/500\n",
      "948/948 [==============================] - 0s - loss: 251.7212 - val_loss: 176.0473\n",
      "Epoch 78/500\n",
      "948/948 [==============================] - 0s - loss: 249.8976 - val_loss: 175.3983\n",
      "Epoch 79/500\n",
      "948/948 [==============================] - 0s - loss: 252.5246 - val_loss: 174.0156\n",
      "Epoch 80/500\n",
      "948/948 [==============================] - 0s - loss: 252.7273 - val_loss: 172.7947\n",
      "Epoch 81/500\n",
      "948/948 [==============================] - 0s - loss: 258.3567 - val_loss: 171.7365\n",
      "Epoch 82/500\n",
      "948/948 [==============================] - 0s - loss: 240.2166 - val_loss: 170.8613\n",
      "Epoch 83/500\n",
      "948/948 [==============================] - 0s - loss: 254.8396 - val_loss: 169.2482\n",
      "Epoch 84/500\n",
      "948/948 [==============================] - 0s - loss: 249.8034 - val_loss: 168.4982\n",
      "Epoch 85/500\n",
      "948/948 [==============================] - 0s - loss: 250.0779 - val_loss: 167.7750\n",
      "Epoch 86/500\n",
      "948/948 [==============================] - 0s - loss: 240.9013 - val_loss: 165.7222\n",
      "Epoch 87/500\n",
      "948/948 [==============================] - 0s - loss: 243.2506 - val_loss: 164.4491\n",
      "Epoch 88/500\n",
      "948/948 [==============================] - 0s - loss: 236.8279 - val_loss: 164.4484\n",
      "Epoch 89/500\n",
      "948/948 [==============================] - 0s - loss: 244.2645 - val_loss: 162.1918\n",
      "Epoch 90/500\n",
      "948/948 [==============================] - 0s - loss: 240.6694 - val_loss: 161.4155\n",
      "Epoch 91/500\n",
      "948/948 [==============================] - 0s - loss: 237.9997 - val_loss: 160.2725\n",
      "Epoch 92/500\n",
      "948/948 [==============================] - 0s - loss: 243.2148 - val_loss: 159.0545\n",
      "Epoch 93/500\n",
      "948/948 [==============================] - 0s - loss: 231.2924 - val_loss: 157.8387\n",
      "Epoch 94/500\n",
      "948/948 [==============================] - 0s - loss: 240.4800 - val_loss: 156.3515\n",
      "Epoch 95/500\n",
      "948/948 [==============================] - 0s - loss: 234.0769 - val_loss: 155.3262\n",
      "Epoch 96/500\n",
      "948/948 [==============================] - 0s - loss: 236.6869 - val_loss: 154.1497\n",
      "Epoch 97/500\n",
      "948/948 [==============================] - 0s - loss: 237.7684 - val_loss: 153.4998\n",
      "Epoch 98/500\n",
      "948/948 [==============================] - 0s - loss: 240.4200 - val_loss: 152.3523\n",
      "Epoch 99/500\n",
      "948/948 [==============================] - 0s - loss: 232.9402 - val_loss: 151.1785\n",
      "Epoch 100/500\n",
      "948/948 [==============================] - 0s - loss: 232.8238 - val_loss: 149.9519\n",
      "Epoch 101/500\n",
      "948/948 [==============================] - 0s - loss: 230.9523 - val_loss: 148.6299\n",
      "Epoch 102/500\n",
      "948/948 [==============================] - 0s - loss: 232.4661 - val_loss: 148.5425\n",
      "Epoch 103/500\n",
      "948/948 [==============================] - 0s - loss: 230.7753 - val_loss: 146.5567\n",
      "Epoch 104/500\n",
      "948/948 [==============================] - 0s - loss: 231.3618 - val_loss: 146.1500\n",
      "Epoch 105/500\n",
      "948/948 [==============================] - 0s - loss: 228.3070 - val_loss: 144.1266\n",
      "Epoch 106/500\n",
      "948/948 [==============================] - 0s - loss: 222.5229 - val_loss: 143.4489\n",
      "Epoch 107/500\n",
      "948/948 [==============================] - 0s - loss: 221.3879 - val_loss: 142.6976\n",
      "Epoch 108/500\n",
      "948/948 [==============================] - 0s - loss: 217.2427 - val_loss: 140.9997\n",
      "Epoch 109/500\n",
      "948/948 [==============================] - 0s - loss: 234.0763 - val_loss: 139.4411\n",
      "Epoch 110/500\n",
      "948/948 [==============================] - 0s - loss: 222.3876 - val_loss: 138.7898\n",
      "Epoch 111/500\n",
      "948/948 [==============================] - 0s - loss: 231.1128 - val_loss: 138.2371\n",
      "Epoch 112/500\n",
      "948/948 [==============================] - 0s - loss: 229.0562 - val_loss: 138.2004\n",
      "Epoch 113/500\n",
      "948/948 [==============================] - 0s - loss: 214.3103 - val_loss: 136.8962\n",
      "Epoch 114/500\n",
      "948/948 [==============================] - 0s - loss: 224.1157 - val_loss: 136.4981\n",
      "Epoch 115/500\n",
      "948/948 [==============================] - 0s - loss: 226.3936 - val_loss: 135.3621\n",
      "Epoch 116/500\n",
      "948/948 [==============================] - 0s - loss: 219.7926 - val_loss: 135.7728\n",
      "Epoch 117/500\n",
      "948/948 [==============================] - 0s - loss: 222.4023 - val_loss: 134.0641\n",
      "Epoch 118/500\n",
      "948/948 [==============================] - 0s - loss: 211.8892 - val_loss: 133.6161\n",
      "Epoch 119/500\n",
      "948/948 [==============================] - 0s - loss: 220.5338 - val_loss: 132.0120\n",
      "Epoch 120/500\n",
      "948/948 [==============================] - 0s - loss: 214.9798 - val_loss: 132.4345\n",
      "Epoch 121/500\n",
      "948/948 [==============================] - 0s - loss: 225.8662 - val_loss: 130.6301\n",
      "Epoch 122/500\n",
      "948/948 [==============================] - 0s - loss: 216.6275 - val_loss: 130.2935\n",
      "Epoch 123/500\n",
      "948/948 [==============================] - 0s - loss: 210.2416 - val_loss: 130.1073\n",
      "Epoch 124/500\n",
      "948/948 [==============================] - 0s - loss: 216.9735 - val_loss: 128.4542\n",
      "Epoch 125/500\n",
      "948/948 [==============================] - 0s - loss: 211.1883 - val_loss: 127.7829\n",
      "Epoch 126/500\n",
      "948/948 [==============================] - 0s - loss: 205.4758 - val_loss: 127.7271\n",
      "Epoch 127/500\n",
      "948/948 [==============================] - 0s - loss: 212.8957 - val_loss: 129.4089\n",
      "Epoch 128/500\n",
      "948/948 [==============================] - 0s - loss: 218.6323 - val_loss: 126.6304\n",
      "Epoch 129/500\n",
      "948/948 [==============================] - 0s - loss: 205.3416 - val_loss: 125.7653\n",
      "Epoch 130/500\n",
      "948/948 [==============================] - 0s - loss: 214.0228 - val_loss: 126.1046\n",
      "Epoch 131/500\n",
      "948/948 [==============================] - 0s - loss: 212.9238 - val_loss: 125.4000\n",
      "Epoch 132/500\n",
      "948/948 [==============================] - 0s - loss: 217.9276 - val_loss: 124.5279\n",
      "Epoch 133/500\n",
      "948/948 [==============================] - 0s - loss: 212.5176 - val_loss: 124.1078\n",
      "Epoch 134/500\n",
      "948/948 [==============================] - 0s - loss: 207.3549 - val_loss: 123.3023\n",
      "Epoch 135/500\n",
      "948/948 [==============================] - 0s - loss: 205.8317 - val_loss: 123.9926\n",
      "Epoch 136/500\n",
      "948/948 [==============================] - 0s - loss: 219.8766 - val_loss: 122.7500\n",
      "Epoch 137/500\n",
      "948/948 [==============================] - 0s - loss: 213.6148 - val_loss: 122.6481\n",
      "Epoch 138/500\n",
      "948/948 [==============================] - 0s - loss: 221.4618 - val_loss: 122.8863\n",
      "Epoch 139/500\n",
      "948/948 [==============================] - 0s - loss: 210.6258 - val_loss: 121.8093\n",
      "Epoch 140/500\n",
      "948/948 [==============================] - 0s - loss: 208.0524 - val_loss: 121.3307\n",
      "Epoch 141/500\n",
      "948/948 [==============================] - 0s - loss: 213.0550 - val_loss: 121.6105\n",
      "Epoch 142/500\n",
      "948/948 [==============================] - 0s - loss: 206.0384 - val_loss: 120.6782\n",
      "Epoch 143/500\n",
      "948/948 [==============================] - 0s - loss: 195.2810 - val_loss: 121.1020\n",
      "Epoch 144/500\n",
      "948/948 [==============================] - 0s - loss: 203.5663 - val_loss: 120.0642\n",
      "Epoch 145/500\n",
      "948/948 [==============================] - 0s - loss: 211.7955 - val_loss: 119.3743\n",
      "Epoch 146/500\n",
      "948/948 [==============================] - 0s - loss: 204.1206 - val_loss: 119.5172\n",
      "Epoch 147/500\n",
      "948/948 [==============================] - 0s - loss: 206.3480 - val_loss: 119.5463\n",
      "Epoch 148/500\n",
      "948/948 [==============================] - 0s - loss: 216.1393 - val_loss: 119.9797\n",
      "Epoch 149/500\n",
      "948/948 [==============================] - 0s - loss: 200.5582 - val_loss: 119.4541\n",
      "Epoch 150/500\n",
      "948/948 [==============================] - 0s - loss: 209.3071 - val_loss: 119.0070\n",
      "Epoch 151/500\n",
      "948/948 [==============================] - 0s - loss: 206.6426 - val_loss: 118.6952\n",
      "Epoch 152/500\n",
      "948/948 [==============================] - 0s - loss: 199.7927 - val_loss: 120.9315\n",
      "Epoch 153/500\n",
      "948/948 [==============================] - 0s - loss: 204.7110 - val_loss: 118.5138\n",
      "Epoch 154/500\n",
      "948/948 [==============================] - 0s - loss: 208.3077 - val_loss: 118.6045\n",
      "Epoch 155/500\n",
      "948/948 [==============================] - 0s - loss: 205.8077 - val_loss: 117.8428\n",
      "Epoch 156/500\n",
      "948/948 [==============================] - 0s - loss: 202.3144 - val_loss: 117.3266\n",
      "Epoch 157/500\n",
      "948/948 [==============================] - 0s - loss: 203.3949 - val_loss: 117.8326\n",
      "Epoch 158/500\n",
      "948/948 [==============================] - 0s - loss: 208.6871 - val_loss: 116.6551\n",
      "Epoch 159/500\n",
      "948/948 [==============================] - 0s - loss: 198.7466 - val_loss: 117.5791\n",
      "Epoch 160/500\n",
      "948/948 [==============================] - 0s - loss: 201.2729 - val_loss: 116.4378\n",
      "Epoch 161/500\n",
      "948/948 [==============================] - 0s - loss: 204.3394 - val_loss: 116.2881\n",
      "Epoch 162/500\n",
      "948/948 [==============================] - 0s - loss: 199.1788 - val_loss: 115.8153\n",
      "Epoch 163/500\n",
      "948/948 [==============================] - 0s - loss: 204.5654 - val_loss: 115.5586\n",
      "Epoch 164/500\n",
      "948/948 [==============================] - 0s - loss: 203.1427 - val_loss: 120.1468\n",
      "Epoch 165/500\n",
      "948/948 [==============================] - 0s - loss: 192.1685 - val_loss: 115.5751\n",
      "Epoch 166/500\n",
      "948/948 [==============================] - 0s - loss: 192.7265 - val_loss: 116.1006\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 213.7445 - val_loss: 115.0802\n",
      "Epoch 168/500\n",
      "948/948 [==============================] - 0s - loss: 203.4547 - val_loss: 116.2868\n",
      "Epoch 169/500\n",
      "948/948 [==============================] - 0s - loss: 192.8424 - val_loss: 114.7429\n",
      "Epoch 170/500\n",
      "948/948 [==============================] - 0s - loss: 205.6389 - val_loss: 115.9824\n",
      "Epoch 171/500\n",
      "948/948 [==============================] - 0s - loss: 196.1587 - val_loss: 115.9592\n",
      "Epoch 172/500\n",
      "948/948 [==============================] - 0s - loss: 201.7158 - val_loss: 116.3966\n",
      "Epoch 173/500\n",
      "948/948 [==============================] - 0s - loss: 209.0548 - val_loss: 114.8523\n",
      "Epoch 174/500\n",
      "948/948 [==============================] - 0s - loss: 196.3797 - val_loss: 114.6576\n",
      "Epoch 175/500\n",
      "948/948 [==============================] - 0s - loss: 192.8955 - val_loss: 114.8443\n",
      "Epoch 176/500\n",
      "948/948 [==============================] - 0s - loss: 197.7696 - val_loss: 114.3985\n",
      "Epoch 177/500\n",
      "948/948 [==============================] - 0s - loss: 202.1333 - val_loss: 114.6661\n",
      "Epoch 178/500\n",
      "948/948 [==============================] - 0s - loss: 197.2335 - val_loss: 113.8014\n",
      "Epoch 179/500\n",
      "948/948 [==============================] - 0s - loss: 210.6092 - val_loss: 113.4127\n",
      "Epoch 180/500\n",
      "948/948 [==============================] - 0s - loss: 194.9520 - val_loss: 112.8311\n",
      "Epoch 181/500\n",
      "948/948 [==============================] - 0s - loss: 192.7270 - val_loss: 114.0389\n",
      "Epoch 182/500\n",
      "948/948 [==============================] - 0s - loss: 192.8512 - val_loss: 113.4748\n",
      "Epoch 183/500\n",
      "948/948 [==============================] - 0s - loss: 201.0990 - val_loss: 114.5238\n",
      "Epoch 184/500\n",
      "948/948 [==============================] - 0s - loss: 206.2635 - val_loss: 112.3648\n",
      "Epoch 185/500\n",
      "948/948 [==============================] - 0s - loss: 187.6997 - val_loss: 112.9224\n",
      "Epoch 186/500\n",
      "948/948 [==============================] - 0s - loss: 205.0462 - val_loss: 112.2815\n",
      "Epoch 187/500\n",
      "948/948 [==============================] - 0s - loss: 198.7497 - val_loss: 112.3317\n",
      "Epoch 188/500\n",
      "948/948 [==============================] - 0s - loss: 199.4908 - val_loss: 111.8568\n",
      "Epoch 189/500\n",
      "948/948 [==============================] - 0s - loss: 196.7411 - val_loss: 112.0362\n",
      "Epoch 190/500\n",
      "948/948 [==============================] - 0s - loss: 195.4269 - val_loss: 112.7145\n",
      "Epoch 191/500\n",
      "948/948 [==============================] - 0s - loss: 196.2829 - val_loss: 112.4149\n",
      "Epoch 192/500\n",
      "948/948 [==============================] - 0s - loss: 188.0789 - val_loss: 111.2455\n",
      "Epoch 193/500\n",
      "948/948 [==============================] - 0s - loss: 209.1208 - val_loss: 112.0629\n",
      "Epoch 194/500\n",
      "948/948 [==============================] - 0s - loss: 194.7431 - val_loss: 111.6268\n",
      "Epoch 195/500\n",
      "948/948 [==============================] - 0s - loss: 194.6762 - val_loss: 110.9071\n",
      "Epoch 196/500\n",
      "948/948 [==============================] - 0s - loss: 199.1106 - val_loss: 111.8854\n",
      "Epoch 197/500\n",
      "948/948 [==============================] - 0s - loss: 193.5384 - val_loss: 111.0841\n",
      "Epoch 198/500\n",
      "948/948 [==============================] - 0s - loss: 194.7062 - val_loss: 111.4308\n",
      "Epoch 199/500\n",
      "948/948 [==============================] - 0s - loss: 192.9296 - val_loss: 111.2919\n",
      "Epoch 200/500\n",
      "948/948 [==============================] - 0s - loss: 202.0355 - val_loss: 111.4613\n",
      "Epoch 201/500\n",
      "948/948 [==============================] - 0s - loss: 199.1891 - val_loss: 111.4786\n",
      "Epoch 202/500\n",
      "948/948 [==============================] - 0s - loss: 194.0267 - val_loss: 110.8557\n",
      "Epoch 203/500\n",
      "948/948 [==============================] - 0s - loss: 191.9001 - val_loss: 110.6399\n",
      "Epoch 204/500\n",
      "948/948 [==============================] - 0s - loss: 202.9106 - val_loss: 110.1320\n",
      "Epoch 205/500\n",
      "948/948 [==============================] - 0s - loss: 185.0428 - val_loss: 110.2340\n",
      "Epoch 206/500\n",
      "948/948 [==============================] - 0s - loss: 197.0621 - val_loss: 109.5086\n",
      "Epoch 207/500\n",
      "948/948 [==============================] - 0s - loss: 189.8104 - val_loss: 110.3130\n",
      "Epoch 208/500\n",
      "948/948 [==============================] - 0s - loss: 186.5852 - val_loss: 110.8171\n",
      "Epoch 209/500\n",
      "948/948 [==============================] - 0s - loss: 192.0328 - val_loss: 109.1726\n",
      "Epoch 210/500\n",
      "948/948 [==============================] - 0s - loss: 202.0411 - val_loss: 111.6528\n",
      "Epoch 211/500\n",
      "948/948 [==============================] - 0s - loss: 190.3685 - val_loss: 110.1342\n",
      "Epoch 212/500\n",
      "948/948 [==============================] - 0s - loss: 188.4060 - val_loss: 110.0067\n",
      "Epoch 213/500\n",
      "948/948 [==============================] - 0s - loss: 192.0480 - val_loss: 109.2469\n",
      "Epoch 214/500\n",
      "948/948 [==============================] - 0s - loss: 197.9842 - val_loss: 110.3255\n",
      "Epoch 215/500\n",
      "948/948 [==============================] - 0s - loss: 190.9001 - val_loss: 109.5114\n",
      "Epoch 216/500\n",
      "948/948 [==============================] - 0s - loss: 187.7679 - val_loss: 109.4204\n",
      "Epoch 217/500\n",
      "948/948 [==============================] - 0s - loss: 190.4698 - val_loss: 108.9603\n",
      "Epoch 218/500\n",
      "948/948 [==============================] - 0s - loss: 192.4633 - val_loss: 110.0035\n",
      "Epoch 219/500\n",
      "948/948 [==============================] - 0s - loss: 196.0715 - val_loss: 108.5428\n",
      "Epoch 220/500\n",
      "948/948 [==============================] - 0s - loss: 196.4632 - val_loss: 109.0781\n",
      "Epoch 221/500\n",
      "948/948 [==============================] - ETA: 0s - loss: 194.511 - 0s - loss: 193.4245 - val_loss: 108.4262\n",
      "Epoch 222/500\n",
      "948/948 [==============================] - 0s - loss: 196.6766 - val_loss: 108.3766\n",
      "Epoch 223/500\n",
      "948/948 [==============================] - 0s - loss: 189.1482 - val_loss: 108.4928\n",
      "Epoch 224/500\n",
      "948/948 [==============================] - 0s - loss: 195.1801 - val_loss: 108.0591\n",
      "Epoch 225/500\n",
      "948/948 [==============================] - 0s - loss: 183.8058 - val_loss: 108.6076\n",
      "Epoch 226/500\n",
      "948/948 [==============================] - 0s - loss: 192.9921 - val_loss: 108.5055\n",
      "Epoch 227/500\n",
      "948/948 [==============================] - 0s - loss: 185.8749 - val_loss: 108.8971\n",
      "Epoch 228/500\n",
      "948/948 [==============================] - 0s - loss: 187.6210 - val_loss: 108.4520\n",
      "Epoch 229/500\n",
      "948/948 [==============================] - 0s - loss: 194.9203 - val_loss: 108.1225\n",
      "Epoch 230/500\n",
      "948/948 [==============================] - 0s - loss: 186.9846 - val_loss: 108.3568\n",
      "Epoch 231/500\n",
      "948/948 [==============================] - 0s - loss: 191.5812 - val_loss: 107.2889\n",
      "Epoch 232/500\n",
      "948/948 [==============================] - 0s - loss: 190.7430 - val_loss: 107.2086\n",
      "Epoch 233/500\n",
      "948/948 [==============================] - 0s - loss: 183.7880 - val_loss: 107.8876\n",
      "Epoch 234/500\n",
      "948/948 [==============================] - 0s - loss: 202.2648 - val_loss: 107.3727\n",
      "Epoch 235/500\n",
      "948/948 [==============================] - 0s - loss: 195.8330 - val_loss: 108.2379\n",
      "Epoch 236/500\n",
      "948/948 [==============================] - 0s - loss: 192.3367 - val_loss: 107.9392\n",
      "Epoch 237/500\n",
      "948/948 [==============================] - 0s - loss: 187.1547 - val_loss: 110.1314\n",
      "Epoch 238/500\n",
      "948/948 [==============================] - 0s - loss: 195.8249 - val_loss: 107.8008\n",
      "Epoch 239/500\n",
      "948/948 [==============================] - 0s - loss: 189.1967 - val_loss: 109.6773\n",
      "Epoch 240/500\n",
      "948/948 [==============================] - 0s - loss: 193.9051 - val_loss: 107.2072\n",
      "Epoch 241/500\n",
      "948/948 [==============================] - 0s - loss: 183.5844 - val_loss: 107.2110\n",
      "Epoch 242/500\n",
      "948/948 [==============================] - 0s - loss: 182.1256 - val_loss: 106.7533\n",
      "Epoch 243/500\n",
      "948/948 [==============================] - 0s - loss: 193.8674 - val_loss: 106.5617\n",
      "Epoch 244/500\n",
      "948/948 [==============================] - 0s - loss: 197.4077 - val_loss: 106.7495\n",
      "Epoch 245/500\n",
      "948/948 [==============================] - 0s - loss: 198.0047 - val_loss: 106.2735\n",
      "Epoch 246/500\n",
      "948/948 [==============================] - 0s - loss: 189.2696 - val_loss: 106.9745\n",
      "Epoch 247/500\n",
      "948/948 [==============================] - 0s - loss: 187.0002 - val_loss: 106.3634\n",
      "Epoch 248/500\n",
      "948/948 [==============================] - 0s - loss: 189.2317 - val_loss: 106.5471\n",
      "Epoch 249/500\n",
      "948/948 [==============================] - 0s - loss: 191.1544 - val_loss: 107.4129\n",
      "Epoch 250/500\n",
      "948/948 [==============================] - 0s - loss: 187.9771 - val_loss: 108.6230\n",
      "Epoch 251/500\n",
      "948/948 [==============================] - 0s - loss: 181.5165 - val_loss: 106.7197\n",
      "Epoch 252/500\n",
      "948/948 [==============================] - 0s - loss: 193.7790 - val_loss: 106.7562\n",
      "Epoch 253/500\n",
      "948/948 [==============================] - 0s - loss: 187.7119 - val_loss: 106.5454\n",
      "Epoch 254/500\n",
      "948/948 [==============================] - 0s - loss: 187.3358 - val_loss: 106.7642\n",
      "Epoch 255/500\n",
      "948/948 [==============================] - 0s - loss: 188.9028 - val_loss: 106.3649\n",
      "Epoch 256/500\n",
      "948/948 [==============================] - 0s - loss: 192.2621 - val_loss: 105.6976\n",
      "Epoch 257/500\n",
      "948/948 [==============================] - 0s - loss: 174.1758 - val_loss: 105.6946\n",
      "Epoch 258/500\n",
      "948/948 [==============================] - 0s - loss: 187.6561 - val_loss: 105.4171\n",
      "Epoch 259/500\n",
      "948/948 [==============================] - 0s - loss: 184.7990 - val_loss: 104.9367\n",
      "Epoch 260/500\n",
      "948/948 [==============================] - 0s - loss: 178.4453 - val_loss: 105.5058\n",
      "Epoch 261/500\n",
      "948/948 [==============================] - 0s - loss: 190.8111 - val_loss: 107.4990\n",
      "Epoch 262/500\n",
      "948/948 [==============================] - 0s - loss: 180.5464 - val_loss: 105.2516\n",
      "Epoch 263/500\n",
      "948/948 [==============================] - 0s - loss: 188.8746 - val_loss: 104.8950\n",
      "Epoch 264/500\n",
      "948/948 [==============================] - 0s - loss: 186.0192 - val_loss: 105.2744\n",
      "Epoch 265/500\n",
      "948/948 [==============================] - 0s - loss: 185.3363 - val_loss: 104.3682\n",
      "Epoch 266/500\n",
      "948/948 [==============================] - 0s - loss: 187.3223 - val_loss: 105.8063\n",
      "Epoch 267/500\n",
      "948/948 [==============================] - 0s - loss: 186.1311 - val_loss: 104.4928\n",
      "Epoch 268/500\n",
      "948/948 [==============================] - 0s - loss: 184.5688 - val_loss: 104.3940\n",
      "Epoch 269/500\n",
      "948/948 [==============================] - 0s - loss: 178.7003 - val_loss: 103.4550\n",
      "Epoch 270/500\n",
      "948/948 [==============================] - 0s - loss: 195.2669 - val_loss: 104.0909\n",
      "Epoch 271/500\n",
      "948/948 [==============================] - 0s - loss: 185.5678 - val_loss: 104.0793\n",
      "Epoch 272/500\n",
      "948/948 [==============================] - 0s - loss: 179.6038 - val_loss: 104.4394\n",
      "Epoch 273/500\n",
      "948/948 [==============================] - 0s - loss: 195.8802 - val_loss: 103.9608\n",
      "Epoch 274/500\n",
      "948/948 [==============================] - 0s - loss: 184.5075 - val_loss: 103.9428\n",
      "Epoch 275/500\n",
      "948/948 [==============================] - 0s - loss: 184.0792 - val_loss: 103.5231\n",
      "Epoch 276/500\n",
      "948/948 [==============================] - 0s - loss: 180.0482 - val_loss: 103.4838\n",
      "Epoch 277/500\n",
      "948/948 [==============================] - 0s - loss: 186.2945 - val_loss: 103.5196\n",
      "Epoch 278/500\n",
      "948/948 [==============================] - 0s - loss: 187.2492 - val_loss: 103.2175\n",
      "Epoch 279/500\n",
      "948/948 [==============================] - 0s - loss: 181.3765 - val_loss: 103.4769\n",
      "Epoch 280/500\n",
      "948/948 [==============================] - 0s - loss: 180.6234 - val_loss: 102.4347\n",
      "Epoch 281/500\n",
      "948/948 [==============================] - 0s - loss: 189.3257 - val_loss: 103.6823\n",
      "Epoch 282/500\n",
      "948/948 [==============================] - 0s - loss: 181.8068 - val_loss: 102.9150\n",
      "Epoch 283/500\n",
      "948/948 [==============================] - 0s - loss: 188.5963 - val_loss: 103.1678\n",
      "Epoch 284/500\n",
      "948/948 [==============================] - 0s - loss: 186.9737 - val_loss: 103.2428\n",
      "Epoch 285/500\n",
      "948/948 [==============================] - 0s - loss: 186.1665 - val_loss: 102.3755\n",
      "Epoch 286/500\n",
      "948/948 [==============================] - 0s - loss: 178.5921 - val_loss: 102.5783\n",
      "Epoch 287/500\n",
      "948/948 [==============================] - 0s - loss: 175.1231 - val_loss: 102.2747\n",
      "Epoch 288/500\n",
      "948/948 [==============================] - 0s - loss: 174.4691 - val_loss: 102.2621\n",
      "Epoch 289/500\n",
      "948/948 [==============================] - 0s - loss: 176.5681 - val_loss: 102.2748\n",
      "Epoch 290/500\n",
      "948/948 [==============================] - 0s - loss: 180.0564 - val_loss: 103.4122\n",
      "Epoch 291/500\n",
      "948/948 [==============================] - 0s - loss: 184.8643 - val_loss: 102.0988\n",
      "Epoch 292/500\n",
      "948/948 [==============================] - 0s - loss: 183.1481 - val_loss: 103.2243\n",
      "Epoch 293/500\n",
      "948/948 [==============================] - 0s - loss: 179.5247 - val_loss: 101.8565\n",
      "Epoch 294/500\n",
      "948/948 [==============================] - 0s - loss: 179.9875 - val_loss: 102.2166\n",
      "Epoch 295/500\n",
      "948/948 [==============================] - 0s - loss: 181.5689 - val_loss: 102.0948\n",
      "Epoch 296/500\n",
      "948/948 [==============================] - 0s - loss: 176.3763 - val_loss: 101.8801\n",
      "Epoch 297/500\n",
      "948/948 [==============================] - 0s - loss: 184.0473 - val_loss: 101.8984\n",
      "Epoch 298/500\n",
      "948/948 [==============================] - 0s - loss: 181.4474 - val_loss: 101.9097\n",
      "Epoch 299/500\n",
      "948/948 [==============================] - 0s - loss: 182.9950 - val_loss: 102.2903\n",
      "Epoch 300/500\n",
      "948/948 [==============================] - 0s - loss: 180.5437 - val_loss: 103.4923\n",
      "Epoch 301/500\n",
      "948/948 [==============================] - 0s - loss: 190.6381 - val_loss: 102.6758\n",
      "Epoch 302/500\n",
      "948/948 [==============================] - 0s - loss: 180.5494 - val_loss: 101.3932\n",
      "Epoch 303/500\n",
      "948/948 [==============================] - 0s - loss: 181.6722 - val_loss: 101.1242\n",
      "Epoch 304/500\n",
      "948/948 [==============================] - 0s - loss: 182.7130 - val_loss: 101.6832\n",
      "Epoch 305/500\n",
      "948/948 [==============================] - 0s - loss: 178.7728 - val_loss: 103.1708\n",
      "Epoch 306/500\n",
      "948/948 [==============================] - 0s - loss: 176.8684 - val_loss: 102.0087\n",
      "Epoch 307/500\n",
      "948/948 [==============================] - 0s - loss: 184.9889 - val_loss: 102.7664\n",
      "Epoch 308/500\n",
      "948/948 [==============================] - 0s - loss: 190.5688 - val_loss: 103.2516\n",
      "Epoch 309/500\n",
      "948/948 [==============================] - 0s - loss: 175.3173 - val_loss: 102.6061\n",
      "Epoch 310/500\n",
      "948/948 [==============================] - 0s - loss: 180.9853 - val_loss: 101.6117\n",
      "Epoch 311/500\n",
      "948/948 [==============================] - 0s - loss: 173.5048 - val_loss: 101.2008\n",
      "Epoch 312/500\n",
      "948/948 [==============================] - 0s - loss: 182.0804 - val_loss: 101.3894\n",
      "Epoch 313/500\n",
      "948/948 [==============================] - 0s - loss: 176.9334 - val_loss: 105.6693\n",
      "Epoch 314/500\n",
      "948/948 [==============================] - 0s - loss: 181.2819 - val_loss: 101.3625\n",
      "Epoch 315/500\n",
      "948/948 [==============================] - 0s - loss: 179.9775 - val_loss: 100.9609\n",
      "Epoch 316/500\n",
      "948/948 [==============================] - 0s - loss: 173.8311 - val_loss: 100.9787\n",
      "Epoch 317/500\n",
      "948/948 [==============================] - 0s - loss: 186.3567 - val_loss: 101.6605\n",
      "Epoch 318/500\n",
      "948/948 [==============================] - 0s - loss: 192.1720 - val_loss: 100.9287\n",
      "Epoch 319/500\n",
      "948/948 [==============================] - 0s - loss: 178.7760 - val_loss: 100.5821\n",
      "Epoch 320/500\n",
      "948/948 [==============================] - 0s - loss: 179.8521 - val_loss: 100.8113\n",
      "Epoch 321/500\n",
      "948/948 [==============================] - 0s - loss: 178.1759 - val_loss: 101.6668\n",
      "Epoch 322/500\n",
      "948/948 [==============================] - 0s - loss: 177.8607 - val_loss: 100.7286\n",
      "Epoch 323/500\n",
      "948/948 [==============================] - 0s - loss: 178.4651 - val_loss: 100.5954\n",
      "Epoch 324/500\n",
      "948/948 [==============================] - 0s - loss: 182.0251 - val_loss: 102.9126\n",
      "Epoch 325/500\n",
      "948/948 [==============================] - 0s - loss: 182.1992 - val_loss: 100.5901\n",
      "Epoch 326/500\n",
      "948/948 [==============================] - 0s - loss: 180.5974 - val_loss: 99.8948\n",
      "Epoch 327/500\n",
      "948/948 [==============================] - 0s - loss: 181.1333 - val_loss: 99.7609\n",
      "Epoch 328/500\n",
      "948/948 [==============================] - 0s - loss: 178.1037 - val_loss: 100.3039\n",
      "Epoch 329/500\n",
      "948/948 [==============================] - 0s - loss: 175.1789 - val_loss: 99.6302\n",
      "Epoch 330/500\n",
      "948/948 [==============================] - 0s - loss: 169.7036 - val_loss: 99.3739\n",
      "Epoch 331/500\n",
      "948/948 [==============================] - 0s - loss: 179.5206 - val_loss: 99.3948\n",
      "Epoch 332/500\n",
      "948/948 [==============================] - 0s - loss: 171.1293 - val_loss: 99.4641\n",
      "Epoch 333/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 182.9124 - val_loss: 100.5627\n",
      "Epoch 334/500\n",
      "948/948 [==============================] - 0s - loss: 174.7265 - val_loss: 101.6926\n",
      "Epoch 335/500\n",
      "948/948 [==============================] - 0s - loss: 175.7109 - val_loss: 100.4332\n",
      "Epoch 336/500\n",
      "948/948 [==============================] - 0s - loss: 185.1435 - val_loss: 100.8324\n",
      "Epoch 337/500\n",
      "948/948 [==============================] - 0s - loss: 182.1277 - val_loss: 99.5827\n",
      "Epoch 338/500\n",
      "948/948 [==============================] - 0s - loss: 176.7974 - val_loss: 99.6692\n",
      "Epoch 339/500\n",
      "948/948 [==============================] - 0s - loss: 181.8233 - val_loss: 102.5372\n",
      "Epoch 340/500\n",
      "948/948 [==============================] - 0s - loss: 172.1817 - val_loss: 100.7997\n",
      "Epoch 341/500\n",
      "948/948 [==============================] - 0s - loss: 180.2376 - val_loss: 100.0058\n",
      "Epoch 342/500\n",
      "948/948 [==============================] - 0s - loss: 176.3668 - val_loss: 99.7977\n",
      "Epoch 343/500\n",
      "948/948 [==============================] - 0s - loss: 176.1376 - val_loss: 99.6999\n",
      "Epoch 344/500\n",
      "948/948 [==============================] - 0s - loss: 182.1110 - val_loss: 100.1480\n",
      "Epoch 345/500\n",
      "948/948 [==============================] - 0s - loss: 177.6739 - val_loss: 100.7013\n",
      "Epoch 346/500\n",
      "948/948 [==============================] - 0s - loss: 170.1052 - val_loss: 99.1635\n",
      "Epoch 347/500\n",
      "948/948 [==============================] - 0s - loss: 182.3170 - val_loss: 99.5323\n",
      "Epoch 348/500\n",
      "948/948 [==============================] - 0s - loss: 174.2440 - val_loss: 99.9829\n",
      "Epoch 349/500\n",
      "948/948 [==============================] - 0s - loss: 177.4235 - val_loss: 100.2204\n",
      "Epoch 350/500\n",
      "948/948 [==============================] - 0s - loss: 174.6814 - val_loss: 99.7520\n",
      "Epoch 351/500\n",
      "948/948 [==============================] - 0s - loss: 169.1520 - val_loss: 99.9262\n",
      "Epoch 352/500\n",
      "948/948 [==============================] - 0s - loss: 174.7016 - val_loss: 99.5539\n",
      "Epoch 353/500\n",
      "948/948 [==============================] - 0s - loss: 176.5368 - val_loss: 99.3386\n",
      "Epoch 354/500\n",
      "948/948 [==============================] - 0s - loss: 166.9167 - val_loss: 98.5594\n",
      "Epoch 355/500\n",
      "948/948 [==============================] - 0s - loss: 186.1137 - val_loss: 98.4407\n",
      "Epoch 356/500\n",
      "948/948 [==============================] - 0s - loss: 174.6158 - val_loss: 98.0374\n",
      "Epoch 357/500\n",
      "948/948 [==============================] - 0s - loss: 166.9594 - val_loss: 98.1931\n",
      "Epoch 358/500\n",
      "948/948 [==============================] - 0s - loss: 171.6291 - val_loss: 99.0258\n",
      "Epoch 359/500\n",
      "948/948 [==============================] - 0s - loss: 175.9949 - val_loss: 97.8620s: 170.0\n",
      "Epoch 360/500\n",
      "948/948 [==============================] - 0s - loss: 177.3657 - val_loss: 98.1711\n",
      "Epoch 361/500\n",
      "948/948 [==============================] - 0s - loss: 170.6257 - val_loss: 99.1556\n",
      "Epoch 362/500\n",
      "948/948 [==============================] - 0s - loss: 182.9865 - val_loss: 97.8112\n",
      "Epoch 363/500\n",
      "948/948 [==============================] - 0s - loss: 171.2472 - val_loss: 99.3956\n",
      "Epoch 364/500\n",
      "948/948 [==============================] - 0s - loss: 173.0776 - val_loss: 98.6395\n",
      "Epoch 365/500\n",
      "948/948 [==============================] - 0s - loss: 176.1403 - val_loss: 99.1175\n",
      "Epoch 366/500\n",
      "948/948 [==============================] - 0s - loss: 174.5084 - val_loss: 98.8066\n",
      "Epoch 367/500\n",
      "948/948 [==============================] - 0s - loss: 185.3440 - val_loss: 98.9662\n",
      "Epoch 368/500\n",
      "948/948 [==============================] - 0s - loss: 183.5184 - val_loss: 98.0246\n",
      "Epoch 369/500\n",
      "948/948 [==============================] - 0s - loss: 184.8190 - val_loss: 99.2603\n",
      "Epoch 370/500\n",
      "948/948 [==============================] - 0s - loss: 173.7278 - val_loss: 98.3136\n",
      "Epoch 371/500\n",
      "948/948 [==============================] - 0s - loss: 177.6214 - val_loss: 97.8843\n",
      "Epoch 372/500\n",
      "948/948 [==============================] - 0s - loss: 169.4635 - val_loss: 97.4544\n",
      "Epoch 373/500\n",
      "948/948 [==============================] - 0s - loss: 176.5761 - val_loss: 99.3099\n",
      "Epoch 374/500\n",
      "948/948 [==============================] - 0s - loss: 178.0844 - val_loss: 99.8146\n",
      "Epoch 375/500\n",
      "948/948 [==============================] - 0s - loss: 177.1753 - val_loss: 100.1202\n",
      "Epoch 376/500\n",
      "948/948 [==============================] - 0s - loss: 173.4637 - val_loss: 97.7106\n",
      "Epoch 377/500\n",
      "948/948 [==============================] - 0s - loss: 167.3058 - val_loss: 97.3734\n",
      "Epoch 378/500\n",
      "948/948 [==============================] - 0s - loss: 171.0751 - val_loss: 97.4336\n",
      "Epoch 379/500\n",
      "948/948 [==============================] - 0s - loss: 175.0215 - val_loss: 98.1687s: 174.405\n",
      "Epoch 380/500\n",
      "948/948 [==============================] - 0s - loss: 175.7708 - val_loss: 98.1443\n",
      "Epoch 381/500\n",
      "948/948 [==============================] - 0s - loss: 179.7389 - val_loss: 97.6285\n",
      "Epoch 382/500\n",
      "948/948 [==============================] - 0s - loss: 170.0359 - val_loss: 98.3335\n",
      "Epoch 383/500\n",
      "948/948 [==============================] - 0s - loss: 173.0752 - val_loss: 98.0003\n",
      "Epoch 384/500\n",
      "948/948 [==============================] - 0s - loss: 178.6541 - val_loss: 97.7581\n",
      "Epoch 385/500\n",
      "948/948 [==============================] - 0s - loss: 169.8831 - val_loss: 99.0239\n",
      "Epoch 386/500\n",
      "948/948 [==============================] - 0s - loss: 171.0548 - val_loss: 97.0391\n",
      "Epoch 387/500\n",
      "948/948 [==============================] - 0s - loss: 176.2438 - val_loss: 97.4538\n",
      "Epoch 388/500\n",
      "948/948 [==============================] - 0s - loss: 170.4594 - val_loss: 98.0013\n",
      "Epoch 389/500\n",
      "948/948 [==============================] - 0s - loss: 181.9120 - val_loss: 97.4651s: 180.15\n",
      "Epoch 390/500\n",
      "948/948 [==============================] - 0s - loss: 173.7651 - val_loss: 97.8766\n",
      "Epoch 391/500\n",
      "948/948 [==============================] - 0s - loss: 178.4583 - val_loss: 97.2536\n",
      "Epoch 392/500\n",
      "948/948 [==============================] - 0s - loss: 177.5791 - val_loss: 98.8474\n",
      "Epoch 393/500\n",
      "948/948 [==============================] - 0s - loss: 178.7899 - val_loss: 96.3442\n",
      "Epoch 394/500\n",
      "948/948 [==============================] - 0s - loss: 167.7956 - val_loss: 96.2402\n",
      "Epoch 395/500\n",
      "948/948 [==============================] - 0s - loss: 171.6806 - val_loss: 96.8762\n",
      "Epoch 396/500\n",
      "948/948 [==============================] - 0s - loss: 174.9127 - val_loss: 96.6048\n",
      "Epoch 397/500\n",
      "948/948 [==============================] - 0s - loss: 175.7835 - val_loss: 97.0199\n",
      "Epoch 398/500\n",
      "948/948 [==============================] - 0s - loss: 168.1846 - val_loss: 96.9083\n",
      "Epoch 399/500\n",
      "948/948 [==============================] - 0s - loss: 179.8434 - val_loss: 96.7587\n",
      "Epoch 400/500\n",
      "948/948 [==============================] - 0s - loss: 169.0601 - val_loss: 97.4313\n",
      "Epoch 401/500\n",
      "948/948 [==============================] - 0s - loss: 174.4817 - val_loss: 97.7051\n",
      "Epoch 402/500\n",
      "948/948 [==============================] - 0s - loss: 170.4833 - val_loss: 97.2338\n",
      "Epoch 403/500\n",
      "948/948 [==============================] - 0s - loss: 167.4684 - val_loss: 98.9584\n",
      "Epoch 404/500\n",
      "948/948 [==============================] - 0s - loss: 180.3373 - val_loss: 96.5898\n",
      "Epoch 405/500\n",
      "948/948 [==============================] - 0s - loss: 166.3211 - val_loss: 96.2393\n",
      "Epoch 406/500\n",
      "948/948 [==============================] - 0s - loss: 173.4267 - val_loss: 96.5009\n",
      "Epoch 407/500\n",
      "948/948 [==============================] - 0s - loss: 174.1583 - val_loss: 98.0767\n",
      "Epoch 408/500\n",
      "948/948 [==============================] - 0s - loss: 164.8296 - val_loss: 96.8793\n",
      "Epoch 409/500\n",
      "948/948 [==============================] - 0s - loss: 184.6044 - val_loss: 96.3992\n",
      "Epoch 410/500\n",
      "948/948 [==============================] - 0s - loss: 173.6620 - val_loss: 96.2986\n",
      "Epoch 411/500\n",
      "948/948 [==============================] - 0s - loss: 161.4499 - val_loss: 96.2058\n",
      "Epoch 412/500\n",
      "948/948 [==============================] - 0s - loss: 166.5586 - val_loss: 96.1009\n",
      "Epoch 413/500\n",
      "948/948 [==============================] - 0s - loss: 164.8176 - val_loss: 96.3141\n",
      "Epoch 414/500\n",
      "948/948 [==============================] - 0s - loss: 165.8901 - val_loss: 97.9425\n",
      "Epoch 415/500\n",
      "948/948 [==============================] - 0s - loss: 161.5931 - val_loss: 97.2091\n",
      "Epoch 416/500\n",
      "948/948 [==============================] - 0s - loss: 184.1866 - val_loss: 97.5633\n",
      "Epoch 417/500\n",
      "948/948 [==============================] - 0s - loss: 169.7565 - val_loss: 96.7134\n",
      "Epoch 418/500\n",
      "948/948 [==============================] - 0s - loss: 169.7248 - val_loss: 97.3306\n",
      "Epoch 419/500\n",
      "948/948 [==============================] - 0s - loss: 175.4711 - val_loss: 96.1625\n",
      "Epoch 420/500\n",
      "948/948 [==============================] - 0s - loss: 171.1302 - val_loss: 96.2670\n",
      "Epoch 421/500\n",
      "948/948 [==============================] - 0s - loss: 171.9332 - val_loss: 96.3346\n",
      "Epoch 422/500\n",
      "948/948 [==============================] - 0s - loss: 169.0511 - val_loss: 96.4073\n",
      "Epoch 423/500\n",
      "948/948 [==============================] - 0s - loss: 177.9397 - val_loss: 96.5856\n",
      "Epoch 424/500\n",
      "948/948 [==============================] - 0s - loss: 168.7987 - val_loss: 97.6071\n",
      "Epoch 425/500\n",
      "948/948 [==============================] - 0s - loss: 176.1965 - val_loss: 95.5607\n",
      "Epoch 426/500\n",
      "948/948 [==============================] - 0s - loss: 167.3551 - val_loss: 95.5658\n",
      "Epoch 427/500\n",
      "948/948 [==============================] - 0s - loss: 159.9452 - val_loss: 95.3340\n",
      "Epoch 428/500\n",
      "948/948 [==============================] - 0s - loss: 169.6645 - val_loss: 95.4965\n",
      "Epoch 429/500\n",
      "948/948 [==============================] - 0s - loss: 161.4697 - val_loss: 95.4279\n",
      "Epoch 430/500\n",
      "948/948 [==============================] - 0s - loss: 177.1087 - val_loss: 95.8137\n",
      "Epoch 431/500\n",
      "948/948 [==============================] - 0s - loss: 175.1827 - val_loss: 97.3100\n",
      "Epoch 432/500\n",
      "948/948 [==============================] - 0s - loss: 175.0501 - val_loss: 95.7257\n",
      "Epoch 433/500\n",
      "948/948 [==============================] - 0s - loss: 167.7385 - val_loss: 95.2386\n",
      "Epoch 434/500\n",
      "948/948 [==============================] - 0s - loss: 167.3505 - val_loss: 95.2536\n",
      "Epoch 435/500\n",
      "948/948 [==============================] - 0s - loss: 175.8803 - val_loss: 95.2863\n",
      "Epoch 436/500\n",
      "948/948 [==============================] - 0s - loss: 167.2827 - val_loss: 95.6698\n",
      "Epoch 437/500\n",
      "948/948 [==============================] - 0s - loss: 167.9683 - val_loss: 95.2490\n",
      "Epoch 438/500\n",
      "948/948 [==============================] - 0s - loss: 164.5203 - val_loss: 94.8171\n",
      "Epoch 439/500\n",
      "948/948 [==============================] - 0s - loss: 169.3453 - val_loss: 94.7836\n",
      "Epoch 440/500\n",
      "948/948 [==============================] - 0s - loss: 170.4405 - val_loss: 94.9526\n",
      "Epoch 441/500\n",
      "948/948 [==============================] - 0s - loss: 163.4769 - val_loss: 97.0923\n",
      "Epoch 442/500\n",
      "948/948 [==============================] - 0s - loss: 177.3581 - val_loss: 96.4141\n",
      "Epoch 443/500\n",
      "948/948 [==============================] - 0s - loss: 173.5365 - val_loss: 97.4475\n",
      "Epoch 444/500\n",
      "948/948 [==============================] - 0s - loss: 177.4613 - val_loss: 96.3702\n",
      "Epoch 445/500\n",
      "948/948 [==============================] - 0s - loss: 172.1232 - val_loss: 95.0975\n",
      "Epoch 446/500\n",
      "948/948 [==============================] - 0s - loss: 169.7742 - val_loss: 95.2055\n",
      "Epoch 447/500\n",
      "948/948 [==============================] - 0s - loss: 161.8154 - val_loss: 94.4535\n",
      "Epoch 448/500\n",
      "948/948 [==============================] - 0s - loss: 164.1055 - val_loss: 94.8972\n",
      "Epoch 449/500\n",
      "948/948 [==============================] - 0s - loss: 166.8026 - val_loss: 95.7365\n",
      "Epoch 450/500\n",
      "948/948 [==============================] - 0s - loss: 173.6010 - val_loss: 96.4719\n",
      "Epoch 451/500\n",
      "948/948 [==============================] - 0s - loss: 163.6962 - val_loss: 95.9740\n",
      "Epoch 452/500\n",
      "948/948 [==============================] - 0s - loss: 166.4343 - val_loss: 94.9872\n",
      "Epoch 453/500\n",
      "948/948 [==============================] - 0s - loss: 163.6689 - val_loss: 94.4534\n",
      "Epoch 454/500\n",
      "948/948 [==============================] - 0s - loss: 167.9446 - val_loss: 94.1680\n",
      "Epoch 455/500\n",
      "948/948 [==============================] - 0s - loss: 172.3283 - val_loss: 93.9506\n",
      "Epoch 456/500\n",
      "948/948 [==============================] - 0s - loss: 164.3506 - val_loss: 93.9887\n",
      "Epoch 457/500\n",
      "948/948 [==============================] - 0s - loss: 166.9845 - val_loss: 93.7386\n",
      "Epoch 458/500\n",
      "948/948 [==============================] - 0s - loss: 174.7643 - val_loss: 96.0773\n",
      "Epoch 459/500\n",
      "948/948 [==============================] - 0s - loss: 176.0340 - val_loss: 95.1819\n",
      "Epoch 460/500\n",
      "948/948 [==============================] - 0s - loss: 167.9194 - val_loss: 95.6542\n",
      "Epoch 461/500\n",
      "948/948 [==============================] - 0s - loss: 164.5764 - val_loss: 93.8690\n",
      "Epoch 462/500\n",
      "948/948 [==============================] - 0s - loss: 172.5106 - val_loss: 93.8281\n",
      "Epoch 463/500\n",
      "948/948 [==============================] - 0s - loss: 170.5286 - val_loss: 93.9346\n",
      "Epoch 464/500\n",
      "948/948 [==============================] - 0s - loss: 167.1303 - val_loss: 94.6875\n",
      "Epoch 465/500\n",
      "948/948 [==============================] - 0s - loss: 169.7444 - val_loss: 94.4974\n",
      "Epoch 466/500\n",
      "948/948 [==============================] - 0s - loss: 160.8503 - val_loss: 93.5155\n",
      "Epoch 467/500\n",
      "948/948 [==============================] - 0s - loss: 174.1510 - val_loss: 94.7907\n",
      "Epoch 468/500\n",
      "948/948 [==============================] - 0s - loss: 164.1695 - val_loss: 94.3484\n",
      "Epoch 469/500\n",
      "948/948 [==============================] - 0s - loss: 173.3966 - val_loss: 93.1738\n",
      "Epoch 470/500\n",
      "948/948 [==============================] - 0s - loss: 157.4044 - val_loss: 93.8810\n",
      "Epoch 471/500\n",
      "948/948 [==============================] - 0s - loss: 157.5192 - val_loss: 93.0056\n",
      "Epoch 472/500\n",
      "948/948 [==============================] - 0s - loss: 164.5260 - val_loss: 93.5656\n",
      "Epoch 473/500\n",
      "948/948 [==============================] - 0s - loss: 170.5524 - val_loss: 93.0886\n",
      "Epoch 474/500\n",
      "948/948 [==============================] - 0s - loss: 166.6144 - val_loss: 93.4738\n",
      "Epoch 475/500\n",
      "948/948 [==============================] - 0s - loss: 171.4188 - val_loss: 93.0627\n",
      "Epoch 476/500\n",
      "948/948 [==============================] - 0s - loss: 161.5733 - val_loss: 93.3735\n",
      "Epoch 477/500\n",
      "948/948 [==============================] - 0s - loss: 165.2688 - val_loss: 93.2108\n",
      "Epoch 478/500\n",
      "948/948 [==============================] - 0s - loss: 163.9939 - val_loss: 94.9063\n",
      "Epoch 479/500\n",
      "948/948 [==============================] - 0s - loss: 170.1511 - val_loss: 93.2767\n",
      "Epoch 480/500\n",
      "948/948 [==============================] - 0s - loss: 167.9685 - val_loss: 93.3645\n",
      "Epoch 481/500\n",
      "948/948 [==============================] - 0s - loss: 165.1747 - val_loss: 94.1012\n",
      "Epoch 482/500\n",
      "948/948 [==============================] - 0s - loss: 170.7210 - val_loss: 94.2263\n",
      "Epoch 483/500\n",
      "948/948 [==============================] - 0s - loss: 169.2013 - val_loss: 93.2137\n",
      "Epoch 484/500\n",
      "948/948 [==============================] - 0s - loss: 167.9909 - val_loss: 95.3493\n",
      "Epoch 485/500\n",
      "948/948 [==============================] - 0s - loss: 168.2101 - val_loss: 92.9517\n",
      "Epoch 486/500\n",
      "948/948 [==============================] - 0s - loss: 163.3498 - val_loss: 93.6883\n",
      "Epoch 487/500\n",
      "948/948 [==============================] - 0s - loss: 161.3248 - val_loss: 92.9834\n",
      "Epoch 488/500\n",
      "948/948 [==============================] - 0s - loss: 160.3355 - val_loss: 93.8339\n",
      "Epoch 489/500\n",
      "948/948 [==============================] - 0s - loss: 163.0277 - val_loss: 92.6898\n",
      "Epoch 490/500\n",
      "948/948 [==============================] - 0s - loss: 165.1075 - val_loss: 93.2370\n",
      "Epoch 491/500\n",
      "948/948 [==============================] - 0s - loss: 166.4507 - val_loss: 93.8218\n",
      "Epoch 492/500\n",
      "948/948 [==============================] - 0s - loss: 177.1213 - val_loss: 95.5301\n",
      "Epoch 493/500\n",
      "948/948 [==============================] - 0s - loss: 166.3240 - val_loss: 93.3777\n",
      "Epoch 494/500\n",
      "948/948 [==============================] - 0s - loss: 158.6825 - val_loss: 94.6442s: 16\n",
      "Epoch 495/500\n",
      "948/948 [==============================] - 0s - loss: 153.3200 - val_loss: 93.1201\n",
      "Epoch 496/500\n",
      "948/948 [==============================] - 0s - loss: 163.2892 - val_loss: 93.8655\n",
      "Epoch 497/500\n",
      "948/948 [==============================] - 0s - loss: 163.9060 - val_loss: 93.2646\n",
      "Epoch 498/500\n",
      "948/948 [==============================] - 0s - loss: 164.1985 - val_loss: 93.4903\n",
      "Epoch 499/500\n",
      "948/948 [==============================] - 0s - loss: 159.0347 - val_loss: 93.9505\n",
      "Epoch 500/500\n",
      "948/948 [==============================] - 0s - loss: 160.3226 - val_loss: 92.2713\n",
      "948/948 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 0s\n",
      "226/226 [==============================] - 0s\n",
      "Loss for training data is 81.3419723511\n",
      "Loss for validation data is 92.2712936401\n",
      "Loss for test data is 129.309967041\n"
     ]
    }
   ],
   "source": [
    "predict1, error, accuracy1 = train_model(train1_r, train1_c, test1_r, test1_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error: 10.6223684992\n"
     ]
    }
   ],
   "source": [
    "print('The average error:', accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum error: 0.480798085786 maximum error: 85.6098146581 variance: 104.949090348\n"
     ]
    }
   ],
   "source": [
    "print('minimum error:', np.amin(error), 'maximum error:', np.amax(error), 'variance:', np.var(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.6555265487118422"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x156ad96cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_sorted= np.sort(error)\n",
    "p = 1. *np.arange(len(error))/(len(error)-1)\n",
    "plt.plot(error_sorted, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Input\n",
    "def cnn_model(rss, locations, test_rss, test_locations):\n",
    "    # get train_X, val_X, train_Y, val_Y\n",
    "    train_X, val_X, train_Y, val_Y = train_val(rss, locations)\n",
    "    test_X, test_Y = predata(test_rss, test_locations)\n",
    "    \n",
    "    # parameters\n",
    "    num_input = train_X.shape[1]# input layer size\n",
    "    act_fun = 'relu'\n",
    "    regularzation_penalty = 0.03\n",
    "    initilization_method = 'he_normal' #'random_uniform' ,'random_normal','TruncatedNormal' ,'glorot_uniform', 'glorot_nomral', 'he_normal', 'he_uniform'\n",
    "    #Optimizer\n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "    \n",
    "    train_X.reshape((1,train_X.shape[0],train_X.shape[1]))\n",
    "    test_X.reshape((1,test_X.shape[0],test_X.shape[1]))\n",
    "    val_X.reshape((1,val_X.shape[0],val_X.shape[1]))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=3,activation='relu',input_shape = (train_X.shape)\n",
    "    model.add(Conv1D(filters=32, kernel_size=2,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=adam, metrics=['accuracy'])          \n",
    "    model.fit(train_X, train_Y,\n",
    "              epochs=1000,\n",
    "              batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n",
    "    train_loss = model.evaluate(train_X,train_Y, batch_size=len(train_Y)) #calculate the data in test mode(Keras)\n",
    "    val_loss = model.evaluate(val_X, val_Y, batch_size=len(val_Y))\n",
    "    test_loss = model.evaluate(test_X, test_Y, batch_size=len(test_Y))\n",
    "    print(model.summary())\n",
    "    print(\"Loss for training data is\",train_loss)\n",
    "    print(\"Loss for validation data is\",val_loss)\n",
    "    print(\"Loss for test data is\",test_loss)\n",
    "    predict_Y = model.predict(test_X)\n",
    "    error_t, accuracy_t = accuracy(predict_Y, test_Y)\n",
    "    return predict_Y, error_t, accuracy_t           \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_9_input to have 3 dimensions, but got array with shape (948, 992)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0bb9a751f9ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict1_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy1_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain1_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain1_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest1_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest1_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-4f7b097587c2>\u001b[0m in \u001b[0;36mcnn_model\u001b[1;34m(rss, locations, test_rss, test_locations)\u001b[0m\n\u001b[0;32m     30\u001b[0m     model.fit(train_X, train_Y,\n\u001b[0;32m     31\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m               batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculate the data in test mode(Keras)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1359\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1232\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1235\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv1d_9_input to have 3 dimensions, but got array with shape (948, 992)"
     ]
    }
   ],
   "source": [
    "predict1_cnn, error_cnn, accuracy1_cnn = cnn_model(train1_r, train1_c, test1_r, test1_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average error:', accuracy1_cnn)\n",
    "print('minimum error:', np.amin(error_cnn), 'maximum error:', np.amax(error_cnn), 'variance:', np.var(error_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sortedcnn= np.sort(error_cnn)\n",
    "p1 = 1. *np.arange(len(error_cnn))/(len(error_cnn)-1)\n",
    "plt.plot(error_sortedcnn, p1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
