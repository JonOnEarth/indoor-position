{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#test\n",
    "#load data\n",
    "path = 'C:/Users/mys12/Desktop/northeastern/summer2018/data_Robust_Fingerprinting-master/DISTRIBUTED_OPENSOURCE/FINGERPRINTING_DB'\n",
    "\n",
    "test_rss = pd.read_csv(path + '/Training_rss_21Aug17.csv', header = 0)\n",
    "test_coord = pd.read_csv(path + '/Training_coordinates_21Aug17.csv', header = 0)\n",
    "test_rss = test_rss.replace(100, 0)\n",
    "\n",
    "train_rss = pd.read_csv(path + '/Test_rss_21Aug17.csv', header = 0)\n",
    "train_coord = pd.read_csv(path + '/Test_coordinates_21Aug17.csv', header = 0)\n",
    "train_rss = train_rss.replace(100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>100.1</th>\n",
       "      <th>100.2</th>\n",
       "      <th>100.3</th>\n",
       "      <th>100.4</th>\n",
       "      <th>100.5</th>\n",
       "      <th>100.6</th>\n",
       "      <th>100.7</th>\n",
       "      <th>100.8</th>\n",
       "      <th>100.9</th>\n",
       "      <th>...</th>\n",
       "      <th>100.939</th>\n",
       "      <th>100.940</th>\n",
       "      <th>100.941</th>\n",
       "      <th>100.942</th>\n",
       "      <th>100.943</th>\n",
       "      <th>100.944</th>\n",
       "      <th>100.945</th>\n",
       "      <th>100.946</th>\n",
       "      <th>100.947</th>\n",
       "      <th>100.948</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-64</td>\n",
       "      <td>0</td>\n",
       "      <td>-64</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 992 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   100  100.1  100.2  100.3  100.4  100.5  100.6  100.7  100.8  100.9  \\\n",
       "0    0      0      0      0      0      0      0    -64      0    -64   \n",
       "1    0      0      0      0      0      0      0      0      0      0   \n",
       "2    0      0      0      0      0      0      0      0      0      0   \n",
       "3    0      0      0      0      0      0      0      0      0      0   \n",
       "4    0      0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "    ...     100.939  100.940  100.941  100.942  100.943  100.944  100.945  \\\n",
       "0   ...           0        0        0        0        0        0        0   \n",
       "1   ...           0        0        0        0        0        0        0   \n",
       "2   ...           0        0        0        0        0        0        0   \n",
       "3   ...           0        0        0        0        0        0        0   \n",
       "4   ...           0        0        0        0        0        0        0   \n",
       "\n",
       "   100.946  100.947  100.948  \n",
       "0        0        0        0  \n",
       "1        0        0        0  \n",
       "2        0        0        0  \n",
       "3        0        0        0  \n",
       "4        0        0        0  \n",
       "\n",
       "[5 rows x 992 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_rss, train_coord], axis=1, ignore_index=True)\n",
    "test = pd.concat([test_rss, test_coord], axis=1, ignore_index=True)\n",
    "train = np.asarray(train)\n",
    "test = np.asarray(test)\n",
    "\n",
    "# first floor\n",
    "train1 = train[train[:,-1]==0.0]\n",
    "normalizer = preprocessing.Normalizer(norm='l1').fit(train1[:,:-4])\n",
    "train1_r=normalizer.transform(train1[:,:-3])\n",
    "train1_c=train1[:,-3:-1]\n",
    "train1_r.shape\n",
    "\n",
    "test1 = test[test[:,-1]==0.0]\n",
    "test1_r=normalizer.transform(test1[:,:-4])\n",
    "test1_c=test1[:,-3:-1]\n",
    "\n",
    "\n",
    "# second floor\n",
    "train2 = train[train[:,-1]==3.7]\n",
    "train2_r=scale(train2[:,:-3])\n",
    "train2_c=train2[:,-3:-1]\n",
    "\n",
    "test2 = test[test[:,-1]==3.7]\n",
    "test2_r=scale(test2[:,:-3])\n",
    "test2_c=test2[:,-3:-1]\n",
    "\n",
    "# third floor\n",
    "train3 = train[train[:,-1]==7.4]\n",
    "train3_r=train3[:,:-3]\n",
    "train3_c=train3[:,-3:-1]\n",
    "\n",
    "test3 = test[test[:,-1]==7.4]\n",
    "test3_r=scale(test2[:,:-3])\n",
    "test3_c=test2[:,-3:-1]\n",
    "\n",
    "# forth floor\n",
    "train4 = train[train[:,-1]==11.1]\n",
    "train4_r=train4[:,:-3]\n",
    "train4_c=train4[:,-3:-1]\n",
    "\n",
    "test4 = test[test[:,-1]==11.1]\n",
    "test4_r=scale(test4[:,:-3])\n",
    "test4_c=test4[:,-3:-1]\n",
    "\n",
    "# fifth floor\n",
    "train5 = train[train[:,-1]==14.8]\n",
    "train5_r=train5[:,:-3]\n",
    "train5_c=train5[:,-3:-1]\n",
    "\n",
    "test5 = test[test[:,-1]==14.8]\n",
    "test5_r=scale(test4[:,:-3])\n",
    "test5_c=test5[:,-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "train1_c[0:5]\n",
    "num_input = train1_r.shape[1]\n",
    "print(num_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata(rss, locations):\n",
    "    # the origin of the room\n",
    "    origin = np.amin(locations,axis=0)\n",
    "    #size of the room\n",
    "    room_size = np.amax(locations, axis=0)-origin\n",
    "    # position respect to origin\n",
    "    train_Yy = locations - origin\n",
    "    train_Xx = np.asarray(rss, dtype=np.float64)\n",
    "    return train_Xx, train_Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpre(train_r,train_c):\n",
    "    train1_X, train1_Y = predata(train1_r, train1_c)\n",
    "    x,y= train1_Y.T\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(rss, locations):\n",
    "    train_Xx, train_Yy = predata(rss, locations)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_Xx, train_Yy, test_size=0.25)\n",
    "    return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    error = np.sqrt(np.sum((predictions - labels)**2, 1))\n",
    "    return error, np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(rss, locations, test_rss, test_locations):\n",
    "    # get train_X, val_X, train_Y, val_Y\n",
    "    train_X, val_X, train_Y, val_Y = train_val(rss, locations)\n",
    "    test_X, test_Y = predata(test_rss, test_locations)\n",
    "    \n",
    "    # parameters\n",
    "    num_input = train_X.shape[1]# input layer size\n",
    "    act_fun = 'relu'\n",
    "    regularzation_penalty = 0.03\n",
    "    initilization_method = 'he_normal' #'random_uniform' ,'random_normal','TruncatedNormal' ,'glorot_uniform', 'glorot_nomral', 'he_normal', 'he_uniform'\n",
    "    #Optimizer\n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    # define model\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation=act_fun, input_dim=num_input, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation=act_fun, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(Dense(128, activation=act_fun, kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "#     model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='linear', kernel_initializer=initilization_method ,kernel_regularizer=regularizers.l2(regularzation_penalty)))\n",
    "\n",
    "    #Model compile\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=adam)\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "    model.fit(train_X, train_Y,\n",
    "              epochs=1000,\n",
    "              batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n",
    "    #model evaluate\n",
    "    train_loss = model.evaluate(train_X,train_Y, batch_size=len(train_Y)) #calculate the data in test mode(Keras)\n",
    "    val_loss = model.evaluate(val_X, val_Y, batch_size=len(val_Y))\n",
    "    test_loss = model.evaluate(test_X, test_Y, batch_size=len(test_Y))\n",
    "    print(\"Loss for training data is\",train_loss)\n",
    "    print(\"Loss for validation data is\",val_loss)\n",
    "    print(\"Loss for test data is\",test_loss)\n",
    "    predict_Y = model.predict(test_X)\n",
    "    error_t, accuracy_t = accuracy(predict_Y, test_Y)\n",
    "    return predict_Y, error_t, accuracy_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 948 samples, validate on 316 samples\n",
      "Epoch 1/1000\n",
      "948/948 [==============================] - 2s - loss: 4704.8767 - val_loss: 4628.3437\n",
      "Epoch 2/1000\n",
      "948/948 [==============================] - 0s - loss: 4629.1117 - val_loss: 4500.4947\n",
      "Epoch 3/1000\n",
      "948/948 [==============================] - 0s - loss: 4423.8621 - val_loss: 4183.7195\n",
      "Epoch 4/1000\n",
      "948/948 [==============================] - 0s - loss: 3974.6695 - val_loss: 3567.0133\n",
      "Epoch 5/1000\n",
      "948/948 [==============================] - 0s - loss: 3185.4014 - val_loss: 2638.4005\n",
      "Epoch 6/1000\n",
      "948/948 [==============================] - 0s - loss: 2183.6122 - val_loss: 1641.7490\n",
      "Epoch 7/1000\n",
      "948/948 [==============================] - 0s - loss: 1317.4762 - val_loss: 985.9749\n",
      "Epoch 8/1000\n",
      "948/948 [==============================] - 0s - loss: 838.9902 - val_loss: 652.4976\n",
      "Epoch 9/1000\n",
      "948/948 [==============================] - 0s - loss: 594.3403 - val_loss: 514.0273\n",
      "Epoch 10/1000\n",
      "948/948 [==============================] - 0s - loss: 495.7263 - val_loss: 457.9248\n",
      "Epoch 11/1000\n",
      "948/948 [==============================] - 0s - loss: 451.8276 - val_loss: 408.3759\n",
      "Epoch 12/1000\n",
      "948/948 [==============================] - 0s - loss: 415.8977 - val_loss: 371.9519\n",
      "Epoch 13/1000\n",
      "948/948 [==============================] - 0s - loss: 372.4836 - val_loss: 343.8715\n",
      "Epoch 14/1000\n",
      "948/948 [==============================] - 0s - loss: 351.9321 - val_loss: 322.7969\n",
      "Epoch 15/1000\n",
      "948/948 [==============================] - 0s - loss: 337.6342 - val_loss: 306.2016\n",
      "Epoch 16/1000\n",
      "948/948 [==============================] - 0s - loss: 320.6160 - val_loss: 293.6467\n",
      "Epoch 17/1000\n",
      "948/948 [==============================] - 0s - loss: 309.5904 - val_loss: 282.8267\n",
      "Epoch 18/1000\n",
      "948/948 [==============================] - 0s - loss: 295.0773 - val_loss: 274.0794\n",
      "Epoch 19/1000\n",
      "948/948 [==============================] - 0s - loss: 295.1156 - val_loss: 268.1150\n",
      "Epoch 20/1000\n",
      "948/948 [==============================] - 0s - loss: 285.8614 - val_loss: 262.3714\n",
      "Epoch 21/1000\n",
      "948/948 [==============================] - 0s - loss: 289.5017 - val_loss: 256.8059\n",
      "Epoch 22/1000\n",
      "948/948 [==============================] - 0s - loss: 278.4245 - val_loss: 250.8433\n",
      "Epoch 23/1000\n",
      "948/948 [==============================] - 0s - loss: 280.8591 - val_loss: 249.5565\n",
      "Epoch 24/1000\n",
      "948/948 [==============================] - 0s - loss: 271.4242 - val_loss: 245.1737\n",
      "Epoch 25/1000\n",
      "948/948 [==============================] - 0s - loss: 268.3489 - val_loss: 240.6608\n",
      "Epoch 26/1000\n",
      "948/948 [==============================] - 0s - loss: 270.7892 - val_loss: 238.1361\n",
      "Epoch 27/1000\n",
      "948/948 [==============================] - 0s - loss: 270.1518 - val_loss: 234.4062\n",
      "Epoch 28/1000\n",
      "948/948 [==============================] - 0s - loss: 259.8904 - val_loss: 232.6575\n",
      "Epoch 29/1000\n",
      "948/948 [==============================] - 0s - loss: 261.5599 - val_loss: 227.5127\n",
      "Epoch 30/1000\n",
      "948/948 [==============================] - 0s - loss: 264.2161 - val_loss: 225.8263\n",
      "Epoch 31/1000\n",
      "948/948 [==============================] - 0s - loss: 256.5015 - val_loss: 224.1571\n",
      "Epoch 32/1000\n",
      "948/948 [==============================] - 0s - loss: 253.5695 - val_loss: 219.5982\n",
      "Epoch 33/1000\n",
      "948/948 [==============================] - 0s - loss: 250.6773 - val_loss: 217.3046\n",
      "Epoch 34/1000\n",
      "948/948 [==============================] - 0s - loss: 241.4746 - val_loss: 214.7800\n",
      "Epoch 35/1000\n",
      "948/948 [==============================] - 0s - loss: 241.5547 - val_loss: 210.9056\n",
      "Epoch 36/1000\n",
      "948/948 [==============================] - 0s - loss: 238.8547 - val_loss: 206.8488\n",
      "Epoch 37/1000\n",
      "948/948 [==============================] - 0s - loss: 236.6250 - val_loss: 204.9878\n",
      "Epoch 38/1000\n",
      "948/948 [==============================] - 0s - loss: 229.4604 - val_loss: 200.4724\n",
      "Epoch 39/1000\n",
      "948/948 [==============================] - 0s - loss: 225.9867 - val_loss: 197.3325\n",
      "Epoch 40/1000\n",
      "948/948 [==============================] - 0s - loss: 221.3194 - val_loss: 193.9522\n",
      "Epoch 41/1000\n",
      "948/948 [==============================] - 0s - loss: 226.9531 - val_loss: 192.7173\n",
      "Epoch 42/1000\n",
      "948/948 [==============================] - 0s - loss: 224.6200 - val_loss: 189.1165\n",
      "Epoch 43/1000\n",
      "948/948 [==============================] - 0s - loss: 221.7104 - val_loss: 185.5856\n",
      "Epoch 44/1000\n",
      "948/948 [==============================] - 0s - loss: 207.8171 - val_loss: 182.7997\n",
      "Epoch 45/1000\n",
      "948/948 [==============================] - 0s - loss: 206.0731 - val_loss: 182.7058\n",
      "Epoch 46/1000\n",
      "948/948 [==============================] - 0s - loss: 211.8527 - val_loss: 176.4242\n",
      "Epoch 47/1000\n",
      "948/948 [==============================] - 0s - loss: 204.3465 - val_loss: 176.7297\n",
      "Epoch 48/1000\n",
      "948/948 [==============================] - 0s - loss: 205.6413 - val_loss: 171.4650\n",
      "Epoch 49/1000\n",
      "948/948 [==============================] - 0s - loss: 201.1305 - val_loss: 167.4953\n",
      "Epoch 50/1000\n",
      "948/948 [==============================] - 0s - loss: 198.4828 - val_loss: 166.7375\n",
      "Epoch 51/1000\n",
      "948/948 [==============================] - 0s - loss: 194.1061 - val_loss: 161.6996\n",
      "Epoch 52/1000\n",
      "948/948 [==============================] - 0s - loss: 190.0911 - val_loss: 160.0252\n",
      "Epoch 53/1000\n",
      "948/948 [==============================] - 0s - loss: 186.5326 - val_loss: 158.2531\n",
      "Epoch 54/1000\n",
      "948/948 [==============================] - 0s - loss: 190.6131 - val_loss: 155.6124\n",
      "Epoch 55/1000\n",
      "948/948 [==============================] - 0s - loss: 188.3769 - val_loss: 154.6967\n",
      "Epoch 56/1000\n",
      "948/948 [==============================] - 0s - loss: 188.5056 - val_loss: 152.8225\n",
      "Epoch 57/1000\n",
      "948/948 [==============================] - 0s - loss: 187.3089 - val_loss: 150.0161\n",
      "Epoch 58/1000\n",
      "948/948 [==============================] - 0s - loss: 184.0670 - val_loss: 149.7704\n",
      "Epoch 59/1000\n",
      "948/948 [==============================] - 0s - loss: 184.8288 - val_loss: 147.0264\n",
      "Epoch 60/1000\n",
      "948/948 [==============================] - 0s - loss: 183.8214 - val_loss: 145.7324\n",
      "Epoch 61/1000\n",
      "948/948 [==============================] - 0s - loss: 183.9615 - val_loss: 146.2005\n",
      "Epoch 62/1000\n",
      "948/948 [==============================] - 0s - loss: 177.8313 - val_loss: 146.3205\n",
      "Epoch 63/1000\n",
      "948/948 [==============================] - 0s - loss: 178.5884 - val_loss: 144.1334\n",
      "Epoch 64/1000\n",
      "948/948 [==============================] - 0s - loss: 181.4230 - val_loss: 141.0724\n",
      "Epoch 65/1000\n",
      "948/948 [==============================] - 0s - loss: 175.9946 - val_loss: 141.7886\n",
      "Epoch 66/1000\n",
      "948/948 [==============================] - 0s - loss: 176.3609 - val_loss: 140.8329\n",
      "Epoch 67/1000\n",
      "948/948 [==============================] - 0s - loss: 179.4158 - val_loss: 140.0094\n",
      "Epoch 68/1000\n",
      "948/948 [==============================] - 0s - loss: 171.0472 - val_loss: 139.0430\n",
      "Epoch 69/1000\n",
      "948/948 [==============================] - 0s - loss: 173.5198 - val_loss: 137.2269\n",
      "Epoch 70/1000\n",
      "948/948 [==============================] - 0s - loss: 168.9691 - val_loss: 138.3815\n",
      "Epoch 71/1000\n",
      "948/948 [==============================] - 0s - loss: 175.5049 - val_loss: 136.8701\n",
      "Epoch 72/1000\n",
      "948/948 [==============================] - 0s - loss: 174.6146 - val_loss: 135.4752\n",
      "Epoch 73/1000\n",
      "948/948 [==============================] - 0s - loss: 170.0327 - val_loss: 135.2672\n",
      "Epoch 74/1000\n",
      "948/948 [==============================] - 0s - loss: 168.9743 - val_loss: 138.2958\n",
      "Epoch 75/1000\n",
      "948/948 [==============================] - 0s - loss: 174.2281 - val_loss: 136.7705\n",
      "Epoch 76/1000\n",
      "948/948 [==============================] - ETA: 0s - loss: 170.710 - 0s - loss: 171.4316 - val_loss: 133.8974\n",
      "Epoch 77/1000\n",
      "948/948 [==============================] - 0s - loss: 174.4377 - val_loss: 133.4706\n",
      "Epoch 78/1000\n",
      "948/948 [==============================] - 0s - loss: 169.3405 - val_loss: 133.8495\n",
      "Epoch 79/1000\n",
      "948/948 [==============================] - 0s - loss: 171.2485 - val_loss: 133.6826\n",
      "Epoch 80/1000\n",
      "948/948 [==============================] - 0s - loss: 165.0046 - val_loss: 134.7189\n",
      "Epoch 81/1000\n",
      "948/948 [==============================] - 0s - loss: 167.1133 - val_loss: 132.7555\n",
      "Epoch 82/1000\n",
      "948/948 [==============================] - 0s - loss: 166.4956 - val_loss: 132.7857\n",
      "Epoch 83/1000\n",
      "948/948 [==============================] - 0s - loss: 167.4014 - val_loss: 130.2176\n",
      "Epoch 84/1000\n",
      "948/948 [==============================] - 0s - loss: 160.8080 - val_loss: 130.9593\n",
      "Epoch 85/1000\n",
      "948/948 [==============================] - 0s - loss: 158.9155 - val_loss: 130.4223\n",
      "Epoch 86/1000\n",
      "948/948 [==============================] - 0s - loss: 162.6936 - val_loss: 130.7761\n",
      "Epoch 87/1000\n",
      "948/948 [==============================] - 0s - loss: 163.7414 - val_loss: 129.6488\n",
      "Epoch 88/1000\n",
      "948/948 [==============================] - 0s - loss: 164.4299 - val_loss: 127.9661\n",
      "Epoch 89/1000\n",
      "948/948 [==============================] - 0s - loss: 160.5511 - val_loss: 131.3446\n",
      "Epoch 90/1000\n",
      "948/948 [==============================] - 0s - loss: 160.0149 - val_loss: 129.0629\n",
      "Epoch 91/1000\n",
      "948/948 [==============================] - 0s - loss: 165.9802 - val_loss: 128.5127\n",
      "Epoch 92/1000\n",
      "948/948 [==============================] - 0s - loss: 165.1182 - val_loss: 129.1711\n",
      "Epoch 93/1000\n",
      "948/948 [==============================] - 0s - loss: 154.7869 - val_loss: 127.6658\n",
      "Epoch 94/1000\n",
      "948/948 [==============================] - 0s - loss: 160.3930 - val_loss: 127.7290\n",
      "Epoch 95/1000\n",
      "948/948 [==============================] - 0s - loss: 164.8183 - val_loss: 127.9745\n",
      "Epoch 96/1000\n",
      "948/948 [==============================] - 0s - loss: 157.5114 - val_loss: 125.4935\n",
      "Epoch 97/1000\n",
      "948/948 [==============================] - 0s - loss: 164.5592 - val_loss: 127.5235\n",
      "Epoch 98/1000\n",
      "948/948 [==============================] - 0s - loss: 160.4004 - val_loss: 125.5813\n",
      "Epoch 99/1000\n",
      "948/948 [==============================] - 0s - loss: 165.4199 - val_loss: 125.6633\n",
      "Epoch 100/1000\n",
      "948/948 [==============================] - 0s - loss: 153.4171 - val_loss: 124.6489\n",
      "Epoch 101/1000\n",
      "948/948 [==============================] - 0s - loss: 151.8664 - val_loss: 124.7655\n",
      "Epoch 102/1000\n",
      "948/948 [==============================] - 0s - loss: 155.8532 - val_loss: 128.0799\n",
      "Epoch 103/1000\n",
      "948/948 [==============================] - 0s - loss: 154.7311 - val_loss: 122.9623\n",
      "Epoch 104/1000\n",
      "948/948 [==============================] - 0s - loss: 160.4260 - val_loss: 126.3151\n",
      "Epoch 105/1000\n",
      "948/948 [==============================] - 0s - loss: 164.0174 - val_loss: 122.9699\n",
      "Epoch 106/1000\n",
      "948/948 [==============================] - 0s - loss: 153.7855 - val_loss: 122.3671\n",
      "Epoch 107/1000\n",
      "948/948 [==============================] - 0s - loss: 161.8903 - val_loss: 125.1955\n",
      "Epoch 108/1000\n",
      "948/948 [==============================] - 0s - loss: 158.3678 - val_loss: 123.6046\n",
      "Epoch 109/1000\n",
      "948/948 [==============================] - 0s - loss: 156.0910 - val_loss: 122.7032\n",
      "Epoch 110/1000\n",
      "948/948 [==============================] - 0s - loss: 153.1008 - val_loss: 123.2103\n",
      "Epoch 111/1000\n",
      "948/948 [==============================] - 0s - loss: 150.8484 - val_loss: 121.8105\n",
      "Epoch 112/1000\n",
      "948/948 [==============================] - 0s - loss: 150.8903 - val_loss: 124.1444\n",
      "Epoch 113/1000\n",
      "948/948 [==============================] - 0s - loss: 156.7685 - val_loss: 123.5505\n",
      "Epoch 114/1000\n",
      "948/948 [==============================] - 0s - loss: 153.4597 - val_loss: 122.5633\n",
      "Epoch 115/1000\n",
      "948/948 [==============================] - 0s - loss: 151.9888 - val_loss: 121.6830\n",
      "Epoch 116/1000\n",
      "948/948 [==============================] - 0s - loss: 147.0378 - val_loss: 120.9124\n",
      "Epoch 117/1000\n",
      "948/948 [==============================] - 0s - loss: 153.1121 - val_loss: 124.9525\n",
      "Epoch 118/1000\n",
      "948/948 [==============================] - 0s - loss: 152.5502 - val_loss: 120.7510\n",
      "Epoch 119/1000\n",
      "948/948 [==============================] - 0s - loss: 155.8325 - val_loss: 121.7164\n",
      "Epoch 120/1000\n",
      "948/948 [==============================] - 0s - loss: 148.4371 - val_loss: 121.7157\n",
      "Epoch 121/1000\n",
      "948/948 [==============================] - 0s - loss: 149.3670 - val_loss: 120.1881\n",
      "Epoch 122/1000\n",
      "948/948 [==============================] - 0s - loss: 147.6297 - val_loss: 121.6292\n",
      "Epoch 123/1000\n",
      "948/948 [==============================] - 0s - loss: 153.7694 - val_loss: 118.3128\n",
      "Epoch 124/1000\n",
      "948/948 [==============================] - 0s - loss: 154.1135 - val_loss: 125.7449\n",
      "Epoch 125/1000\n",
      "948/948 [==============================] - 0s - loss: 154.5576 - val_loss: 120.0693\n",
      "Epoch 126/1000\n",
      "948/948 [==============================] - 0s - loss: 147.2333 - val_loss: 119.5951\n",
      "Epoch 127/1000\n",
      "948/948 [==============================] - 0s - loss: 146.8337 - val_loss: 119.7310\n",
      "Epoch 128/1000\n",
      "948/948 [==============================] - 0s - loss: 145.3142 - val_loss: 120.1879\n",
      "Epoch 129/1000\n",
      "948/948 [==============================] - 0s - loss: 152.7793 - val_loss: 118.9864\n",
      "Epoch 130/1000\n",
      "948/948 [==============================] - 0s - loss: 151.9694 - val_loss: 117.3904\n",
      "Epoch 131/1000\n",
      "948/948 [==============================] - 0s - loss: 149.9339 - val_loss: 119.9939\n",
      "Epoch 132/1000\n",
      "948/948 [==============================] - 0s - loss: 139.4656 - val_loss: 117.6566\n",
      "Epoch 133/1000\n",
      "948/948 [==============================] - 0s - loss: 142.5770 - val_loss: 118.3469\n",
      "Epoch 134/1000\n",
      "948/948 [==============================] - 0s - loss: 144.8043 - val_loss: 119.5922\n",
      "Epoch 135/1000\n",
      "948/948 [==============================] - 0s - loss: 148.9533 - val_loss: 117.3944\n",
      "Epoch 136/1000\n",
      "948/948 [==============================] - 0s - loss: 150.4065 - val_loss: 118.1020\n",
      "Epoch 137/1000\n",
      "948/948 [==============================] - 0s - loss: 146.5769 - val_loss: 120.9210\n",
      "Epoch 138/1000\n",
      "948/948 [==============================] - 0s - loss: 151.9568 - val_loss: 119.6714\n",
      "Epoch 139/1000\n",
      "948/948 [==============================] - 0s - loss: 151.2854 - val_loss: 117.2362\n",
      "Epoch 140/1000\n",
      "948/948 [==============================] - 0s - loss: 151.9372 - val_loss: 120.0346\n",
      "Epoch 141/1000\n",
      "948/948 [==============================] - 0s - loss: 145.4969 - val_loss: 117.9396\n",
      "Epoch 142/1000\n",
      "948/948 [==============================] - 0s - loss: 146.8199 - val_loss: 115.5954\n",
      "Epoch 143/1000\n",
      "948/948 [==============================] - 0s - loss: 146.4693 - val_loss: 122.3117\n",
      "Epoch 144/1000\n",
      "948/948 [==============================] - 0s - loss: 148.2920 - val_loss: 115.5701\n",
      "Epoch 145/1000\n",
      "948/948 [==============================] - 0s - loss: 143.8537 - val_loss: 118.6518\n",
      "Epoch 146/1000\n",
      "948/948 [==============================] - 0s - loss: 148.8674 - val_loss: 115.9033\n",
      "Epoch 147/1000\n",
      "948/948 [==============================] - 0s - loss: 148.2518 - val_loss: 118.2758\n",
      "Epoch 148/1000\n",
      "948/948 [==============================] - 0s - loss: 144.0824 - val_loss: 116.8738\n",
      "Epoch 149/1000\n",
      "948/948 [==============================] - 0s - loss: 149.3516 - val_loss: 119.5504\n",
      "Epoch 150/1000\n",
      "948/948 [==============================] - 0s - loss: 144.6360 - val_loss: 115.7530\n",
      "Epoch 151/1000\n",
      "948/948 [==============================] - 0s - loss: 144.2675 - val_loss: 114.8933\n",
      "Epoch 152/1000\n",
      "948/948 [==============================] - 0s - loss: 145.9827 - val_loss: 115.3436\n",
      "Epoch 153/1000\n",
      "948/948 [==============================] - 0s - loss: 143.4013 - val_loss: 116.1597\n",
      "Epoch 154/1000\n",
      "948/948 [==============================] - 0s - loss: 139.3192 - val_loss: 116.0353\n",
      "Epoch 155/1000\n",
      "948/948 [==============================] - 0s - loss: 144.9903 - val_loss: 113.8048\n",
      "Epoch 156/1000\n",
      "948/948 [==============================] - 0s - loss: 143.9619 - val_loss: 115.3669\n",
      "Epoch 157/1000\n",
      "948/948 [==============================] - 0s - loss: 148.0191 - val_loss: 116.7200\n",
      "Epoch 158/1000\n",
      "948/948 [==============================] - 0s - loss: 143.5829 - val_loss: 112.9331\n",
      "Epoch 159/1000\n",
      "948/948 [==============================] - 0s - loss: 146.4229 - val_loss: 115.3491\n",
      "Epoch 160/1000\n",
      "948/948 [==============================] - 0s - loss: 146.2788 - val_loss: 112.0599\n",
      "Epoch 161/1000\n",
      "948/948 [==============================] - 0s - loss: 143.5609 - val_loss: 116.3383\n",
      "Epoch 162/1000\n",
      "948/948 [==============================] - 0s - loss: 136.6504 - val_loss: 112.5658\n",
      "Epoch 163/1000\n",
      "948/948 [==============================] - 0s - loss: 147.7423 - val_loss: 116.3398\n",
      "Epoch 164/1000\n",
      "948/948 [==============================] - 0s - loss: 142.4370 - val_loss: 113.3973\n",
      "Epoch 165/1000\n",
      "948/948 [==============================] - 0s - loss: 142.8786 - val_loss: 113.0589\n",
      "Epoch 166/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 143.4827 - val_loss: 115.4350\n",
      "Epoch 167/1000\n",
      "948/948 [==============================] - 0s - loss: 142.5514 - val_loss: 112.2814\n",
      "Epoch 168/1000\n",
      "948/948 [==============================] - 0s - loss: 142.9367 - val_loss: 112.3166\n",
      "Epoch 169/1000\n",
      "948/948 [==============================] - 0s - loss: 140.0688 - val_loss: 113.7012\n",
      "Epoch 170/1000\n",
      "948/948 [==============================] - 0s - loss: 142.0701 - val_loss: 112.0389\n",
      "Epoch 171/1000\n",
      "948/948 [==============================] - 0s - loss: 140.6325 - val_loss: 111.0822\n",
      "Epoch 172/1000\n",
      "948/948 [==============================] - 0s - loss: 138.8575 - val_loss: 112.6352\n",
      "Epoch 173/1000\n",
      "948/948 [==============================] - 0s - loss: 139.1340 - val_loss: 110.7016\n",
      "Epoch 174/1000\n",
      "948/948 [==============================] - 0s - loss: 135.9778 - val_loss: 110.1231\n",
      "Epoch 175/1000\n",
      "948/948 [==============================] - 0s - loss: 135.1776 - val_loss: 115.2836\n",
      "Epoch 176/1000\n",
      "948/948 [==============================] - 0s - loss: 139.1856 - val_loss: 111.4974\n",
      "Epoch 177/1000\n",
      "948/948 [==============================] - 0s - loss: 139.3785 - val_loss: 110.8905\n",
      "Epoch 178/1000\n",
      "948/948 [==============================] - 0s - loss: 142.8090 - val_loss: 114.4754\n",
      "Epoch 179/1000\n",
      "948/948 [==============================] - 0s - loss: 133.1121 - val_loss: 111.3630\n",
      "Epoch 180/1000\n",
      "948/948 [==============================] - 0s - loss: 140.1930 - val_loss: 113.8421\n",
      "Epoch 181/1000\n",
      "948/948 [==============================] - 0s - loss: 145.0453 - val_loss: 111.4204\n",
      "Epoch 182/1000\n",
      "948/948 [==============================] - 0s - loss: 142.3381 - val_loss: 113.1834\n",
      "Epoch 183/1000\n",
      "948/948 [==============================] - 0s - loss: 136.5041 - val_loss: 110.0576\n",
      "Epoch 184/1000\n",
      "948/948 [==============================] - 0s - loss: 138.5445 - val_loss: 112.0779\n",
      "Epoch 185/1000\n",
      "948/948 [==============================] - 0s - loss: 138.4129 - val_loss: 112.2497\n",
      "Epoch 186/1000\n",
      "948/948 [==============================] - 0s - loss: 142.0921 - val_loss: 110.4231\n",
      "Epoch 187/1000\n",
      "948/948 [==============================] - 0s - loss: 140.2186 - val_loss: 110.4005\n",
      "Epoch 188/1000\n",
      "948/948 [==============================] - 0s - loss: 140.1937 - val_loss: 112.2304\n",
      "Epoch 189/1000\n",
      "948/948 [==============================] - 0s - loss: 137.4532 - val_loss: 109.8770\n",
      "Epoch 190/1000\n",
      "948/948 [==============================] - 0s - loss: 138.4122 - val_loss: 110.2659\n",
      "Epoch 191/1000\n",
      "948/948 [==============================] - 0s - loss: 132.7486 - val_loss: 109.7639\n",
      "Epoch 192/1000\n",
      "948/948 [==============================] - 0s - loss: 134.5855 - val_loss: 112.1210\n",
      "Epoch 193/1000\n",
      "948/948 [==============================] - 0s - loss: 133.8102 - val_loss: 108.8514\n",
      "Epoch 194/1000\n",
      "948/948 [==============================] - 0s - loss: 137.4834 - val_loss: 111.0175\n",
      "Epoch 195/1000\n",
      "948/948 [==============================] - 0s - loss: 133.2908 - val_loss: 108.2330\n",
      "Epoch 196/1000\n",
      "948/948 [==============================] - 0s - loss: 139.7259 - val_loss: 109.4635\n",
      "Epoch 197/1000\n",
      "948/948 [==============================] - 0s - loss: 135.4472 - val_loss: 111.8246\n",
      "Epoch 198/1000\n",
      "948/948 [==============================] - 0s - loss: 132.8018 - val_loss: 109.1642\n",
      "Epoch 199/1000\n",
      "948/948 [==============================] - 0s - loss: 134.2793 - val_loss: 109.3086\n",
      "Epoch 200/1000\n",
      "948/948 [==============================] - 0s - loss: 140.0363 - val_loss: 110.9954\n",
      "Epoch 201/1000\n",
      "948/948 [==============================] - 0s - loss: 129.6587 - val_loss: 107.5145\n",
      "Epoch 202/1000\n",
      "948/948 [==============================] - 0s - loss: 137.5850 - val_loss: 107.8548\n",
      "Epoch 203/1000\n",
      "948/948 [==============================] - 0s - loss: 132.9915 - val_loss: 115.6421\n",
      "Epoch 204/1000\n",
      "948/948 [==============================] - 0s - loss: 131.5785 - val_loss: 109.2701\n",
      "Epoch 205/1000\n",
      "948/948 [==============================] - 0s - loss: 139.2798 - val_loss: 110.3683\n",
      "Epoch 206/1000\n",
      "948/948 [==============================] - 0s - loss: 130.7169 - val_loss: 108.2430\n",
      "Epoch 207/1000\n",
      "948/948 [==============================] - 0s - loss: 131.4080 - val_loss: 109.6639\n",
      "Epoch 208/1000\n",
      "948/948 [==============================] - 0s - loss: 141.5382 - val_loss: 109.5766\n",
      "Epoch 209/1000\n",
      "948/948 [==============================] - 0s - loss: 135.4799 - val_loss: 108.0327\n",
      "Epoch 210/1000\n",
      "948/948 [==============================] - 0s - loss: 134.1912 - val_loss: 111.6030\n",
      "Epoch 211/1000\n",
      "948/948 [==============================] - 0s - loss: 133.1241 - val_loss: 108.9298\n",
      "Epoch 212/1000\n",
      "948/948 [==============================] - 0s - loss: 135.2899 - val_loss: 108.9215\n",
      "Epoch 213/1000\n",
      "948/948 [==============================] - 0s - loss: 131.7036 - val_loss: 107.9756\n",
      "Epoch 214/1000\n",
      "948/948 [==============================] - 0s - loss: 130.3171 - val_loss: 109.1506\n",
      "Epoch 215/1000\n",
      "948/948 [==============================] - 0s - loss: 131.6678 - val_loss: 109.1637\n",
      "Epoch 216/1000\n",
      "948/948 [==============================] - 0s - loss: 130.9056 - val_loss: 108.0235\n",
      "Epoch 217/1000\n",
      "948/948 [==============================] - 0s - loss: 130.9114 - val_loss: 107.2290\n",
      "Epoch 218/1000\n",
      "948/948 [==============================] - 0s - loss: 130.3402 - val_loss: 110.7847\n",
      "Epoch 219/1000\n",
      "948/948 [==============================] - 0s - loss: 135.5643 - val_loss: 109.4178\n",
      "Epoch 220/1000\n",
      "948/948 [==============================] - 0s - loss: 132.9861 - val_loss: 106.4680\n",
      "Epoch 221/1000\n",
      "948/948 [==============================] - 0s - loss: 138.0654 - val_loss: 108.5394\n",
      "Epoch 222/1000\n",
      "948/948 [==============================] - 0s - loss: 133.8812 - val_loss: 110.2947\n",
      "Epoch 223/1000\n",
      "948/948 [==============================] - 0s - loss: 136.6406 - val_loss: 106.1723\n",
      "Epoch 224/1000\n",
      "948/948 [==============================] - 0s - loss: 133.2921 - val_loss: 112.9425\n",
      "Epoch 225/1000\n",
      "948/948 [==============================] - 0s - loss: 135.1539 - val_loss: 105.9131\n",
      "Epoch 226/1000\n",
      "948/948 [==============================] - 0s - loss: 130.0504 - val_loss: 110.7673\n",
      "Epoch 227/1000\n",
      "948/948 [==============================] - 0s - loss: 130.2193 - val_loss: 106.3571\n",
      "Epoch 228/1000\n",
      "948/948 [==============================] - 0s - loss: 132.9882 - val_loss: 106.9163\n",
      "Epoch 229/1000\n",
      "948/948 [==============================] - 0s - loss: 130.5413 - val_loss: 106.0678\n",
      "Epoch 230/1000\n",
      "948/948 [==============================] - 0s - loss: 130.1564 - val_loss: 109.6911\n",
      "Epoch 231/1000\n",
      "948/948 [==============================] - 0s - loss: 137.5580 - val_loss: 106.3483\n",
      "Epoch 232/1000\n",
      "948/948 [==============================] - 0s - loss: 134.5817 - val_loss: 109.4119\n",
      "Epoch 233/1000\n",
      "948/948 [==============================] - 0s - loss: 137.7107 - val_loss: 107.9572\n",
      "Epoch 234/1000\n",
      "948/948 [==============================] - 0s - loss: 131.4168 - val_loss: 106.6158\n",
      "Epoch 235/1000\n",
      "948/948 [==============================] - 0s - loss: 129.7536 - val_loss: 104.5383\n",
      "Epoch 236/1000\n",
      "948/948 [==============================] - 0s - loss: 131.7612 - val_loss: 107.1222\n",
      "Epoch 237/1000\n",
      "948/948 [==============================] - 0s - loss: 137.0905 - val_loss: 106.7765\n",
      "Epoch 238/1000\n",
      "948/948 [==============================] - 0s - loss: 136.2063 - val_loss: 105.3388\n",
      "Epoch 239/1000\n",
      "948/948 [==============================] - 0s - loss: 130.7274 - val_loss: 107.1801\n",
      "Epoch 240/1000\n",
      "948/948 [==============================] - 0s - loss: 132.5055 - val_loss: 103.9284\n",
      "Epoch 241/1000\n",
      "948/948 [==============================] - 0s - loss: 133.1647 - val_loss: 110.0861\n",
      "Epoch 242/1000\n",
      "948/948 [==============================] - 0s - loss: 129.1028 - val_loss: 107.3660\n",
      "Epoch 243/1000\n",
      "948/948 [==============================] - 0s - loss: 127.4109 - val_loss: 104.4522\n",
      "Epoch 244/1000\n",
      "948/948 [==============================] - 0s - loss: 132.9143 - val_loss: 108.2243\n",
      "Epoch 245/1000\n",
      "948/948 [==============================] - 0s - loss: 125.6549 - val_loss: 105.7200\n",
      "Epoch 246/1000\n",
      "948/948 [==============================] - 0s - loss: 135.8033 - val_loss: 104.0992\n",
      "Epoch 247/1000\n",
      "948/948 [==============================] - 0s - loss: 129.2961 - val_loss: 103.9736\n",
      "Epoch 248/1000\n",
      "948/948 [==============================] - 0s - loss: 131.7779 - val_loss: 106.0984\n",
      "Epoch 249/1000\n",
      "948/948 [==============================] - 0s - loss: 129.6977 - val_loss: 104.9362\n",
      "Epoch 250/1000\n",
      "948/948 [==============================] - 0s - loss: 133.2998 - val_loss: 106.2808\n",
      "Epoch 251/1000\n",
      "948/948 [==============================] - 0s - loss: 130.0524 - val_loss: 105.0196\n",
      "Epoch 252/1000\n",
      "948/948 [==============================] - 0s - loss: 132.8913 - val_loss: 104.7869\n",
      "Epoch 253/1000\n",
      "948/948 [==============================] - 0s - loss: 131.4941 - val_loss: 107.2411\n",
      "Epoch 254/1000\n",
      "948/948 [==============================] - 0s - loss: 133.6530 - val_loss: 105.4400\n",
      "Epoch 255/1000\n",
      "948/948 [==============================] - 0s - loss: 125.9934 - val_loss: 107.5304\n",
      "Epoch 256/1000\n",
      "948/948 [==============================] - 0s - loss: 128.3882 - val_loss: 103.5216\n",
      "Epoch 257/1000\n",
      "948/948 [==============================] - 0s - loss: 128.3431 - val_loss: 102.9950\n",
      "Epoch 258/1000\n",
      "948/948 [==============================] - 0s - loss: 131.7325 - val_loss: 104.3421\n",
      "Epoch 259/1000\n",
      "948/948 [==============================] - 0s - loss: 121.5347 - val_loss: 106.4896\n",
      "Epoch 260/1000\n",
      "948/948 [==============================] - 0s - loss: 125.9344 - val_loss: 104.1048\n",
      "Epoch 261/1000\n",
      "948/948 [==============================] - 0s - loss: 127.3619 - val_loss: 105.7597\n",
      "Epoch 262/1000\n",
      "948/948 [==============================] - 0s - loss: 128.4062 - val_loss: 105.7607\n",
      "Epoch 263/1000\n",
      "948/948 [==============================] - 0s - loss: 131.2655 - val_loss: 102.5344\n",
      "Epoch 264/1000\n",
      "948/948 [==============================] - 0s - loss: 130.3699 - val_loss: 104.0077\n",
      "Epoch 265/1000\n",
      "948/948 [==============================] - 0s - loss: 134.7519 - val_loss: 103.7099\n",
      "Epoch 266/1000\n",
      "948/948 [==============================] - 0s - loss: 126.7132 - val_loss: 101.8198\n",
      "Epoch 267/1000\n",
      "948/948 [==============================] - 0s - loss: 125.4942 - val_loss: 107.1357\n",
      "Epoch 268/1000\n",
      "948/948 [==============================] - 0s - loss: 126.5608 - val_loss: 104.5009\n",
      "Epoch 269/1000\n",
      "948/948 [==============================] - 0s - loss: 126.0422 - val_loss: 104.4720\n",
      "Epoch 270/1000\n",
      "948/948 [==============================] - 0s - loss: 125.8157 - val_loss: 102.4374\n",
      "Epoch 271/1000\n",
      "948/948 [==============================] - 0s - loss: 128.6539 - val_loss: 105.0101\n",
      "Epoch 272/1000\n",
      "948/948 [==============================] - 0s - loss: 131.1781 - val_loss: 104.0630\n",
      "Epoch 273/1000\n",
      "948/948 [==============================] - 0s - loss: 123.2639 - val_loss: 101.6268\n",
      "Epoch 274/1000\n",
      "948/948 [==============================] - 0s - loss: 120.2639 - val_loss: 101.7233\n",
      "Epoch 275/1000\n",
      "948/948 [==============================] - 0s - loss: 127.7166 - val_loss: 103.0845\n",
      "Epoch 276/1000\n",
      "948/948 [==============================] - 0s - loss: 129.3178 - val_loss: 105.6631\n",
      "Epoch 277/1000\n",
      "948/948 [==============================] - 0s - loss: 128.3648 - val_loss: 108.7625\n",
      "Epoch 278/1000\n",
      "948/948 [==============================] - 0s - loss: 130.4573 - val_loss: 103.2381\n",
      "Epoch 279/1000\n",
      "948/948 [==============================] - 0s - loss: 129.7953 - val_loss: 106.3817\n",
      "Epoch 280/1000\n",
      "948/948 [==============================] - 0s - loss: 131.0001 - val_loss: 105.6386\n",
      "Epoch 281/1000\n",
      "948/948 [==============================] - 0s - loss: 131.5413 - val_loss: 103.0516\n",
      "Epoch 282/1000\n",
      "948/948 [==============================] - 0s - loss: 121.3578 - val_loss: 104.6906\n",
      "Epoch 283/1000\n",
      "948/948 [==============================] - 0s - loss: 123.9328 - val_loss: 102.3982\n",
      "Epoch 284/1000\n",
      "948/948 [==============================] - 0s - loss: 125.3188 - val_loss: 107.0522\n",
      "Epoch 285/1000\n",
      "948/948 [==============================] - 0s - loss: 125.5224 - val_loss: 103.6332\n",
      "Epoch 286/1000\n",
      "948/948 [==============================] - 0s - loss: 125.9976 - val_loss: 100.9117\n",
      "Epoch 287/1000\n",
      "948/948 [==============================] - 0s - loss: 129.9524 - val_loss: 104.3696\n",
      "Epoch 288/1000\n",
      "948/948 [==============================] - 0s - loss: 123.6729 - val_loss: 102.0384\n",
      "Epoch 289/1000\n",
      "948/948 [==============================] - 0s - loss: 123.4389 - val_loss: 104.4627\n",
      "Epoch 290/1000\n",
      "948/948 [==============================] - 0s - loss: 122.3579 - val_loss: 102.3812\n",
      "Epoch 291/1000\n",
      "948/948 [==============================] - 0s - loss: 124.0310 - val_loss: 101.9648\n",
      "Epoch 292/1000\n",
      "948/948 [==============================] - 0s - loss: 124.4172 - val_loss: 102.8390\n",
      "Epoch 293/1000\n",
      "948/948 [==============================] - 0s - loss: 126.6543 - val_loss: 103.3143\n",
      "Epoch 294/1000\n",
      "948/948 [==============================] - 0s - loss: 123.1512 - val_loss: 107.2784\n",
      "Epoch 295/1000\n",
      "948/948 [==============================] - 0s - loss: 121.5449 - val_loss: 101.1184\n",
      "Epoch 296/1000\n",
      "948/948 [==============================] - 0s - loss: 120.0290 - val_loss: 100.3034\n",
      "Epoch 297/1000\n",
      "948/948 [==============================] - 0s - loss: 128.7187 - val_loss: 99.8191\n",
      "Epoch 298/1000\n",
      "948/948 [==============================] - 0s - loss: 127.8215 - val_loss: 101.3404\n",
      "Epoch 299/1000\n",
      "948/948 [==============================] - 0s - loss: 128.4253 - val_loss: 100.9181\n",
      "Epoch 300/1000\n",
      "948/948 [==============================] - 0s - loss: 128.2641 - val_loss: 105.4118\n",
      "Epoch 301/1000\n",
      "948/948 [==============================] - 0s - loss: 118.7215 - val_loss: 102.4167\n",
      "Epoch 302/1000\n",
      "948/948 [==============================] - 0s - loss: 124.1238 - val_loss: 102.1945\n",
      "Epoch 303/1000\n",
      "948/948 [==============================] - 0s - loss: 129.9067 - val_loss: 100.1190\n",
      "Epoch 304/1000\n",
      "948/948 [==============================] - 0s - loss: 127.6456 - val_loss: 101.5188\n",
      "Epoch 305/1000\n",
      "948/948 [==============================] - 0s - loss: 129.8815 - val_loss: 99.7462\n",
      "Epoch 306/1000\n",
      "948/948 [==============================] - 0s - loss: 124.6279 - val_loss: 99.3968\n",
      "Epoch 307/1000\n",
      "948/948 [==============================] - 0s - loss: 125.3326 - val_loss: 101.4144\n",
      "Epoch 308/1000\n",
      "948/948 [==============================] - 0s - loss: 126.4425 - val_loss: 98.9434\n",
      "Epoch 309/1000\n",
      "948/948 [==============================] - 0s - loss: 125.0094 - val_loss: 102.0535\n",
      "Epoch 310/1000\n",
      "948/948 [==============================] - 0s - loss: 126.0318 - val_loss: 106.2823\n",
      "Epoch 311/1000\n",
      "948/948 [==============================] - 0s - loss: 125.6117 - val_loss: 99.9745\n",
      "Epoch 312/1000\n",
      "948/948 [==============================] - 0s - loss: 125.5260 - val_loss: 102.0737\n",
      "Epoch 313/1000\n",
      "948/948 [==============================] - 0s - loss: 127.2428 - val_loss: 103.4778\n",
      "Epoch 314/1000\n",
      "948/948 [==============================] - 0s - loss: 127.0913 - val_loss: 99.3048\n",
      "Epoch 315/1000\n",
      "948/948 [==============================] - 0s - loss: 123.4015 - val_loss: 101.0385\n",
      "Epoch 316/1000\n",
      "948/948 [==============================] - 0s - loss: 128.5786 - val_loss: 99.7157\n",
      "Epoch 317/1000\n",
      "948/948 [==============================] - 0s - loss: 122.7610 - val_loss: 99.4015\n",
      "Epoch 318/1000\n",
      "948/948 [==============================] - 0s - loss: 120.8679 - val_loss: 99.3003\n",
      "Epoch 319/1000\n",
      "948/948 [==============================] - 0s - loss: 121.8253 - val_loss: 97.6492\n",
      "Epoch 320/1000\n",
      "948/948 [==============================] - 0s - loss: 124.1035 - val_loss: 101.0195\n",
      "Epoch 321/1000\n",
      "948/948 [==============================] - 0s - loss: 123.4500 - val_loss: 101.6475\n",
      "Epoch 322/1000\n",
      "948/948 [==============================] - 0s - loss: 123.1331 - val_loss: 98.7960\n",
      "Epoch 323/1000\n",
      "948/948 [==============================] - 0s - loss: 125.2705 - val_loss: 97.7718\n",
      "Epoch 324/1000\n",
      "948/948 [==============================] - 0s - loss: 125.9956 - val_loss: 98.6809\n",
      "Epoch 325/1000\n",
      "948/948 [==============================] - 0s - loss: 125.0786 - val_loss: 99.3951\n",
      "Epoch 326/1000\n",
      "948/948 [==============================] - 0s - loss: 120.9352 - val_loss: 99.2194\n",
      "Epoch 327/1000\n",
      "948/948 [==============================] - 0s - loss: 117.7445 - val_loss: 97.6658\n",
      "Epoch 328/1000\n",
      "948/948 [==============================] - 0s - loss: 125.9249 - val_loss: 99.4383\n",
      "Epoch 329/1000\n",
      "948/948 [==============================] - 0s - loss: 123.8577 - val_loss: 100.9002\n",
      "Epoch 330/1000\n",
      "948/948 [==============================] - 0s - loss: 120.1025 - val_loss: 97.4386\n",
      "Epoch 331/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 122.1741 - val_loss: 98.3757\n",
      "Epoch 332/1000\n",
      "948/948 [==============================] - 0s - loss: 118.1906 - val_loss: 101.4752\n",
      "Epoch 333/1000\n",
      "948/948 [==============================] - 0s - loss: 122.9739 - val_loss: 97.0498\n",
      "Epoch 334/1000\n",
      "948/948 [==============================] - 0s - loss: 121.3745 - val_loss: 99.7218\n",
      "Epoch 335/1000\n",
      "948/948 [==============================] - 0s - loss: 114.9673 - val_loss: 97.8679\n",
      "Epoch 336/1000\n",
      "948/948 [==============================] - 0s - loss: 121.5300 - val_loss: 100.9687\n",
      "Epoch 337/1000\n",
      "948/948 [==============================] - 0s - loss: 120.6766 - val_loss: 99.0820\n",
      "Epoch 338/1000\n",
      "948/948 [==============================] - 0s - loss: 116.2155 - val_loss: 98.5355\n",
      "Epoch 339/1000\n",
      "948/948 [==============================] - 0s - loss: 123.6518 - val_loss: 104.4100\n",
      "Epoch 340/1000\n",
      "948/948 [==============================] - 0s - loss: 122.4118 - val_loss: 97.8929\n",
      "Epoch 341/1000\n",
      "948/948 [==============================] - 0s - loss: 121.6048 - val_loss: 98.3774\n",
      "Epoch 342/1000\n",
      "948/948 [==============================] - 0s - loss: 121.4686 - val_loss: 98.3326\n",
      "Epoch 343/1000\n",
      "948/948 [==============================] - 0s - loss: 126.3109 - val_loss: 99.8453\n",
      "Epoch 344/1000\n",
      "948/948 [==============================] - 0s - loss: 122.2836 - val_loss: 98.3941\n",
      "Epoch 345/1000\n",
      "948/948 [==============================] - 0s - loss: 121.2044 - val_loss: 96.9022\n",
      "Epoch 346/1000\n",
      "948/948 [==============================] - 0s - loss: 113.7468 - val_loss: 98.8387\n",
      "Epoch 347/1000\n",
      "948/948 [==============================] - 0s - loss: 118.4311 - val_loss: 97.1650\n",
      "Epoch 348/1000\n",
      "948/948 [==============================] - 0s - loss: 121.2568 - val_loss: 97.9593\n",
      "Epoch 349/1000\n",
      "948/948 [==============================] - 0s - loss: 128.2896 - val_loss: 103.2997\n",
      "Epoch 350/1000\n",
      "948/948 [==============================] - 0s - loss: 122.6267 - val_loss: 98.4136\n",
      "Epoch 351/1000\n",
      "948/948 [==============================] - 0s - loss: 125.3107 - val_loss: 96.8225\n",
      "Epoch 352/1000\n",
      "948/948 [==============================] - 0s - loss: 116.3884 - val_loss: 98.4908\n",
      "Epoch 353/1000\n",
      "948/948 [==============================] - 0s - loss: 118.8001 - val_loss: 97.8501\n",
      "Epoch 354/1000\n",
      "948/948 [==============================] - 0s - loss: 119.1199 - val_loss: 97.1946\n",
      "Epoch 355/1000\n",
      "948/948 [==============================] - 0s - loss: 119.7262 - val_loss: 98.8518\n",
      "Epoch 356/1000\n",
      "948/948 [==============================] - 0s - loss: 116.4729 - val_loss: 102.2500\n",
      "Epoch 357/1000\n",
      "948/948 [==============================] - 0s - loss: 126.9130 - val_loss: 97.8430\n",
      "Epoch 358/1000\n",
      "948/948 [==============================] - 0s - loss: 123.2324 - val_loss: 98.1855\n",
      "Epoch 359/1000\n",
      "948/948 [==============================] - 0s - loss: 116.4397 - val_loss: 100.3488\n",
      "Epoch 360/1000\n",
      "948/948 [==============================] - 0s - loss: 122.8858 - val_loss: 96.6055\n",
      "Epoch 361/1000\n",
      "948/948 [==============================] - 0s - loss: 124.3856 - val_loss: 97.3144\n",
      "Epoch 362/1000\n",
      "948/948 [==============================] - 0s - loss: 120.6189 - val_loss: 98.2000\n",
      "Epoch 363/1000\n",
      "948/948 [==============================] - 0s - loss: 117.6033 - val_loss: 100.6597\n",
      "Epoch 364/1000\n",
      "948/948 [==============================] - 0s - loss: 116.5688 - val_loss: 97.6601\n",
      "Epoch 365/1000\n",
      "948/948 [==============================] - 0s - loss: 122.7392 - val_loss: 97.7263\n",
      "Epoch 366/1000\n",
      "948/948 [==============================] - 0s - loss: 122.4507 - val_loss: 98.4623s: 125.3\n",
      "Epoch 367/1000\n",
      "948/948 [==============================] - 0s - loss: 121.3801 - val_loss: 99.4897\n",
      "Epoch 368/1000\n",
      "948/948 [==============================] - 0s - loss: 116.8104 - val_loss: 97.2215\n",
      "Epoch 369/1000\n",
      "948/948 [==============================] - 0s - loss: 115.9034 - val_loss: 97.4663\n",
      "Epoch 370/1000\n",
      "948/948 [==============================] - 0s - loss: 129.4595 - val_loss: 96.9782\n",
      "Epoch 371/1000\n",
      "948/948 [==============================] - 0s - loss: 127.1765 - val_loss: 96.9530\n",
      "Epoch 372/1000\n",
      "948/948 [==============================] - 0s - loss: 116.9687 - val_loss: 97.2000\n",
      "Epoch 373/1000\n",
      "948/948 [==============================] - 0s - loss: 118.4229 - val_loss: 98.1684\n",
      "Epoch 374/1000\n",
      "948/948 [==============================] - 0s - loss: 114.5059 - val_loss: 97.8371\n",
      "Epoch 375/1000\n",
      "948/948 [==============================] - 0s - loss: 113.2309 - val_loss: 97.9871\n",
      "Epoch 376/1000\n",
      "948/948 [==============================] - 0s - loss: 120.1131 - val_loss: 95.0560\n",
      "Epoch 377/1000\n",
      "948/948 [==============================] - 0s - loss: 123.6645 - val_loss: 101.3908\n",
      "Epoch 378/1000\n",
      "948/948 [==============================] - 0s - loss: 119.6239 - val_loss: 100.5830\n",
      "Epoch 379/1000\n",
      "948/948 [==============================] - 0s - loss: 119.6536 - val_loss: 96.8674\n",
      "Epoch 380/1000\n",
      "948/948 [==============================] - 0s - loss: 113.2988 - val_loss: 97.0275\n",
      "Epoch 381/1000\n",
      "948/948 [==============================] - 0s - loss: 119.4967 - val_loss: 100.7502\n",
      "Epoch 382/1000\n",
      "948/948 [==============================] - 0s - loss: 121.2901 - val_loss: 96.5309\n",
      "Epoch 383/1000\n",
      "948/948 [==============================] - 0s - loss: 120.4576 - val_loss: 95.5438\n",
      "Epoch 384/1000\n",
      "948/948 [==============================] - 0s - loss: 116.4985 - val_loss: 96.2463\n",
      "Epoch 385/1000\n",
      "948/948 [==============================] - 0s - loss: 116.0871 - val_loss: 98.5020\n",
      "Epoch 386/1000\n",
      "948/948 [==============================] - 0s - loss: 123.3017 - val_loss: 96.7364\n",
      "Epoch 387/1000\n",
      "948/948 [==============================] - 0s - loss: 116.1579 - val_loss: 95.5606\n",
      "Epoch 388/1000\n",
      "948/948 [==============================] - 0s - loss: 114.6175 - val_loss: 98.9400\n",
      "Epoch 389/1000\n",
      "948/948 [==============================] - 0s - loss: 117.3387 - val_loss: 95.8333\n",
      "Epoch 390/1000\n",
      "948/948 [==============================] - 0s - loss: 117.3126 - val_loss: 97.2404\n",
      "Epoch 391/1000\n",
      "948/948 [==============================] - 0s - loss: 123.8384 - val_loss: 95.0356\n",
      "Epoch 392/1000\n",
      "948/948 [==============================] - 0s - loss: 116.5968 - val_loss: 94.8271\n",
      "Epoch 393/1000\n",
      "948/948 [==============================] - 0s - loss: 119.3869 - val_loss: 94.7292\n",
      "Epoch 394/1000\n",
      "948/948 [==============================] - 0s - loss: 115.5806 - val_loss: 94.4439\n",
      "Epoch 395/1000\n",
      "948/948 [==============================] - 0s - loss: 116.7962 - val_loss: 97.5177\n",
      "Epoch 396/1000\n",
      "948/948 [==============================] - 0s - loss: 117.1478 - val_loss: 96.2300\n",
      "Epoch 397/1000\n",
      "948/948 [==============================] - 0s - loss: 113.5316 - val_loss: 96.6404\n",
      "Epoch 398/1000\n",
      "948/948 [==============================] - 0s - loss: 122.3876 - val_loss: 93.9459\n",
      "Epoch 399/1000\n",
      "948/948 [==============================] - 0s - loss: 117.2035 - val_loss: 94.9697\n",
      "Epoch 400/1000\n",
      "948/948 [==============================] - 0s - loss: 121.0762 - val_loss: 96.5598\n",
      "Epoch 401/1000\n",
      "948/948 [==============================] - 0s - loss: 118.4210 - val_loss: 94.0736\n",
      "Epoch 402/1000\n",
      "948/948 [==============================] - 0s - loss: 119.6350 - val_loss: 94.9866\n",
      "Epoch 403/1000\n",
      "948/948 [==============================] - 0s - loss: 117.0177 - val_loss: 97.9456\n",
      "Epoch 404/1000\n",
      "948/948 [==============================] - 0s - loss: 114.9911 - val_loss: 95.7392\n",
      "Epoch 405/1000\n",
      "948/948 [==============================] - 0s - loss: 116.1231 - val_loss: 95.4299\n",
      "Epoch 406/1000\n",
      "948/948 [==============================] - 0s - loss: 116.4581 - val_loss: 94.6877\n",
      "Epoch 407/1000\n",
      "948/948 [==============================] - 0s - loss: 125.3112 - val_loss: 95.7646\n",
      "Epoch 408/1000\n",
      "948/948 [==============================] - 0s - loss: 115.9663 - val_loss: 93.6617\n",
      "Epoch 409/1000\n",
      "948/948 [==============================] - 0s - loss: 119.6231 - val_loss: 93.7695\n",
      "Epoch 410/1000\n",
      "948/948 [==============================] - 0s - loss: 119.0788 - val_loss: 93.5392\n",
      "Epoch 411/1000\n",
      "948/948 [==============================] - 0s - loss: 115.5892 - val_loss: 93.2607\n",
      "Epoch 412/1000\n",
      "948/948 [==============================] - 0s - loss: 113.4891 - val_loss: 97.4376\n",
      "Epoch 413/1000\n",
      "948/948 [==============================] - 0s - loss: 115.8718 - val_loss: 96.6392\n",
      "Epoch 414/1000\n",
      "948/948 [==============================] - 0s - loss: 122.8793 - val_loss: 93.7080\n",
      "Epoch 415/1000\n",
      "948/948 [==============================] - 0s - loss: 118.2152 - val_loss: 94.3035\n",
      "Epoch 416/1000\n",
      "948/948 [==============================] - 0s - loss: 116.3876 - val_loss: 93.6301\n",
      "Epoch 417/1000\n",
      "948/948 [==============================] - 0s - loss: 112.9944 - val_loss: 93.2892\n",
      "Epoch 418/1000\n",
      "948/948 [==============================] - 0s - loss: 117.7640 - val_loss: 97.9647\n",
      "Epoch 419/1000\n",
      "948/948 [==============================] - 0s - loss: 113.1280 - val_loss: 93.9595\n",
      "Epoch 420/1000\n",
      "948/948 [==============================] - 0s - loss: 111.0635 - val_loss: 93.8908\n",
      "Epoch 421/1000\n",
      "948/948 [==============================] - 0s - loss: 116.4924 - val_loss: 92.5463\n",
      "Epoch 422/1000\n",
      "948/948 [==============================] - 0s - loss: 113.2140 - val_loss: 96.1150\n",
      "Epoch 423/1000\n",
      "948/948 [==============================] - 0s - loss: 114.4795 - val_loss: 94.6494\n",
      "Epoch 424/1000\n",
      "948/948 [==============================] - 0s - loss: 117.3618 - val_loss: 92.9949\n",
      "Epoch 425/1000\n",
      "948/948 [==============================] - 0s - loss: 114.7441 - val_loss: 94.3722\n",
      "Epoch 426/1000\n",
      "948/948 [==============================] - 0s - loss: 111.6864 - val_loss: 96.4057\n",
      "Epoch 427/1000\n",
      "948/948 [==============================] - 0s - loss: 116.5070 - val_loss: 96.7767\n",
      "Epoch 428/1000\n",
      "948/948 [==============================] - 0s - loss: 119.1323 - val_loss: 93.7543\n",
      "Epoch 429/1000\n",
      "948/948 [==============================] - 0s - loss: 115.6163 - val_loss: 92.3427\n",
      "Epoch 430/1000\n",
      "948/948 [==============================] - 0s - loss: 112.8936 - val_loss: 96.7678\n",
      "Epoch 431/1000\n",
      "948/948 [==============================] - 0s - loss: 114.1457 - val_loss: 94.0221\n",
      "Epoch 432/1000\n",
      "948/948 [==============================] - 0s - loss: 118.8903 - val_loss: 92.9713\n",
      "Epoch 433/1000\n",
      "948/948 [==============================] - 0s - loss: 113.4259 - val_loss: 95.6936\n",
      "Epoch 434/1000\n",
      "948/948 [==============================] - 0s - loss: 109.1044 - val_loss: 94.5577\n",
      "Epoch 435/1000\n",
      "948/948 [==============================] - 0s - loss: 112.5560 - val_loss: 96.2216\n",
      "Epoch 436/1000\n",
      "948/948 [==============================] - 0s - loss: 115.4560 - val_loss: 92.8482\n",
      "Epoch 437/1000\n",
      "948/948 [==============================] - 0s - loss: 112.5782 - val_loss: 95.4826\n",
      "Epoch 438/1000\n",
      "948/948 [==============================] - 0s - loss: 116.9026 - val_loss: 93.2597\n",
      "Epoch 439/1000\n",
      "948/948 [==============================] - 0s - loss: 120.9955 - val_loss: 93.3161\n",
      "Epoch 440/1000\n",
      "948/948 [==============================] - 0s - loss: 114.0391 - val_loss: 92.9300\n",
      "Epoch 441/1000\n",
      "948/948 [==============================] - 0s - loss: 115.4308 - val_loss: 91.8776\n",
      "Epoch 442/1000\n",
      "948/948 [==============================] - 0s - loss: 115.6546 - val_loss: 92.4783\n",
      "Epoch 443/1000\n",
      "948/948 [==============================] - 0s - loss: 116.5153 - val_loss: 93.2707\n",
      "Epoch 444/1000\n",
      "948/948 [==============================] - 0s - loss: 111.3689 - val_loss: 95.7278\n",
      "Epoch 445/1000\n",
      "948/948 [==============================] - 0s - loss: 116.6350 - val_loss: 93.9064\n",
      "Epoch 446/1000\n",
      "948/948 [==============================] - 0s - loss: 111.3552 - val_loss: 92.4532\n",
      "Epoch 447/1000\n",
      "948/948 [==============================] - 0s - loss: 113.2213 - val_loss: 92.0698s: 112\n",
      "Epoch 448/1000\n",
      "948/948 [==============================] - 0s - loss: 110.7101 - val_loss: 94.5591\n",
      "Epoch 449/1000\n",
      "948/948 [==============================] - 0s - loss: 114.9055 - val_loss: 93.3753\n",
      "Epoch 450/1000\n",
      "948/948 [==============================] - 0s - loss: 112.5633 - val_loss: 92.6050\n",
      "Epoch 451/1000\n",
      "948/948 [==============================] - 0s - loss: 114.5001 - val_loss: 91.2795\n",
      "Epoch 452/1000\n",
      "948/948 [==============================] - 0s - loss: 117.2435 - val_loss: 91.6005\n",
      "Epoch 453/1000\n",
      "948/948 [==============================] - 0s - loss: 113.3747 - val_loss: 92.7352\n",
      "Epoch 454/1000\n",
      "948/948 [==============================] - 0s - loss: 109.4359 - val_loss: 92.4247\n",
      "Epoch 455/1000\n",
      "948/948 [==============================] - 0s - loss: 111.7663 - val_loss: 94.2229\n",
      "Epoch 456/1000\n",
      "948/948 [==============================] - 0s - loss: 114.7419 - val_loss: 93.2613\n",
      "Epoch 457/1000\n",
      "948/948 [==============================] - 0s - loss: 113.5918 - val_loss: 96.4612\n",
      "Epoch 458/1000\n",
      "948/948 [==============================] - 0s - loss: 118.4309 - val_loss: 94.3104\n",
      "Epoch 459/1000\n",
      "948/948 [==============================] - 0s - loss: 116.0172 - val_loss: 91.9041\n",
      "Epoch 460/1000\n",
      "948/948 [==============================] - 0s - loss: 107.2153 - val_loss: 92.4607\n",
      "Epoch 461/1000\n",
      "948/948 [==============================] - 0s - loss: 113.9101 - val_loss: 91.9586\n",
      "Epoch 462/1000\n",
      "948/948 [==============================] - 0s - loss: 114.9726 - val_loss: 91.7492\n",
      "Epoch 463/1000\n",
      "948/948 [==============================] - 0s - loss: 109.8056 - val_loss: 91.5536\n",
      "Epoch 464/1000\n",
      "948/948 [==============================] - 0s - loss: 116.5188 - val_loss: 93.2821\n",
      "Epoch 465/1000\n",
      "948/948 [==============================] - 0s - loss: 109.9696 - val_loss: 96.5984\n",
      "Epoch 466/1000\n",
      "948/948 [==============================] - 0s - loss: 112.8101 - val_loss: 92.5656\n",
      "Epoch 467/1000\n",
      "948/948 [==============================] - 0s - loss: 114.6451 - val_loss: 97.7031\n",
      "Epoch 468/1000\n",
      "948/948 [==============================] - 0s - loss: 112.7444 - val_loss: 92.4357\n",
      "Epoch 469/1000\n",
      "948/948 [==============================] - 0s - loss: 110.3315 - val_loss: 90.8651\n",
      "Epoch 470/1000\n",
      "948/948 [==============================] - 0s - loss: 114.6822 - val_loss: 91.1743\n",
      "Epoch 471/1000\n",
      "948/948 [==============================] - 0s - loss: 115.2897 - val_loss: 93.9700\n",
      "Epoch 472/1000\n",
      "948/948 [==============================] - 0s - loss: 107.7278 - val_loss: 94.1063\n",
      "Epoch 473/1000\n",
      "948/948 [==============================] - 0s - loss: 111.9396 - val_loss: 92.0781\n",
      "Epoch 474/1000\n",
      "948/948 [==============================] - 0s - loss: 112.1549 - val_loss: 94.3636\n",
      "Epoch 475/1000\n",
      "948/948 [==============================] - 0s - loss: 113.0569 - val_loss: 93.5998\n",
      "Epoch 476/1000\n",
      "948/948 [==============================] - 0s - loss: 110.1508 - val_loss: 91.2063\n",
      "Epoch 477/1000\n",
      "948/948 [==============================] - 0s - loss: 105.7528 - val_loss: 91.7683\n",
      "Epoch 478/1000\n",
      "948/948 [==============================] - 0s - loss: 107.8493 - val_loss: 92.9189\n",
      "Epoch 479/1000\n",
      "948/948 [==============================] - 0s - loss: 113.6025 - val_loss: 91.6874\n",
      "Epoch 480/1000\n",
      "948/948 [==============================] - 0s - loss: 111.9577 - val_loss: 91.3041\n",
      "Epoch 481/1000\n",
      "948/948 [==============================] - 0s - loss: 115.0702 - val_loss: 94.3933\n",
      "Epoch 482/1000\n",
      "948/948 [==============================] - 0s - loss: 112.4302 - val_loss: 91.5024\n",
      "Epoch 483/1000\n",
      "948/948 [==============================] - 0s - loss: 110.0448 - val_loss: 90.7535\n",
      "Epoch 484/1000\n",
      "948/948 [==============================] - 0s - loss: 109.9827 - val_loss: 91.1716\n",
      "Epoch 485/1000\n",
      "948/948 [==============================] - 0s - loss: 111.5438 - val_loss: 91.4794\n",
      "Epoch 486/1000\n",
      "948/948 [==============================] - 0s - loss: 111.1417 - val_loss: 92.4841\n",
      "Epoch 487/1000\n",
      "948/948 [==============================] - 0s - loss: 115.2386 - val_loss: 94.4421\n",
      "Epoch 488/1000\n",
      "948/948 [==============================] - 0s - loss: 114.4217 - val_loss: 90.5541\n",
      "Epoch 489/1000\n",
      "948/948 [==============================] - 0s - loss: 109.4810 - val_loss: 90.6528\n",
      "Epoch 490/1000\n",
      "948/948 [==============================] - 0s - loss: 109.8562 - val_loss: 94.2486\n",
      "Epoch 491/1000\n",
      "948/948 [==============================] - 0s - loss: 110.8939 - val_loss: 92.4940\n",
      "Epoch 492/1000\n",
      "948/948 [==============================] - 0s - loss: 113.1140 - val_loss: 92.8740\n",
      "Epoch 493/1000\n",
      "948/948 [==============================] - 0s - loss: 108.9199 - val_loss: 92.4711\n",
      "Epoch 494/1000\n",
      "948/948 [==============================] - 0s - loss: 108.3844 - val_loss: 90.4638\n",
      "Epoch 495/1000\n",
      "948/948 [==============================] - 0s - loss: 110.5180 - val_loss: 92.5386\n",
      "Epoch 496/1000\n",
      "948/948 [==============================] - 0s - loss: 105.8769 - val_loss: 93.4311\n",
      "Epoch 497/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 111.7179 - val_loss: 94.7728\n",
      "Epoch 498/1000\n",
      "948/948 [==============================] - 0s - loss: 108.6220 - val_loss: 92.6673\n",
      "Epoch 499/1000\n",
      "948/948 [==============================] - 0s - loss: 114.1579 - val_loss: 92.1034\n",
      "Epoch 500/1000\n",
      "948/948 [==============================] - 0s - loss: 112.8154 - val_loss: 90.0220\n",
      "Epoch 501/1000\n",
      "948/948 [==============================] - 0s - loss: 104.8593 - val_loss: 90.6076\n",
      "Epoch 502/1000\n",
      "948/948 [==============================] - 0s - loss: 107.8329 - val_loss: 91.6823\n",
      "Epoch 503/1000\n",
      "948/948 [==============================] - 0s - loss: 108.0328 - val_loss: 90.4736\n",
      "Epoch 504/1000\n",
      "948/948 [==============================] - 0s - loss: 109.6575 - val_loss: 93.9941\n",
      "Epoch 505/1000\n",
      "948/948 [==============================] - 0s - loss: 109.9827 - val_loss: 91.9547\n",
      "Epoch 506/1000\n",
      "948/948 [==============================] - 0s - loss: 110.7560 - val_loss: 92.0626\n",
      "Epoch 507/1000\n",
      "948/948 [==============================] - 0s - loss: 108.6689 - val_loss: 90.0998\n",
      "Epoch 508/1000\n",
      "948/948 [==============================] - 0s - loss: 110.0275 - val_loss: 93.7458\n",
      "Epoch 509/1000\n",
      "948/948 [==============================] - 0s - loss: 109.0037 - val_loss: 91.3027\n",
      "Epoch 510/1000\n",
      "948/948 [==============================] - 0s - loss: 110.8378 - val_loss: 90.2608\n",
      "Epoch 511/1000\n",
      "948/948 [==============================] - 0s - loss: 112.3732 - val_loss: 96.0595\n",
      "Epoch 512/1000\n",
      "948/948 [==============================] - 0s - loss: 111.1352 - val_loss: 92.7862\n",
      "Epoch 513/1000\n",
      "948/948 [==============================] - 0s - loss: 111.5628 - val_loss: 92.0028\n",
      "Epoch 514/1000\n",
      "948/948 [==============================] - 0s - loss: 111.1703 - val_loss: 89.9269\n",
      "Epoch 515/1000\n",
      "948/948 [==============================] - 0s - loss: 108.4847 - val_loss: 90.1318\n",
      "Epoch 516/1000\n",
      "948/948 [==============================] - 0s - loss: 111.4958 - val_loss: 91.2229\n",
      "Epoch 517/1000\n",
      "948/948 [==============================] - 0s - loss: 113.5569 - val_loss: 91.4487\n",
      "Epoch 518/1000\n",
      "948/948 [==============================] - 0s - loss: 108.3093 - val_loss: 89.6755\n",
      "Epoch 519/1000\n",
      "948/948 [==============================] - 0s - loss: 108.7964 - val_loss: 93.2657\n",
      "Epoch 520/1000\n",
      "948/948 [==============================] - 0s - loss: 110.4697 - val_loss: 90.1353\n",
      "Epoch 521/1000\n",
      "948/948 [==============================] - 0s - loss: 108.8857 - val_loss: 89.8035\n",
      "Epoch 522/1000\n",
      "948/948 [==============================] - 0s - loss: 104.6740 - val_loss: 90.8049\n",
      "Epoch 523/1000\n",
      "948/948 [==============================] - 0s - loss: 109.6854 - val_loss: 92.5098\n",
      "Epoch 524/1000\n",
      "948/948 [==============================] - 0s - loss: 109.9440 - val_loss: 90.7720\n",
      "Epoch 525/1000\n",
      "948/948 [==============================] - ETA: 0s - loss: 110.822 - 0s - loss: 111.5250 - val_loss: 90.5744\n",
      "Epoch 526/1000\n",
      "948/948 [==============================] - 0s - loss: 107.9039 - val_loss: 92.0481\n",
      "Epoch 527/1000\n",
      "948/948 [==============================] - 0s - loss: 107.8298 - val_loss: 91.9053\n",
      "Epoch 528/1000\n",
      "948/948 [==============================] - 0s - loss: 110.2765 - val_loss: 93.4315\n",
      "Epoch 529/1000\n",
      "948/948 [==============================] - 0s - loss: 109.5431 - val_loss: 91.6228\n",
      "Epoch 530/1000\n",
      "948/948 [==============================] - 0s - loss: 110.2870 - val_loss: 90.1941\n",
      "Epoch 531/1000\n",
      "948/948 [==============================] - 0s - loss: 113.1530 - val_loss: 89.7875\n",
      "Epoch 532/1000\n",
      "948/948 [==============================] - 0s - loss: 106.7237 - val_loss: 89.8068\n",
      "Epoch 533/1000\n",
      "948/948 [==============================] - 0s - loss: 111.5097 - val_loss: 89.2736\n",
      "Epoch 534/1000\n",
      "948/948 [==============================] - 0s - loss: 109.0439 - val_loss: 91.5321\n",
      "Epoch 535/1000\n",
      "948/948 [==============================] - 0s - loss: 108.3300 - val_loss: 92.4056\n",
      "Epoch 536/1000\n",
      "948/948 [==============================] - 0s - loss: 111.1797 - val_loss: 89.1506\n",
      "Epoch 537/1000\n",
      "948/948 [==============================] - 0s - loss: 113.1884 - val_loss: 88.7637\n",
      "Epoch 538/1000\n",
      "948/948 [==============================] - 0s - loss: 107.3046 - val_loss: 89.6420\n",
      "Epoch 539/1000\n",
      "948/948 [==============================] - 0s - loss: 104.0769 - val_loss: 92.9294\n",
      "Epoch 540/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1736 - val_loss: 90.7897\n",
      "Epoch 541/1000\n",
      "948/948 [==============================] - 0s - loss: 109.8227 - val_loss: 88.6977\n",
      "Epoch 542/1000\n",
      "948/948 [==============================] - 0s - loss: 109.2397 - val_loss: 88.1888\n",
      "Epoch 543/1000\n",
      "948/948 [==============================] - 0s - loss: 106.2408 - val_loss: 92.3466\n",
      "Epoch 544/1000\n",
      "948/948 [==============================] - 0s - loss: 106.2154 - val_loss: 87.8962\n",
      "Epoch 545/1000\n",
      "948/948 [==============================] - 0s - loss: 108.5473 - val_loss: 93.2043\n",
      "Epoch 546/1000\n",
      "948/948 [==============================] - 0s - loss: 111.8363 - val_loss: 89.2406\n",
      "Epoch 547/1000\n",
      "948/948 [==============================] - 0s - loss: 105.6397 - val_loss: 96.4144\n",
      "Epoch 548/1000\n",
      "948/948 [==============================] - 0s - loss: 106.4464 - val_loss: 91.9589\n",
      "Epoch 549/1000\n",
      "948/948 [==============================] - 0s - loss: 108.8639 - val_loss: 88.9212\n",
      "Epoch 550/1000\n",
      "948/948 [==============================] - 0s - loss: 104.3427 - val_loss: 89.9627\n",
      "Epoch 551/1000\n",
      "948/948 [==============================] - 0s - loss: 107.9874 - val_loss: 89.3754\n",
      "Epoch 552/1000\n",
      "948/948 [==============================] - 0s - loss: 107.9980 - val_loss: 91.4173\n",
      "Epoch 553/1000\n",
      "948/948 [==============================] - 0s - loss: 105.2110 - val_loss: 89.1053\n",
      "Epoch 554/1000\n",
      "948/948 [==============================] - 0s - loss: 110.6890 - val_loss: 89.0476\n",
      "Epoch 555/1000\n",
      "948/948 [==============================] - 0s - loss: 107.6645 - val_loss: 89.7739\n",
      "Epoch 556/1000\n",
      "948/948 [==============================] - 0s - loss: 113.2619 - val_loss: 90.4485\n",
      "Epoch 557/1000\n",
      "948/948 [==============================] - 0s - loss: 110.3332 - val_loss: 89.1024\n",
      "Epoch 558/1000\n",
      "948/948 [==============================] - 0s - loss: 107.6149 - val_loss: 89.7710\n",
      "Epoch 559/1000\n",
      "948/948 [==============================] - 0s - loss: 107.9648 - val_loss: 88.0642\n",
      "Epoch 560/1000\n",
      "948/948 [==============================] - 0s - loss: 111.5965 - val_loss: 87.9935\n",
      "Epoch 561/1000\n",
      "948/948 [==============================] - 0s - loss: 111.4831 - val_loss: 87.8458\n",
      "Epoch 562/1000\n",
      "948/948 [==============================] - 0s - loss: 108.7944 - val_loss: 88.3766\n",
      "Epoch 563/1000\n",
      "948/948 [==============================] - 0s - loss: 105.7795 - val_loss: 88.3376\n",
      "Epoch 564/1000\n",
      "948/948 [==============================] - 0s - loss: 112.7481 - val_loss: 88.2160\n",
      "Epoch 565/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1403 - val_loss: 92.8997\n",
      "Epoch 566/1000\n",
      "948/948 [==============================] - 0s - loss: 107.8316 - val_loss: 87.8215\n",
      "Epoch 567/1000\n",
      "948/948 [==============================] - 0s - loss: 108.4983 - val_loss: 87.6545\n",
      "Epoch 568/1000\n",
      "948/948 [==============================] - 0s - loss: 106.9011 - val_loss: 92.5710\n",
      "Epoch 569/1000\n",
      "948/948 [==============================] - 0s - loss: 109.5655 - val_loss: 89.5250\n",
      "Epoch 570/1000\n",
      "948/948 [==============================] - 0s - loss: 108.7190 - val_loss: 88.4130\n",
      "Epoch 571/1000\n",
      "948/948 [==============================] - 0s - loss: 107.0576 - val_loss: 88.7734\n",
      "Epoch 572/1000\n",
      "948/948 [==============================] - 0s - loss: 111.9557 - val_loss: 88.4872\n",
      "Epoch 573/1000\n",
      "948/948 [==============================] - 0s - loss: 103.6332 - val_loss: 89.7976\n",
      "Epoch 574/1000\n",
      "948/948 [==============================] - 0s - loss: 110.1414 - val_loss: 89.6345\n",
      "Epoch 575/1000\n",
      "948/948 [==============================] - 0s - loss: 104.3041 - val_loss: 90.3008\n",
      "Epoch 576/1000\n",
      "948/948 [==============================] - 0s - loss: 112.3434 - val_loss: 91.0031\n",
      "Epoch 577/1000\n",
      "948/948 [==============================] - 0s - loss: 110.6664 - val_loss: 87.4065\n",
      "Epoch 578/1000\n",
      "948/948 [==============================] - 0s - loss: 108.9520 - val_loss: 90.4362\n",
      "Epoch 579/1000\n",
      "948/948 [==============================] - 0s - loss: 105.9445 - val_loss: 90.9668\n",
      "Epoch 580/1000\n",
      "948/948 [==============================] - 0s - loss: 107.3295 - val_loss: 87.9323\n",
      "Epoch 581/1000\n",
      "948/948 [==============================] - 0s - loss: 108.1032 - val_loss: 87.6024\n",
      "Epoch 582/1000\n",
      "948/948 [==============================] - 0s - loss: 106.3328 - val_loss: 89.0125\n",
      "Epoch 583/1000\n",
      "948/948 [==============================] - 0s - loss: 106.4152 - val_loss: 89.1112\n",
      "Epoch 584/1000\n",
      "948/948 [==============================] - 0s - loss: 107.2721 - val_loss: 86.7741\n",
      "Epoch 585/1000\n",
      "948/948 [==============================] - 0s - loss: 109.8337 - val_loss: 89.4801\n",
      "Epoch 586/1000\n",
      "948/948 [==============================] - 0s - loss: 110.9888 - val_loss: 87.5246\n",
      "Epoch 587/1000\n",
      "948/948 [==============================] - 0s - loss: 107.5152 - val_loss: 87.4923\n",
      "Epoch 588/1000\n",
      "948/948 [==============================] - 0s - loss: 103.4778 - val_loss: 90.7559\n",
      "Epoch 589/1000\n",
      "948/948 [==============================] - 0s - loss: 106.9885 - val_loss: 89.4990\n",
      "Epoch 590/1000\n",
      "948/948 [==============================] - 0s - loss: 107.8470 - val_loss: 88.8079\n",
      "Epoch 591/1000\n",
      "948/948 [==============================] - 0s - loss: 104.0996 - val_loss: 88.5161\n",
      "Epoch 592/1000\n",
      "948/948 [==============================] - 0s - loss: 109.2490 - val_loss: 90.9246\n",
      "Epoch 593/1000\n",
      "948/948 [==============================] - 0s - loss: 107.5706 - val_loss: 88.7356\n",
      "Epoch 594/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7656 - val_loss: 88.9106\n",
      "Epoch 595/1000\n",
      "948/948 [==============================] - 0s - loss: 105.8312 - val_loss: 88.9025\n",
      "Epoch 596/1000\n",
      "948/948 [==============================] - 0s - loss: 106.6363 - val_loss: 89.8540\n",
      "Epoch 597/1000\n",
      "948/948 [==============================] - 0s - loss: 105.4072 - val_loss: 90.2548\n",
      "Epoch 598/1000\n",
      "948/948 [==============================] - 0s - loss: 106.3943 - val_loss: 89.3549\n",
      "Epoch 599/1000\n",
      "948/948 [==============================] - 0s - loss: 109.3955 - val_loss: 87.1187\n",
      "Epoch 600/1000\n",
      "948/948 [==============================] - 0s - loss: 104.7460 - val_loss: 86.6421\n",
      "Epoch 601/1000\n",
      "948/948 [==============================] - 0s - loss: 107.0003 - val_loss: 89.9861\n",
      "Epoch 602/1000\n",
      "948/948 [==============================] - 0s - loss: 106.0065 - val_loss: 87.7337\n",
      "Epoch 603/1000\n",
      "948/948 [==============================] - 0s - loss: 102.0347 - val_loss: 87.2776\n",
      "Epoch 604/1000\n",
      "948/948 [==============================] - 0s - loss: 106.3539 - val_loss: 88.0360\n",
      "Epoch 605/1000\n",
      "948/948 [==============================] - 0s - loss: 109.1489 - val_loss: 87.0670\n",
      "Epoch 606/1000\n",
      "948/948 [==============================] - 0s - loss: 106.4379 - val_loss: 86.8341\n",
      "Epoch 607/1000\n",
      "948/948 [==============================] - 0s - loss: 107.0402 - val_loss: 93.7244\n",
      "Epoch 608/1000\n",
      "948/948 [==============================] - 0s - loss: 106.1347 - val_loss: 87.7728\n",
      "Epoch 609/1000\n",
      "948/948 [==============================] - 0s - loss: 105.7513 - val_loss: 88.7021\n",
      "Epoch 610/1000\n",
      "948/948 [==============================] - 0s - loss: 104.4814 - val_loss: 88.0157\n",
      "Epoch 611/1000\n",
      "948/948 [==============================] - 0s - loss: 104.1355 - val_loss: 88.1023\n",
      "Epoch 612/1000\n",
      "948/948 [==============================] - 0s - loss: 106.8169 - val_loss: 87.1413\n",
      "Epoch 613/1000\n",
      "948/948 [==============================] - 0s - loss: 107.3208 - val_loss: 86.4084\n",
      "Epoch 614/1000\n",
      "948/948 [==============================] - 0s - loss: 108.5235 - val_loss: 89.3749\n",
      "Epoch 615/1000\n",
      "948/948 [==============================] - 0s - loss: 109.2836 - val_loss: 90.5248\n",
      "Epoch 616/1000\n",
      "948/948 [==============================] - 0s - loss: 106.8396 - val_loss: 88.9658\n",
      "Epoch 617/1000\n",
      "948/948 [==============================] - 0s - loss: 102.6252 - val_loss: 85.9192\n",
      "Epoch 618/1000\n",
      "948/948 [==============================] - 0s - loss: 103.1588 - val_loss: 85.2636\n",
      "Epoch 619/1000\n",
      "948/948 [==============================] - 0s - loss: 113.4006 - val_loss: 88.0422\n",
      "Epoch 620/1000\n",
      "948/948 [==============================] - 0s - loss: 106.1461 - val_loss: 86.8394\n",
      "Epoch 621/1000\n",
      "948/948 [==============================] - 0s - loss: 104.4475 - val_loss: 89.3485\n",
      "Epoch 622/1000\n",
      "948/948 [==============================] - 0s - loss: 103.6101 - val_loss: 86.8953\n",
      "Epoch 623/1000\n",
      "948/948 [==============================] - 0s - loss: 104.9466 - val_loss: 89.8899\n",
      "Epoch 624/1000\n",
      "948/948 [==============================] - 0s - loss: 104.7639 - val_loss: 89.5729\n",
      "Epoch 625/1000\n",
      "948/948 [==============================] - 0s - loss: 102.4632 - val_loss: 86.2896\n",
      "Epoch 626/1000\n",
      "948/948 [==============================] - 0s - loss: 104.7895 - val_loss: 86.5813\n",
      "Epoch 627/1000\n",
      "948/948 [==============================] - 0s - loss: 105.2062 - val_loss: 86.8733\n",
      "Epoch 628/1000\n",
      "948/948 [==============================] - 0s - loss: 105.3059 - val_loss: 88.9796\n",
      "Epoch 629/1000\n",
      "948/948 [==============================] - 0s - loss: 102.0566 - val_loss: 86.3751\n",
      "Epoch 630/1000\n",
      "948/948 [==============================] - 0s - loss: 107.3456 - val_loss: 89.7119\n",
      "Epoch 631/1000\n",
      "948/948 [==============================] - 0s - loss: 104.0052 - val_loss: 88.1503\n",
      "Epoch 632/1000\n",
      "948/948 [==============================] - 0s - loss: 105.9178 - val_loss: 86.6212\n",
      "Epoch 633/1000\n",
      "948/948 [==============================] - 0s - loss: 102.6904 - val_loss: 85.7947\n",
      "Epoch 634/1000\n",
      "948/948 [==============================] - 0s - loss: 102.8286 - val_loss: 86.0850\n",
      "Epoch 635/1000\n",
      "948/948 [==============================] - 0s - loss: 108.0939 - val_loss: 87.7831\n",
      "Epoch 636/1000\n",
      "948/948 [==============================] - 0s - loss: 107.4090 - val_loss: 86.6788\n",
      "Epoch 637/1000\n",
      "948/948 [==============================] - 0s - loss: 102.9651 - val_loss: 87.7199\n",
      "Epoch 638/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8264 - val_loss: 88.7232\n",
      "Epoch 639/1000\n",
      "948/948 [==============================] - 0s - loss: 103.6865 - val_loss: 86.7837\n",
      "Epoch 640/1000\n",
      "948/948 [==============================] - 0s - loss: 101.9242 - val_loss: 86.9578\n",
      "Epoch 641/1000\n",
      "948/948 [==============================] - 0s - loss: 103.8458 - val_loss: 86.5181\n",
      "Epoch 642/1000\n",
      "948/948 [==============================] - 0s - loss: 102.2186 - val_loss: 88.3283\n",
      "Epoch 643/1000\n",
      "948/948 [==============================] - 0s - loss: 104.3433 - val_loss: 86.0598\n",
      "Epoch 644/1000\n",
      "948/948 [==============================] - 0s - loss: 106.5905 - val_loss: 88.4773\n",
      "Epoch 645/1000\n",
      "948/948 [==============================] - 0s - loss: 106.7841 - val_loss: 87.9490\n",
      "Epoch 646/1000\n",
      "948/948 [==============================] - 0s - loss: 104.6861 - val_loss: 85.8197\n",
      "Epoch 647/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0892 - val_loss: 86.2027\n",
      "Epoch 648/1000\n",
      "948/948 [==============================] - 0s - loss: 102.1092 - val_loss: 87.2357\n",
      "Epoch 649/1000\n",
      "948/948 [==============================] - 0s - loss: 107.4901 - val_loss: 87.3972\n",
      "Epoch 650/1000\n",
      "948/948 [==============================] - 0s - loss: 104.0734 - val_loss: 87.3670\n",
      "Epoch 651/1000\n",
      "948/948 [==============================] - 0s - loss: 106.2778 - val_loss: 88.3954\n",
      "Epoch 652/1000\n",
      "948/948 [==============================] - 0s - loss: 111.6812 - val_loss: 85.9894\n",
      "Epoch 653/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2325 - val_loss: 84.8622\n",
      "Epoch 654/1000\n",
      "948/948 [==============================] - 0s - loss: 108.3883 - val_loss: 85.1656\n",
      "Epoch 655/1000\n",
      "948/948 [==============================] - 0s - loss: 107.9108 - val_loss: 86.9572\n",
      "Epoch 656/1000\n",
      "948/948 [==============================] - 0s - loss: 104.3968 - val_loss: 87.5867\n",
      "Epoch 657/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1220 - val_loss: 87.3216\n",
      "Epoch 658/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1714 - val_loss: 87.4928\n",
      "Epoch 659/1000\n",
      "948/948 [==============================] - 0s - loss: 104.4119 - val_loss: 85.6166\n",
      "Epoch 660/1000\n",
      "948/948 [==============================] - 0s - loss: 105.5842 - val_loss: 86.4456\n",
      "Epoch 661/1000\n",
      "948/948 [==============================] - 0s - loss: 105.6469 - val_loss: 85.9531\n",
      "Epoch 662/1000\n",
      "948/948 [==============================] - 0s - loss: 103.1021 - val_loss: 89.8345\n",
      "Epoch 663/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 109.2332 - val_loss: 89.7245\n",
      "Epoch 664/1000\n",
      "948/948 [==============================] - 0s - loss: 102.0156 - val_loss: 87.4516\n",
      "Epoch 665/1000\n",
      "948/948 [==============================] - 0s - loss: 102.4388 - val_loss: 86.4714\n",
      "Epoch 666/1000\n",
      "948/948 [==============================] - 0s - loss: 105.2812 - val_loss: 89.7322\n",
      "Epoch 667/1000\n",
      "948/948 [==============================] - 0s - loss: 106.1364 - val_loss: 85.6931\n",
      "Epoch 668/1000\n",
      "948/948 [==============================] - 0s - loss: 103.1010 - val_loss: 85.3733\n",
      "Epoch 669/1000\n",
      "948/948 [==============================] - 0s - loss: 107.8560 - val_loss: 85.7740\n",
      "Epoch 670/1000\n",
      "948/948 [==============================] - 0s - loss: 103.9697 - val_loss: 86.5445\n",
      "Epoch 671/1000\n",
      "948/948 [==============================] - 0s - loss: 106.3912 - val_loss: 87.5569\n",
      "Epoch 672/1000\n",
      "948/948 [==============================] - 0s - loss: 103.8055 - val_loss: 86.0235\n",
      "Epoch 673/1000\n",
      "948/948 [==============================] - 0s - loss: 101.4844 - val_loss: 85.9585\n",
      "Epoch 674/1000\n",
      "948/948 [==============================] - 0s - loss: 106.5615 - val_loss: 85.6694\n",
      "Epoch 675/1000\n",
      "948/948 [==============================] - 0s - loss: 105.8719 - val_loss: 85.5147\n",
      "Epoch 676/1000\n",
      "948/948 [==============================] - 0s - loss: 101.1532 - val_loss: 84.2982\n",
      "Epoch 677/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2148 - val_loss: 86.3576\n",
      "Epoch 678/1000\n",
      "948/948 [==============================] - 0s - loss: 103.1043 - val_loss: 85.2323\n",
      "Epoch 679/1000\n",
      "948/948 [==============================] - 0s - loss: 103.2220 - val_loss: 84.9524\n",
      "Epoch 680/1000\n",
      "948/948 [==============================] - 0s - loss: 101.5764 - val_loss: 85.3389\n",
      "Epoch 681/1000\n",
      "948/948 [==============================] - 0s - loss: 102.0264 - val_loss: 87.2606\n",
      "Epoch 682/1000\n",
      "948/948 [==============================] - 0s - loss: 104.2208 - val_loss: 86.8007\n",
      "Epoch 683/1000\n",
      "948/948 [==============================] - 0s - loss: 103.3819 - val_loss: 89.4179\n",
      "Epoch 684/1000\n",
      "948/948 [==============================] - 0s - loss: 101.3810 - val_loss: 84.7205\n",
      "Epoch 685/1000\n",
      "948/948 [==============================] - 0s - loss: 109.0048 - val_loss: 85.5067\n",
      "Epoch 686/1000\n",
      "948/948 [==============================] - 0s - loss: 107.5480 - val_loss: 87.1801\n",
      "Epoch 687/1000\n",
      "948/948 [==============================] - 0s - loss: 104.7967 - val_loss: 86.0233\n",
      "Epoch 688/1000\n",
      "948/948 [==============================] - 0s - loss: 104.7644 - val_loss: 85.4340\n",
      "Epoch 689/1000\n",
      "948/948 [==============================] - 0s - loss: 104.3089 - val_loss: 85.4726\n",
      "Epoch 690/1000\n",
      "948/948 [==============================] - 0s - loss: 104.5033 - val_loss: 87.3039\n",
      "Epoch 691/1000\n",
      "948/948 [==============================] - 0s - loss: 107.1969 - val_loss: 88.2564\n",
      "Epoch 692/1000\n",
      "948/948 [==============================] - 0s - loss: 104.4012 - val_loss: 85.0540\n",
      "Epoch 693/1000\n",
      "948/948 [==============================] - 0s - loss: 103.7109 - val_loss: 86.4953\n",
      "Epoch 694/1000\n",
      "948/948 [==============================] - 0s - loss: 103.2779 - val_loss: 85.8321\n",
      "Epoch 695/1000\n",
      "948/948 [==============================] - 0s - loss: 103.8880 - val_loss: 85.7447\n",
      "Epoch 696/1000\n",
      "948/948 [==============================] - 0s - loss: 108.0014 - val_loss: 87.4806\n",
      "Epoch 697/1000\n",
      "948/948 [==============================] - 0s - loss: 100.2233 - val_loss: 86.1387\n",
      "Epoch 698/1000\n",
      "948/948 [==============================] - 0s - loss: 105.5257 - val_loss: 85.0658\n",
      "Epoch 699/1000\n",
      "948/948 [==============================] - 0s - loss: 98.9704 - val_loss: 85.9102ss: 10\n",
      "Epoch 700/1000\n",
      "948/948 [==============================] - 0s - loss: 107.7099 - val_loss: 84.4316\n",
      "Epoch 701/1000\n",
      "948/948 [==============================] - 0s - loss: 107.6773 - val_loss: 85.9501\n",
      "Epoch 702/1000\n",
      "948/948 [==============================] - 0s - loss: 103.2279 - val_loss: 87.5100\n",
      "Epoch 703/1000\n",
      "948/948 [==============================] - 0s - loss: 104.0003 - val_loss: 87.2137\n",
      "Epoch 704/1000\n",
      "948/948 [==============================] - 0s - loss: 103.2300 - val_loss: 84.6443\n",
      "Epoch 705/1000\n",
      "948/948 [==============================] - 0s - loss: 105.4578 - val_loss: 87.1743s: 106.\n",
      "Epoch 706/1000\n",
      "948/948 [==============================] - 0s - loss: 104.2927 - val_loss: 88.0845\n",
      "Epoch 707/1000\n",
      "948/948 [==============================] - 0s - loss: 102.8614 - val_loss: 85.4842\n",
      "Epoch 708/1000\n",
      "948/948 [==============================] - 0s - loss: 102.7105 - val_loss: 85.4323\n",
      "Epoch 709/1000\n",
      "948/948 [==============================] - 0s - loss: 101.1874 - val_loss: 87.7075\n",
      "Epoch 710/1000\n",
      "948/948 [==============================] - 0s - loss: 101.3250 - val_loss: 85.8025\n",
      "Epoch 711/1000\n",
      "948/948 [==============================] - 0s - loss: 102.8630 - val_loss: 84.2772s: 102\n",
      "Epoch 712/1000\n",
      "948/948 [==============================] - 0s - loss: 100.2176 - val_loss: 83.9710\n",
      "Epoch 713/1000\n",
      "948/948 [==============================] - 0s - loss: 104.6941 - val_loss: 85.6318\n",
      "Epoch 714/1000\n",
      "948/948 [==============================] - 0s - loss: 106.8751 - val_loss: 84.7088\n",
      "Epoch 715/1000\n",
      "948/948 [==============================] - 0s - loss: 104.6887 - val_loss: 84.7156\n",
      "Epoch 716/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7570 - val_loss: 86.4114\n",
      "Epoch 717/1000\n",
      "948/948 [==============================] - 0s - loss: 101.9117 - val_loss: 89.6171\n",
      "Epoch 718/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1894 - val_loss: 85.5430\n",
      "Epoch 719/1000\n",
      "948/948 [==============================] - 0s - loss: 104.8349 - val_loss: 85.3343\n",
      "Epoch 720/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2672 - val_loss: 85.0374\n",
      "Epoch 721/1000\n",
      "948/948 [==============================] - 0s - loss: 101.8276 - val_loss: 84.0261\n",
      "Epoch 722/1000\n",
      "948/948 [==============================] - 0s - loss: 101.6702 - val_loss: 86.5522\n",
      "Epoch 723/1000\n",
      "948/948 [==============================] - 0s - loss: 102.1236 - val_loss: 89.7984\n",
      "Epoch 724/1000\n",
      "948/948 [==============================] - 0s - loss: 101.3085 - val_loss: 86.7589\n",
      "Epoch 725/1000\n",
      "948/948 [==============================] - 0s - loss: 99.5600 - val_loss: 83.7964\n",
      "Epoch 726/1000\n",
      "948/948 [==============================] - 0s - loss: 98.0142 - val_loss: 85.9915\n",
      "Epoch 727/1000\n",
      "948/948 [==============================] - 0s - loss: 103.5170 - val_loss: 85.8782\n",
      "Epoch 728/1000\n",
      "948/948 [==============================] - 0s - loss: 105.5388 - val_loss: 86.3236\n",
      "Epoch 729/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1772 - val_loss: 87.2110\n",
      "Epoch 730/1000\n",
      "948/948 [==============================] - 0s - loss: 102.8697 - val_loss: 85.5785\n",
      "Epoch 731/1000\n",
      "948/948 [==============================] - 0s - loss: 105.3163 - val_loss: 85.9194\n",
      "Epoch 732/1000\n",
      "948/948 [==============================] - 0s - loss: 98.9685 - val_loss: 84.8530\n",
      "Epoch 733/1000\n",
      "948/948 [==============================] - 0s - loss: 97.2396 - val_loss: 86.4486\n",
      "Epoch 734/1000\n",
      "948/948 [==============================] - 0s - loss: 97.5448 - val_loss: 85.3117\n",
      "Epoch 735/1000\n",
      "948/948 [==============================] - 0s - loss: 99.6388 - val_loss: 84.1610\n",
      "Epoch 736/1000\n",
      "948/948 [==============================] - 0s - loss: 99.2807 - val_loss: 85.7786\n",
      "Epoch 737/1000\n",
      "948/948 [==============================] - 0s - loss: 100.2877 - val_loss: 85.7333\n",
      "Epoch 738/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4678 - val_loss: 88.0460\n",
      "Epoch 739/1000\n",
      "948/948 [==============================] - 0s - loss: 106.0369 - val_loss: 87.7466\n",
      "Epoch 740/1000\n",
      "948/948 [==============================] - 0s - loss: 104.2546 - val_loss: 86.2656\n",
      "Epoch 741/1000\n",
      "948/948 [==============================] - 0s - loss: 102.0606 - val_loss: 84.1486\n",
      "Epoch 742/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4802 - val_loss: 85.0072\n",
      "Epoch 743/1000\n",
      "948/948 [==============================] - 0s - loss: 103.3358 - val_loss: 86.5389\n",
      "Epoch 744/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0906 - val_loss: 84.3232\n",
      "Epoch 745/1000\n",
      "948/948 [==============================] - 0s - loss: 103.4196 - val_loss: 84.7221\n",
      "Epoch 746/1000\n",
      "948/948 [==============================] - 0s - loss: 105.2489 - val_loss: 85.9358\n",
      "Epoch 747/1000\n",
      "948/948 [==============================] - 0s - loss: 100.0318 - val_loss: 84.1397\n",
      "Epoch 748/1000\n",
      "948/948 [==============================] - 0s - loss: 99.2341 - val_loss: 85.1886\n",
      "Epoch 749/1000\n",
      "948/948 [==============================] - 0s - loss: 106.5629 - val_loss: 83.3786\n",
      "Epoch 750/1000\n",
      "948/948 [==============================] - 0s - loss: 100.8802 - val_loss: 83.9571\n",
      "Epoch 751/1000\n",
      "948/948 [==============================] - 0s - loss: 100.4051 - val_loss: 88.1243\n",
      "Epoch 752/1000\n",
      "948/948 [==============================] - 0s - loss: 100.1311 - val_loss: 84.0826\n",
      "Epoch 753/1000\n",
      "948/948 [==============================] - 0s - loss: 102.2294 - val_loss: 83.6970\n",
      "Epoch 754/1000\n",
      "948/948 [==============================] - 0s - loss: 104.4881 - val_loss: 85.5770\n",
      "Epoch 755/1000\n",
      "948/948 [==============================] - 0s - loss: 99.0558 - val_loss: 86.2645\n",
      "Epoch 756/1000\n",
      "948/948 [==============================] - 0s - loss: 98.1394 - val_loss: 84.2243\n",
      "Epoch 757/1000\n",
      "948/948 [==============================] - 0s - loss: 100.5187 - val_loss: 88.1408\n",
      "Epoch 758/1000\n",
      "948/948 [==============================] - 0s - loss: 101.3442 - val_loss: 85.7716\n",
      "Epoch 759/1000\n",
      "948/948 [==============================] - 0s - loss: 97.2454 - val_loss: 85.9066\n",
      "Epoch 760/1000\n",
      "948/948 [==============================] - 0s - loss: 102.0102 - val_loss: 84.6324\n",
      "Epoch 761/1000\n",
      "948/948 [==============================] - 0s - loss: 95.5024 - val_loss: 86.2919\n",
      "Epoch 762/1000\n",
      "948/948 [==============================] - 0s - loss: 99.4356 - val_loss: 85.8102\n",
      "Epoch 763/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4273 - val_loss: 84.3754\n",
      "Epoch 764/1000\n",
      "948/948 [==============================] - 0s - loss: 101.1657 - val_loss: 84.4174\n",
      "Epoch 765/1000\n",
      "948/948 [==============================] - 0s - loss: 105.1364 - val_loss: 85.4729\n",
      "Epoch 766/1000\n",
      "948/948 [==============================] - 0s - loss: 104.7361 - val_loss: 83.9082\n",
      "Epoch 767/1000\n",
      "948/948 [==============================] - 0s - loss: 104.5423 - val_loss: 82.9450\n",
      "Epoch 768/1000\n",
      "948/948 [==============================] - 0s - loss: 101.1354 - val_loss: 84.0520\n",
      "Epoch 769/1000\n",
      "948/948 [==============================] - 0s - loss: 97.6573 - val_loss: 84.1586\n",
      "Epoch 770/1000\n",
      "948/948 [==============================] - 0s - loss: 100.0484 - val_loss: 84.5493\n",
      "Epoch 771/1000\n",
      "948/948 [==============================] - 0s - loss: 100.5857 - val_loss: 84.3356\n",
      "Epoch 772/1000\n",
      "948/948 [==============================] - 0s - loss: 103.7338 - val_loss: 84.9812\n",
      "Epoch 773/1000\n",
      "948/948 [==============================] - 0s - loss: 103.7979 - val_loss: 94.8965\n",
      "Epoch 774/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4530 - val_loss: 85.3119\n",
      "Epoch 775/1000\n",
      "948/948 [==============================] - 0s - loss: 98.5996 - val_loss: 83.5195\n",
      "Epoch 776/1000\n",
      "948/948 [==============================] - 0s - loss: 102.6259 - val_loss: 84.8397\n",
      "Epoch 777/1000\n",
      "948/948 [==============================] - 0s - loss: 99.3455 - val_loss: 85.0784\n",
      "Epoch 778/1000\n",
      "948/948 [==============================] - 0s - loss: 97.5867 - val_loss: 87.9237\n",
      "Epoch 779/1000\n",
      "948/948 [==============================] - 0s - loss: 100.1162 - val_loss: 89.5873\n",
      "Epoch 780/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2789 - val_loss: 84.5450\n",
      "Epoch 781/1000\n",
      "948/948 [==============================] - 0s - loss: 102.9155 - val_loss: 84.1047\n",
      "Epoch 782/1000\n",
      "948/948 [==============================] - 0s - loss: 100.4443 - val_loss: 84.5556\n",
      "Epoch 783/1000\n",
      "948/948 [==============================] - 0s - loss: 98.6963 - val_loss: 84.1693\n",
      "Epoch 784/1000\n",
      "948/948 [==============================] - 0s - loss: 98.9777 - val_loss: 86.0563\n",
      "Epoch 785/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4759 - val_loss: 86.2736\n",
      "Epoch 786/1000\n",
      "948/948 [==============================] - 0s - loss: 94.9827 - val_loss: 86.0090\n",
      "Epoch 787/1000\n",
      "948/948 [==============================] - 0s - loss: 97.1676 - val_loss: 82.9606\n",
      "Epoch 788/1000\n",
      "948/948 [==============================] - 0s - loss: 105.6711 - val_loss: 85.1729\n",
      "Epoch 789/1000\n",
      "948/948 [==============================] - 0s - loss: 97.3888 - val_loss: 83.8186\n",
      "Epoch 790/1000\n",
      "948/948 [==============================] - 0s - loss: 100.3328 - val_loss: 85.1404\n",
      "Epoch 791/1000\n",
      "948/948 [==============================] - 0s - loss: 97.3250 - val_loss: 85.3833\n",
      "Epoch 792/1000\n",
      "948/948 [==============================] - 0s - loss: 104.0622 - val_loss: 85.2883\n",
      "Epoch 793/1000\n",
      "948/948 [==============================] - 0s - loss: 100.8704 - val_loss: 83.7907\n",
      "Epoch 794/1000\n",
      "948/948 [==============================] - 0s - loss: 98.1222 - val_loss: 85.7579\n",
      "Epoch 795/1000\n",
      "948/948 [==============================] - 0s - loss: 96.3037 - val_loss: 86.3104\n",
      "Epoch 796/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2440 - val_loss: 84.2656\n",
      "Epoch 797/1000\n",
      "948/948 [==============================] - 0s - loss: 99.6858 - val_loss: 83.9982\n",
      "Epoch 798/1000\n",
      "948/948 [==============================] - 0s - loss: 100.8514 - val_loss: 85.6322\n",
      "Epoch 799/1000\n",
      "948/948 [==============================] - 0s - loss: 96.9386 - val_loss: 83.1811\n",
      "Epoch 800/1000\n",
      "948/948 [==============================] - 0s - loss: 100.5566 - val_loss: 82.6576\n",
      "Epoch 801/1000\n",
      "948/948 [==============================] - 0s - loss: 97.8027 - val_loss: 84.5027\n",
      "Epoch 802/1000\n",
      "948/948 [==============================] - 0s - loss: 98.8047 - val_loss: 83.6528\n",
      "Epoch 803/1000\n",
      "948/948 [==============================] - 0s - loss: 103.9969 - val_loss: 84.6501\n",
      "Epoch 804/1000\n",
      "948/948 [==============================] - 0s - loss: 100.3568 - val_loss: 85.3554\n",
      "Epoch 805/1000\n",
      "948/948 [==============================] - 0s - loss: 101.9107 - val_loss: 84.6738\n",
      "Epoch 806/1000\n",
      "948/948 [==============================] - 0s - loss: 98.6790 - val_loss: 84.1239\n",
      "Epoch 807/1000\n",
      "948/948 [==============================] - 0s - loss: 100.5974 - val_loss: 84.4181\n",
      "Epoch 808/1000\n",
      "948/948 [==============================] - 0s - loss: 101.7706 - val_loss: 84.5320\n",
      "Epoch 809/1000\n",
      "948/948 [==============================] - 0s - loss: 104.4918 - val_loss: 83.2787\n",
      "Epoch 810/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0324 - val_loss: 85.4319\n",
      "Epoch 811/1000\n",
      "948/948 [==============================] - 0s - loss: 100.3300 - val_loss: 84.5587\n",
      "Epoch 812/1000\n",
      "948/948 [==============================] - 0s - loss: 99.5610 - val_loss: 83.8213\n",
      "Epoch 813/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7633 - val_loss: 85.7494\n",
      "Epoch 814/1000\n",
      "948/948 [==============================] - 0s - loss: 98.2049 - val_loss: 83.4609\n",
      "Epoch 815/1000\n",
      "948/948 [==============================] - 0s - loss: 91.7788 - val_loss: 82.7793\n",
      "Epoch 816/1000\n",
      "948/948 [==============================] - 0s - loss: 100.4537 - val_loss: 85.0973\n",
      "Epoch 817/1000\n",
      "948/948 [==============================] - 0s - loss: 94.9448 - val_loss: 85.9152\n",
      "Epoch 818/1000\n",
      "948/948 [==============================] - 0s - loss: 99.3030 - val_loss: 83.2980\n",
      "Epoch 819/1000\n",
      "948/948 [==============================] - 0s - loss: 101.7842 - val_loss: 82.4472\n",
      "Epoch 820/1000\n",
      "948/948 [==============================] - 0s - loss: 99.3452 - val_loss: 85.9360\n",
      "Epoch 821/1000\n",
      "948/948 [==============================] - 0s - loss: 99.9202 - val_loss: 83.7922\n",
      "Epoch 822/1000\n",
      "948/948 [==============================] - 0s - loss: 102.3676 - val_loss: 82.8314\n",
      "Epoch 823/1000\n",
      "948/948 [==============================] - 0s - loss: 97.3841 - val_loss: 82.9681\n",
      "Epoch 824/1000\n",
      "948/948 [==============================] - 0s - loss: 97.7932 - val_loss: 84.2233\n",
      "Epoch 825/1000\n",
      "948/948 [==============================] - 0s - loss: 97.2285 - val_loss: 82.6454\n",
      "Epoch 826/1000\n",
      "948/948 [==============================] - 0s - loss: 100.9477 - val_loss: 84.5654\n",
      "Epoch 827/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8278 - val_loss: 82.8765\n",
      "Epoch 828/1000\n",
      "948/948 [==============================] - 0s - loss: 102.9574 - val_loss: 84.1355\n",
      "Epoch 829/1000\n",
      "948/948 [==============================] - 0s - loss: 102.3583 - val_loss: 82.6182\n",
      "Epoch 830/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 99.7119 - val_loss: 81.8680\n",
      "Epoch 831/1000\n",
      "948/948 [==============================] - 0s - loss: 96.8337 - val_loss: 85.8294\n",
      "Epoch 832/1000\n",
      "948/948 [==============================] - 0s - loss: 101.6763 - val_loss: 85.5092\n",
      "Epoch 833/1000\n",
      "948/948 [==============================] - 0s - loss: 98.9434 - val_loss: 82.2094\n",
      "Epoch 834/1000\n",
      "948/948 [==============================] - 0s - loss: 95.3687 - val_loss: 83.1167\n",
      "Epoch 835/1000\n",
      "948/948 [==============================] - 0s - loss: 97.4267 - val_loss: 81.2648\n",
      "Epoch 836/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7664 - val_loss: 82.0349\n",
      "Epoch 837/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8222 - val_loss: 82.0622\n",
      "Epoch 838/1000\n",
      "948/948 [==============================] - 0s - loss: 95.1632 - val_loss: 83.4034\n",
      "Epoch 839/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8973 - val_loss: 82.5041\n",
      "Epoch 840/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7926 - val_loss: 82.4938\n",
      "Epoch 841/1000\n",
      "948/948 [==============================] - 0s - loss: 94.3201 - val_loss: 85.6066\n",
      "Epoch 842/1000\n",
      "948/948 [==============================] - 0s - loss: 100.0029 - val_loss: 84.7439\n",
      "Epoch 843/1000\n",
      "948/948 [==============================] - 0s - loss: 100.3197 - val_loss: 82.2057\n",
      "Epoch 844/1000\n",
      "948/948 [==============================] - 0s - loss: 96.7432 - val_loss: 83.9290\n",
      "Epoch 845/1000\n",
      "948/948 [==============================] - 0s - loss: 97.7241 - val_loss: 83.7157\n",
      "Epoch 846/1000\n",
      "948/948 [==============================] - 0s - loss: 100.6569 - val_loss: 82.6805\n",
      "Epoch 847/1000\n",
      "948/948 [==============================] - 0s - loss: 95.2647 - val_loss: 84.2063\n",
      "Epoch 848/1000\n",
      "948/948 [==============================] - 0s - loss: 104.3636 - val_loss: 83.2210\n",
      "Epoch 849/1000\n",
      "948/948 [==============================] - 0s - loss: 99.9848 - val_loss: 82.9361\n",
      "Epoch 850/1000\n",
      "948/948 [==============================] - 0s - loss: 98.3124 - val_loss: 85.3522\n",
      "Epoch 851/1000\n",
      "948/948 [==============================] - 0s - loss: 100.8084 - val_loss: 81.8854\n",
      "Epoch 852/1000\n",
      "948/948 [==============================] - 0s - loss: 99.6364 - val_loss: 82.5002\n",
      "Epoch 853/1000\n",
      "948/948 [==============================] - 0s - loss: 100.6116 - val_loss: 83.3945\n",
      "Epoch 854/1000\n",
      "948/948 [==============================] - 0s - loss: 99.9391 - val_loss: 83.4736\n",
      "Epoch 855/1000\n",
      "948/948 [==============================] - 0s - loss: 99.1412 - val_loss: 81.8753\n",
      "Epoch 856/1000\n",
      "948/948 [==============================] - 0s - loss: 93.6682 - val_loss: 81.2293\n",
      "Epoch 857/1000\n",
      "948/948 [==============================] - 0s - loss: 100.7822 - val_loss: 81.4623\n",
      "Epoch 858/1000\n",
      "948/948 [==============================] - 0s - loss: 101.8562 - val_loss: 82.7257\n",
      "Epoch 859/1000\n",
      "948/948 [==============================] - 0s - loss: 102.1748 - val_loss: 85.3410\n",
      "Epoch 860/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4373 - val_loss: 84.4152\n",
      "Epoch 861/1000\n",
      "948/948 [==============================] - 0s - loss: 96.8771 - val_loss: 85.7062\n",
      "Epoch 862/1000\n",
      "948/948 [==============================] - 0s - loss: 102.1027 - val_loss: 81.6949\n",
      "Epoch 863/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0737 - val_loss: 82.2776\n",
      "Epoch 864/1000\n",
      "948/948 [==============================] - 0s - loss: 96.3035 - val_loss: 82.8073\n",
      "Epoch 865/1000\n",
      "948/948 [==============================] - 0s - loss: 101.6659 - val_loss: 81.7496\n",
      "Epoch 866/1000\n",
      "948/948 [==============================] - 0s - loss: 91.6377 - val_loss: 83.1958\n",
      "Epoch 867/1000\n",
      "948/948 [==============================] - 0s - loss: 94.9813 - val_loss: 83.7123\n",
      "Epoch 868/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0659 - val_loss: 81.2139\n",
      "Epoch 869/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8199 - val_loss: 82.8067\n",
      "Epoch 870/1000\n",
      "948/948 [==============================] - 0s - loss: 99.9557 - val_loss: 82.5550\n",
      "Epoch 871/1000\n",
      "948/948 [==============================] - 0s - loss: 99.1784 - val_loss: 81.5379\n",
      "Epoch 872/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7119 - val_loss: 82.5798\n",
      "Epoch 873/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0371 - val_loss: 82.0669\n",
      "Epoch 874/1000\n",
      "948/948 [==============================] - 0s - loss: 95.9213 - val_loss: 83.9757\n",
      "Epoch 875/1000\n",
      "948/948 [==============================] - 0s - loss: 96.2860 - val_loss: 83.0727\n",
      "Epoch 876/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0387 - val_loss: 92.7022\n",
      "Epoch 877/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0949 - val_loss: 89.5932\n",
      "Epoch 878/1000\n",
      "948/948 [==============================] - 0s - loss: 101.3995 - val_loss: 81.7545\n",
      "Epoch 879/1000\n",
      "948/948 [==============================] - 0s - loss: 94.7577 - val_loss: 81.9616\n",
      "Epoch 880/1000\n",
      "948/948 [==============================] - 0s - loss: 92.9482 - val_loss: 82.6993\n",
      "Epoch 881/1000\n",
      "948/948 [==============================] - 0s - loss: 98.0559 - val_loss: 83.0084\n",
      "Epoch 882/1000\n",
      "948/948 [==============================] - 0s - loss: 93.8587 - val_loss: 84.0818\n",
      "Epoch 883/1000\n",
      "948/948 [==============================] - 0s - loss: 102.2030 - val_loss: 82.2739\n",
      "Epoch 884/1000\n",
      "948/948 [==============================] - 0s - loss: 103.5233 - val_loss: 85.3854\n",
      "Epoch 885/1000\n",
      "948/948 [==============================] - 0s - loss: 93.9397 - val_loss: 81.8880\n",
      "Epoch 886/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0165 - val_loss: 81.9652\n",
      "Epoch 887/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0556 - val_loss: 80.8209\n",
      "Epoch 888/1000\n",
      "948/948 [==============================] - 0s - loss: 99.9102 - val_loss: 80.7162\n",
      "Epoch 889/1000\n",
      "948/948 [==============================] - 0s - loss: 98.0022 - val_loss: 82.8667\n",
      "Epoch 890/1000\n",
      "948/948 [==============================] - 0s - loss: 97.3365 - val_loss: 86.7209\n",
      "Epoch 891/1000\n",
      "948/948 [==============================] - 0s - loss: 96.5385 - val_loss: 82.6158\n",
      "Epoch 892/1000\n",
      "948/948 [==============================] - 0s - loss: 91.1335 - val_loss: 82.2655\n",
      "Epoch 893/1000\n",
      "948/948 [==============================] - 0s - loss: 101.5955 - val_loss: 81.3804\n",
      "Epoch 894/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2216 - val_loss: 82.5210\n",
      "Epoch 895/1000\n",
      "948/948 [==============================] - 0s - loss: 90.4631 - val_loss: 83.9493\n",
      "Epoch 896/1000\n",
      "948/948 [==============================] - 0s - loss: 92.6583 - val_loss: 81.6794\n",
      "Epoch 897/1000\n",
      "948/948 [==============================] - 0s - loss: 97.3793 - val_loss: 81.7579\n",
      "Epoch 898/1000\n",
      "948/948 [==============================] - 0s - loss: 100.0575 - val_loss: 81.9061\n",
      "Epoch 899/1000\n",
      "948/948 [==============================] - 0s - loss: 96.3978 - val_loss: 81.8453\n",
      "Epoch 900/1000\n",
      "948/948 [==============================] - 0s - loss: 102.4748 - val_loss: 83.2419\n",
      "Epoch 901/1000\n",
      "948/948 [==============================] - 0s - loss: 94.6826 - val_loss: 81.8062\n",
      "Epoch 902/1000\n",
      "948/948 [==============================] - 0s - loss: 94.0955 - val_loss: 81.7919\n",
      "Epoch 903/1000\n",
      "948/948 [==============================] - 0s - loss: 93.8976 - val_loss: 80.8980\n",
      "Epoch 904/1000\n",
      "948/948 [==============================] - 0s - loss: 97.6432 - val_loss: 81.7603\n",
      "Epoch 905/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4729 - val_loss: 82.2691\n",
      "Epoch 906/1000\n",
      "948/948 [==============================] - 0s - loss: 97.2594 - val_loss: 81.1368\n",
      "Epoch 907/1000\n",
      "948/948 [==============================] - 0s - loss: 98.3141 - val_loss: 80.8459\n",
      "Epoch 908/1000\n",
      "948/948 [==============================] - 0s - loss: 100.5719 - val_loss: 81.2313\n",
      "Epoch 909/1000\n",
      "948/948 [==============================] - 0s - loss: 102.2838 - val_loss: 80.8210\n",
      "Epoch 910/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8082 - val_loss: 81.9305\n",
      "Epoch 911/1000\n",
      "948/948 [==============================] - 0s - loss: 96.2441 - val_loss: 81.6531\n",
      "Epoch 912/1000\n",
      "948/948 [==============================] - 0s - loss: 95.3466 - val_loss: 81.1172\n",
      "Epoch 913/1000\n",
      "948/948 [==============================] - 0s - loss: 95.4877 - val_loss: 81.5227\n",
      "Epoch 914/1000\n",
      "948/948 [==============================] - 0s - loss: 99.5220 - val_loss: 83.8896\n",
      "Epoch 915/1000\n",
      "948/948 [==============================] - 1s - loss: 99.7167 - val_loss: 81.4875\n",
      "Epoch 916/1000\n",
      "948/948 [==============================] - 0s - loss: 100.4336 - val_loss: 80.6405\n",
      "Epoch 917/1000\n",
      "948/948 [==============================] - 0s - loss: 96.6569 - val_loss: 83.9877\n",
      "Epoch 918/1000\n",
      "948/948 [==============================] - 0s - loss: 97.9531 - val_loss: 82.3265\n",
      "Epoch 919/1000\n",
      "948/948 [==============================] - 0s - loss: 97.3392 - val_loss: 82.4789\n",
      "Epoch 920/1000\n",
      "948/948 [==============================] - 0s - loss: 96.4645 - val_loss: 83.7245\n",
      "Epoch 921/1000\n",
      "948/948 [==============================] - 0s - loss: 100.1445 - val_loss: 82.4201\n",
      "Epoch 922/1000\n",
      "948/948 [==============================] - 0s - loss: 101.0871 - val_loss: 80.9988\n",
      "Epoch 923/1000\n",
      "948/948 [==============================] - 0s - loss: 100.1595 - val_loss: 83.0764\n",
      "Epoch 924/1000\n",
      "948/948 [==============================] - 0s - loss: 99.3483 - val_loss: 81.8928\n",
      "Epoch 925/1000\n",
      "948/948 [==============================] - 0s - loss: 97.2047 - val_loss: 83.9998\n",
      "Epoch 926/1000\n",
      "948/948 [==============================] - 0s - loss: 98.1108 - val_loss: 82.2658\n",
      "Epoch 927/1000\n",
      "948/948 [==============================] - 0s - loss: 99.7653 - val_loss: 81.1713\n",
      "Epoch 928/1000\n",
      "948/948 [==============================] - 0s - loss: 95.4094 - val_loss: 81.6644\n",
      "Epoch 929/1000\n",
      "948/948 [==============================] - 0s - loss: 94.5190 - val_loss: 81.2082\n",
      "Epoch 930/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4404 - val_loss: 81.5230\n",
      "Epoch 931/1000\n",
      "948/948 [==============================] - 0s - loss: 97.4452 - val_loss: 82.4170\n",
      "Epoch 932/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0165 - val_loss: 81.8946\n",
      "Epoch 933/1000\n",
      "948/948 [==============================] - 0s - loss: 94.8113 - val_loss: 80.8232\n",
      "Epoch 934/1000\n",
      "948/948 [==============================] - 0s - loss: 92.1712 - val_loss: 80.2118\n",
      "Epoch 935/1000\n",
      "948/948 [==============================] - 0s - loss: 99.1494 - val_loss: 80.6795\n",
      "Epoch 936/1000\n",
      "948/948 [==============================] - 0s - loss: 98.9258 - val_loss: 81.0021\n",
      "Epoch 937/1000\n",
      "948/948 [==============================] - 0s - loss: 97.0157 - val_loss: 85.9277\n",
      "Epoch 938/1000\n",
      "948/948 [==============================] - 0s - loss: 97.0511 - val_loss: 80.7526\n",
      "Epoch 939/1000\n",
      "948/948 [==============================] - 0s - loss: 97.7253 - val_loss: 80.3075\n",
      "Epoch 940/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0945 - val_loss: 81.0133\n",
      "Epoch 941/1000\n",
      "948/948 [==============================] - 0s - loss: 96.4179 - val_loss: 81.4458\n",
      "Epoch 942/1000\n",
      "948/948 [==============================] - 0s - loss: 95.6896 - val_loss: 80.3025\n",
      "Epoch 943/1000\n",
      "948/948 [==============================] - 0s - loss: 95.2107 - val_loss: 81.1152\n",
      "Epoch 944/1000\n",
      "948/948 [==============================] - 0s - loss: 96.7878 - val_loss: 86.3041\n",
      "Epoch 945/1000\n",
      "948/948 [==============================] - 0s - loss: 96.9824 - val_loss: 83.6859\n",
      "Epoch 946/1000\n",
      "948/948 [==============================] - 0s - loss: 94.5824 - val_loss: 81.6041\n",
      "Epoch 947/1000\n",
      "948/948 [==============================] - 0s - loss: 96.1977 - val_loss: 80.6573\n",
      "Epoch 948/1000\n",
      "948/948 [==============================] - 0s - loss: 94.3088 - val_loss: 81.1352\n",
      "Epoch 949/1000\n",
      "948/948 [==============================] - 0s - loss: 93.4781 - val_loss: 82.6347\n",
      "Epoch 950/1000\n",
      "948/948 [==============================] - 0s - loss: 95.3006 - val_loss: 79.7336\n",
      "Epoch 951/1000\n",
      "948/948 [==============================] - 0s - loss: 93.3644 - val_loss: 81.0276\n",
      "Epoch 952/1000\n",
      "948/948 [==============================] - 0s - loss: 91.1239 - val_loss: 80.5089\n",
      "Epoch 953/1000\n",
      "948/948 [==============================] - 0s - loss: 99.9318 - val_loss: 79.9563\n",
      "Epoch 954/1000\n",
      "948/948 [==============================] - 0s - loss: 98.6203 - val_loss: 79.7299\n",
      "Epoch 955/1000\n",
      "948/948 [==============================] - 0s - loss: 101.9511 - val_loss: 79.2502\n",
      "Epoch 956/1000\n",
      "948/948 [==============================] - 0s - loss: 97.9814 - val_loss: 79.7966\n",
      "Epoch 957/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0557 - val_loss: 79.9483\n",
      "Epoch 958/1000\n",
      "948/948 [==============================] - 0s - loss: 100.4157 - val_loss: 80.9546\n",
      "Epoch 959/1000\n",
      "948/948 [==============================] - 0s - loss: 96.4238 - val_loss: 79.3518\n",
      "Epoch 960/1000\n",
      "948/948 [==============================] - 0s - loss: 101.5838 - val_loss: 81.8491\n",
      "Epoch 961/1000\n",
      "948/948 [==============================] - 0s - loss: 94.8380 - val_loss: 80.0974\n",
      "Epoch 962/1000\n",
      "948/948 [==============================] - 0s - loss: 101.2963 - val_loss: 80.8771\n",
      "Epoch 963/1000\n",
      "948/948 [==============================] - 0s - loss: 93.7398 - val_loss: 81.0285\n",
      "Epoch 964/1000\n",
      "948/948 [==============================] - 0s - loss: 96.0661 - val_loss: 79.8861\n",
      "Epoch 965/1000\n",
      "948/948 [==============================] - 0s - loss: 98.8043 - val_loss: 80.2624\n",
      "Epoch 966/1000\n",
      "948/948 [==============================] - 0s - loss: 98.3278 - val_loss: 82.1938\n",
      "Epoch 967/1000\n",
      "948/948 [==============================] - 0s - loss: 94.6452 - val_loss: 80.0424\n",
      "Epoch 968/1000\n",
      "948/948 [==============================] - 0s - loss: 94.4307 - val_loss: 79.8080\n",
      "Epoch 969/1000\n",
      "948/948 [==============================] - 0s - loss: 91.1972 - val_loss: 80.7667\n",
      "Epoch 970/1000\n",
      "948/948 [==============================] - 0s - loss: 97.4528 - val_loss: 82.5262\n",
      "Epoch 971/1000\n",
      "948/948 [==============================] - 0s - loss: 97.1647 - val_loss: 81.7201\n",
      "Epoch 972/1000\n",
      "948/948 [==============================] - 0s - loss: 96.1354 - val_loss: 81.0996\n",
      "Epoch 973/1000\n",
      "948/948 [==============================] - 0s - loss: 98.1149 - val_loss: 80.5986\n",
      "Epoch 974/1000\n",
      "948/948 [==============================] - 0s - loss: 93.1875 - val_loss: 81.9227\n",
      "Epoch 975/1000\n",
      "948/948 [==============================] - 0s - loss: 95.0264 - val_loss: 80.6676\n",
      "Epoch 976/1000\n",
      "948/948 [==============================] - 0s - loss: 98.4820 - val_loss: 83.8457\n",
      "Epoch 977/1000\n",
      "948/948 [==============================] - 0s - loss: 99.8216 - val_loss: 81.1337\n",
      "Epoch 978/1000\n",
      "948/948 [==============================] - 0s - loss: 98.2004 - val_loss: 79.8540\n",
      "Epoch 979/1000\n",
      "948/948 [==============================] - 0s - loss: 97.1206 - val_loss: 79.8471\n",
      "Epoch 980/1000\n",
      "948/948 [==============================] - 0s - loss: 101.5277 - val_loss: 82.1891\n",
      "Epoch 981/1000\n",
      "948/948 [==============================] - 0s - loss: 90.9288 - val_loss: 80.0064\n",
      "Epoch 982/1000\n",
      "948/948 [==============================] - 0s - loss: 95.1590 - val_loss: 79.7046\n",
      "Epoch 983/1000\n",
      "948/948 [==============================] - 0s - loss: 95.7283 - val_loss: 81.1856\n",
      "Epoch 984/1000\n",
      "948/948 [==============================] - 0s - loss: 93.0352 - val_loss: 81.6122\n",
      "Epoch 985/1000\n",
      "948/948 [==============================] - 0s - loss: 95.0615 - val_loss: 79.6751\n",
      "Epoch 986/1000\n",
      "948/948 [==============================] - 0s - loss: 91.7255 - val_loss: 79.0630\n",
      "Epoch 987/1000\n",
      "948/948 [==============================] - 0s - loss: 92.4923 - val_loss: 80.0949\n",
      "Epoch 988/1000\n",
      "948/948 [==============================] - 0s - loss: 94.4403 - val_loss: 79.7172\n",
      "Epoch 989/1000\n",
      "948/948 [==============================] - 0s - loss: 101.5198 - val_loss: 88.0637\n",
      "Epoch 990/1000\n",
      "948/948 [==============================] - 0s - loss: 97.1312 - val_loss: 79.2219\n",
      "Epoch 991/1000\n",
      "948/948 [==============================] - 0s - loss: 94.9674 - val_loss: 80.1482\n",
      "Epoch 992/1000\n",
      "948/948 [==============================] - 0s - loss: 93.9146 - val_loss: 79.1759\n",
      "Epoch 993/1000\n",
      "948/948 [==============================] - 0s - loss: 97.1729 - val_loss: 78.8787\n",
      "Epoch 994/1000\n",
      "948/948 [==============================] - 0s - loss: 95.4893 - val_loss: 78.4591\n",
      "Epoch 995/1000\n",
      "948/948 [==============================] - 0s - loss: 96.3383 - val_loss: 81.8100\n",
      "Epoch 996/1000\n",
      "948/948 [==============================] - 0s - loss: 95.8874 - val_loss: 80.5295\n",
      "Epoch 997/1000\n",
      "948/948 [==============================] - 0s - loss: 94.6790 - val_loss: 79.2965\n",
      "Epoch 998/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 96.1500 - val_loss: 79.1806\n",
      "Epoch 999/1000\n",
      "948/948 [==============================] - 0s - loss: 95.4687 - val_loss: 79.9765\n",
      "Epoch 1000/1000\n",
      "948/948 [==============================] - 0s - loss: 96.6611 - val_loss: 80.4739\n",
      "948/948 [==============================] - 0s\n",
      "316/316 [==============================] - 0s\n",
      "226/226 [==============================] - 0s\n",
      "Loss for training data is 55.6273994446\n",
      "Loss for validation data is 80.4739151001\n",
      "Loss for test data is 109.512710571\n"
     ]
    }
   ],
   "source": [
    "predict1, error, accuracy1 = train_model(train1_r, train1_c, test1_r, test1_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error: 9.71187968549\n"
     ]
    }
   ],
   "source": [
    "print('The average error:', accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum error: 0.512018594564 maximum error: 95.0999141293 variance: 65.8952158261\n"
     ]
    }
   ],
   "source": [
    "print('minimum error:', np.amin(error), 'maximum error:', np.amax(error), 'variance:', np.var(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.6555265487118422"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x156ad96cb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_sorted= np.sort(error)\n",
    "p = 1. *np.arange(len(error))/(len(error)-1)\n",
    "plt.plot(error_sorted, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Input\n",
    "def cnn_model(rss, locations, test_rss, test_locations):\n",
    "    # get train_X, val_X, train_Y, val_Y\n",
    "    train_X, val_X, train_Y, val_Y = train_val(rss, locations)\n",
    "    test_X, test_Y = predata(test_rss, test_locations)\n",
    "    \n",
    "    # parameters\n",
    "    num_input = train_X.shape[1]# input layer size\n",
    "    act_fun = 'relu'\n",
    "    regularzation_penalty = 0.03\n",
    "    initilization_method = 'he_normal' #'random_uniform' ,'random_normal','TruncatedNormal' ,'glorot_uniform', 'glorot_nomral', 'he_normal', 'he_uniform'\n",
    "    #Optimizer\n",
    "    adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "    \n",
    "    train_X.reshape((1,train_X.shape[0],train_X.shape[1]))\n",
    "    test_X.reshape((1,test_X.shape[0],test_X.shape[1]))\n",
    "    val_X.reshape((1,val_X.shape[0],val_X.shape[1]))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=16, kernel_size=3,activation='relu',input_shape = (train_X.shape[1],num_input)))\n",
    "    model.add(Conv1D(filters=32, kernel_size=2,activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=adam, metrics=['accuracy'])          \n",
    "    model.fit(train_X, train_Y,\n",
    "              epochs=1000,\n",
    "              batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n",
    "    train_loss = model.evaluate(train_X,train_Y, batch_size=len(train_Y)) #calculate the data in test mode(Keras)\n",
    "    val_loss = model.evaluate(val_X, val_Y, batch_size=len(val_Y))\n",
    "    test_loss = model.evaluate(test_X, test_Y, batch_size=len(test_Y))\n",
    "    print(model.summary())\n",
    "    print(\"Loss for training data is\",train_loss)\n",
    "    print(\"Loss for validation data is\",val_loss)\n",
    "    print(\"Loss for test data is\",test_loss)\n",
    "    predict_Y = model.predict(test_X)\n",
    "    error_t, accuracy_t = accuracy(predict_Y, test_Y)\n",
    "    return predict_Y, error_t, accuracy_t           \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_9_input to have 3 dimensions, but got array with shape (948, 992)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0bb9a751f9ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict1_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy1_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain1_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain1_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest1_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest1_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-4f7b097587c2>\u001b[0m in \u001b[0;36mcnn_model\u001b[1;34m(rss, locations, test_rss, test_locations)\u001b[0m\n\u001b[0;32m     30\u001b[0m     model.fit(train_X, train_Y,\n\u001b[0;32m     31\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m               batch_size=64,callbacks=[earlyStopping],validation_data=(val_X, val_Y))#tbCallBack,\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculate the data in test mode(Keras)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1359\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1232\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1234\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1235\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv1d_9_input to have 3 dimensions, but got array with shape (948, 992)"
     ]
    }
   ],
   "source": [
    "predict1_cnn, error_cnn, accuracy1_cnn = cnn_model(train1_r, train1_c, test1_r, test1_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The average error:', accuracy1_cnn)\n",
    "print('minimum error:', np.amin(error_cnn), 'maximum error:', np.amax(error_cnn), 'variance:', np.var(error_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_sortedcnn= np.sort(error_cnn)\n",
    "p1 = 1. *np.arange(len(error_cnn))/(len(error_cnn)-1)\n",
    "plt.plot(error_sortedcnn, p1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
