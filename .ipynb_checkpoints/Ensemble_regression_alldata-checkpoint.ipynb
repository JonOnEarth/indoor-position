{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import auto_regression as ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#test\n",
    "#load data\n",
    "path = 'C:/Users/mys12/Desktop/northeastern/summer2018/data_Robust_Fingerprinting-master/DISTRIBUTED_OPENSOURCE/FINGERPRINTING_DB'\n",
    "\n",
    "test_rss = pd.read_csv(path + '/Training_rss_21Aug17.csv', header = 0)\n",
    "test_coord = pd.read_csv(path + '/Training_coordinates_21Aug17.csv', header = 0)\n",
    "test_rss = test_rss.replace(100, 0)\n",
    "\n",
    "train_rss = pd.read_csv(path + '/Test_rss_21Aug17.csv', header = 0)\n",
    "train_coord = pd.read_csv(path + '/Test_coordinates_21Aug17.csv', header = 0)\n",
    "train_rss = train_rss.replace(100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train_rss, train_coord], axis=1, ignore_index=True)\n",
    "test = pd.concat([test_rss, test_coord], axis=1, ignore_index=True)\n",
    "train = np.asarray(train)\n",
    "test = np.asarray(test)\n",
    "\n",
    "# first floor\n",
    "train1 = train[train[:,-1]==0.0]\n",
    "normalizer = preprocessing.Normalizer().fit(train1[:,:-3])\n",
    "train1_r=normalizer.transform(train1[:,:-3])\n",
    "train1_c=train1[:,-3:-1]\n",
    "print(train1_r.shape[1])\n",
    "\n",
    "test1 = test[test[:,-1]==0.0]\n",
    "test1_r=normalizer.transform(test1[:,:-3])\n",
    "test1_c=test1[:,-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata(rss, locations):\n",
    "    # the origin of the room\n",
    "    origin = np.amin(locations,axis=0)\n",
    "    #size of the room\n",
    "    room_size = np.amax(locations, axis=0)-origin\n",
    "    # position respect to origin\n",
    "    train_Yy = locations - origin\n",
    "    train_Xx = np.asarray(rss, dtype=np.float64)\n",
    "    return train_Xx, train_Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(rss, locations):\n",
    "    train_Xx, train_Yy = predata(rss, locations)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_Xx, train_Yy, test_size=0.25)\n",
    "    return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    error = np.sqrt(np.sum((predictions - labels)**2, 1))\n",
    "    return error, np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=992, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='linear', bias=True))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=992, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu', bias=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='linear', bias=True))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = predata(train1_r, train1_c)\n",
    "train_X, val_X, train_Y, val_Y = train_val(train_x, train_y)\n",
    "test_x, test_y = predata(test1_r, test1_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs =500\n",
    "batch_size = 64\n",
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=60, verbose=0, mode='auto')\n",
    "Model_best= keras.callbacks.ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=992, activation=\"relu\", use_bias=True)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", use_bias=True)`\n",
      "  \"\"\"\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"linear\", use_bias=True)`\n",
      "  import sys\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 948 samples, validate on 316 samples\n",
      "Epoch 1/500\n",
      "948/948 [==============================] - 0s - loss: 4629.0360 - acc: 0.8882 - val_loss: 4445.2366 - val_acc: 0.9462\n",
      "Epoch 2/500\n",
      "948/948 [==============================] - 0s - loss: 4226.6194 - acc: 0.9430 - val_loss: 3724.1758 - val_acc: 0.9462\n",
      "Epoch 3/500\n",
      "948/948 [==============================] - 0s - loss: 3078.4992 - acc: 0.9430 - val_loss: 2173.8149 - val_acc: 0.9462\n",
      "Epoch 4/500\n",
      "948/948 [==============================] - 0s - loss: 1479.5861 - acc: 0.9430 - val_loss: 961.4111 - val_acc: 0.9462\n",
      "Epoch 5/500\n",
      "948/948 [==============================] - 0s - loss: 923.1158 - acc: 0.9430 - val_loss: 659.9563 - val_acc: 0.9462\n",
      "Epoch 6/500\n",
      "948/948 [==============================] - 0s - loss: 657.9818 - acc: 0.9430 - val_loss: 479.4258 - val_acc: 0.9462\n",
      "Epoch 7/500\n",
      "948/948 [==============================] - 0s - loss: 498.0227 - acc: 0.9430 - val_loss: 356.1720 - val_acc: 0.9462\n",
      "Epoch 8/500\n",
      "948/948 [==============================] - 0s - loss: 422.4542 - acc: 0.9430 - val_loss: 290.6936 - val_acc: 0.9462\n",
      "Epoch 9/500\n",
      "948/948 [==============================] - 0s - loss: 361.7037 - acc: 0.9430 - val_loss: 248.7179 - val_acc: 0.9462\n",
      "Epoch 10/500\n",
      "948/948 [==============================] - 0s - loss: 325.6908 - acc: 0.9430 - val_loss: 225.7407 - val_acc: 0.9462\n",
      "Epoch 11/500\n",
      "948/948 [==============================] - 0s - loss: 311.5472 - acc: 0.9430 - val_loss: 210.9347 - val_acc: 0.9462\n",
      "Epoch 12/500\n",
      "948/948 [==============================] - 0s - loss: 307.4218 - acc: 0.9430 - val_loss: 200.8817 - val_acc: 0.9462\n",
      "Epoch 13/500\n",
      "948/948 [==============================] - 0s - loss: 286.6861 - acc: 0.9430 - val_loss: 191.5052 - val_acc: 0.9462\n",
      "Epoch 14/500\n",
      "948/948 [==============================] - 0s - loss: 277.9010 - acc: 0.9430 - val_loss: 185.6490 - val_acc: 0.9462\n",
      "Epoch 15/500\n",
      "948/948 [==============================] - 0s - loss: 282.1017 - acc: 0.9430 - val_loss: 177.8307 - val_acc: 0.9462\n",
      "Epoch 16/500\n",
      "948/948 [==============================] - 0s - loss: 248.4099 - acc: 0.9430 - val_loss: 171.9958 - val_acc: 0.9462\n",
      "Epoch 17/500\n",
      "948/948 [==============================] - 0s - loss: 249.3587 - acc: 0.9430 - val_loss: 164.0188 - val_acc: 0.9462\n",
      "Epoch 18/500\n",
      "948/948 [==============================] - 0s - loss: 247.6664 - acc: 0.9430 - val_loss: 159.0605 - val_acc: 0.9462\n",
      "Epoch 19/500\n",
      "948/948 [==============================] - 0s - loss: 233.9044 - acc: 0.9430 - val_loss: 151.1220 - val_acc: 0.9462\n",
      "Epoch 20/500\n",
      "948/948 [==============================] - ETA: 0s - loss: 236.9783 - acc: 0.943 - 0s - loss: 228.6807 - acc: 0.9430 - val_loss: 142.9342 - val_acc: 0.9462\n",
      "Epoch 21/500\n",
      "948/948 [==============================] - 0s - loss: 212.8986 - acc: 0.9462 - val_loss: 132.2830 - val_acc: 0.9462\n",
      "Epoch 22/500\n",
      "948/948 [==============================] - 0s - loss: 207.6729 - acc: 0.9473 - val_loss: 122.6959 - val_acc: 0.9462\n",
      "Epoch 23/500\n",
      "948/948 [==============================] - 0s - loss: 200.2179 - acc: 0.9557 - val_loss: 114.3963 - val_acc: 0.9462\n",
      "Epoch 24/500\n",
      "948/948 [==============================] - 0s - loss: 198.4339 - acc: 0.9525 - val_loss: 105.0226 - val_acc: 0.9430\n",
      "Epoch 25/500\n",
      "948/948 [==============================] - 0s - loss: 189.4974 - acc: 0.9515 - val_loss: 100.4651 - val_acc: 0.9399\n",
      "Epoch 26/500\n",
      "948/948 [==============================] - 0s - loss: 195.1822 - acc: 0.9525 - val_loss: 94.2842 - val_acc: 0.9399\n",
      "Epoch 27/500\n",
      "948/948 [==============================] - 0s - loss: 180.0460 - acc: 0.9536 - val_loss: 91.8574 - val_acc: 0.9399\n",
      "Epoch 28/500\n",
      "948/948 [==============================] - 0s - loss: 187.9630 - acc: 0.9610 - val_loss: 90.7941 - val_acc: 0.9399\n",
      "Epoch 29/500\n",
      "948/948 [==============================] - 0s - loss: 171.0029 - acc: 0.9610 - val_loss: 88.9380 - val_acc: 0.9399\n",
      "Epoch 30/500\n",
      "948/948 [==============================] - 0s - loss: 166.8146 - acc: 0.9568 - val_loss: 90.0888 - val_acc: 0.9399\n",
      "Epoch 31/500\n",
      "948/948 [==============================] - 0s - loss: 180.1347 - acc: 0.9578 - val_loss: 87.4112 - val_acc: 0.9399\n",
      "Epoch 32/500\n",
      "948/948 [==============================] - 0s - loss: 175.5706 - acc: 0.9599 - val_loss: 84.0580 - val_acc: 0.9399\n",
      "Epoch 33/500\n",
      "948/948 [==============================] - 0s - loss: 159.5781 - acc: 0.9557 - val_loss: 84.5122 - val_acc: 0.9399\n",
      "Epoch 34/500\n",
      "948/948 [==============================] - 0s - loss: 174.7725 - acc: 0.9578 - val_loss: 80.5162 - val_acc: 0.9430\n",
      "Epoch 35/500\n",
      "948/948 [==============================] - 0s - loss: 165.7162 - acc: 0.9589 - val_loss: 81.5497 - val_acc: 0.9430\n",
      "Epoch 36/500\n",
      "948/948 [==============================] - 0s - loss: 174.1750 - acc: 0.9684 - val_loss: 80.5684 - val_acc: 0.9462\n",
      "Epoch 37/500\n",
      "948/948 [==============================] - 0s - loss: 166.3134 - acc: 0.9631 - val_loss: 81.2546 - val_acc: 0.9430\n",
      "Epoch 38/500\n",
      "948/948 [==============================] - 0s - loss: 158.8314 - acc: 0.9589 - val_loss: 79.8844 - val_acc: 0.9462\n",
      "Epoch 39/500\n",
      "948/948 [==============================] - 0s - loss: 160.7567 - acc: 0.9673 - val_loss: 83.2422 - val_acc: 0.9430\n",
      "Epoch 40/500\n",
      "948/948 [==============================] - 0s - loss: 152.5544 - acc: 0.9589 - val_loss: 78.2570 - val_acc: 0.9430\n",
      "Epoch 41/500\n",
      "948/948 [==============================] - 0s - loss: 141.4703 - acc: 0.9662 - val_loss: 80.5426 - val_acc: 0.9430\n",
      "Epoch 42/500\n",
      "948/948 [==============================] - 0s - loss: 160.3092 - acc: 0.9631 - val_loss: 76.5736 - val_acc: 0.9430\n",
      "Epoch 43/500\n",
      "948/948 [==============================] - 0s - loss: 162.8407 - acc: 0.9599 - val_loss: 74.8517 - val_acc: 0.9430\n",
      "Epoch 44/500\n",
      "948/948 [==============================] - 0s - loss: 147.6820 - acc: 0.9578 - val_loss: 75.2505 - val_acc: 0.9430\n",
      "Epoch 45/500\n",
      "948/948 [==============================] - 0s - loss: 153.1610 - acc: 0.9694 - val_loss: 75.9775 - val_acc: 0.9430\n",
      "Epoch 46/500\n",
      "948/948 [==============================] - 0s - loss: 148.8264 - acc: 0.9662 - val_loss: 76.9079 - val_acc: 0.9430\n",
      "Epoch 47/500\n",
      "948/948 [==============================] - 0s - loss: 137.3907 - acc: 0.9673 - val_loss: 74.0146 - val_acc: 0.9462\n",
      "Epoch 48/500\n",
      "948/948 [==============================] - 0s - loss: 138.6724 - acc: 0.9662 - val_loss: 75.3348 - val_acc: 0.9462\n",
      "Epoch 49/500\n",
      "948/948 [==============================] - 0s - loss: 137.8519 - acc: 0.9610 - val_loss: 73.6915 - val_acc: 0.9462\n",
      "Epoch 50/500\n",
      "948/948 [==============================] - 0s - loss: 150.6199 - acc: 0.9673 - val_loss: 71.8914 - val_acc: 0.9462\n",
      "Epoch 51/500\n",
      "948/948 [==============================] - 0s - loss: 138.1494 - acc: 0.9652 - val_loss: 71.5033 - val_acc: 0.9462\n",
      "Epoch 52/500\n",
      "948/948 [==============================] - 0s - loss: 136.1472 - acc: 0.9673 - val_loss: 71.6098 - val_acc: 0.9494\n",
      "Epoch 53/500\n",
      "948/948 [==============================] - 0s - loss: 139.9853 - acc: 0.9673 - val_loss: 70.8002 - val_acc: 0.9494\n",
      "Epoch 54/500\n",
      "948/948 [==============================] - 0s - loss: 131.8120 - acc: 0.9610 - val_loss: 74.6978 - val_acc: 0.9494\n",
      "Epoch 55/500\n",
      "948/948 [==============================] - 0s - loss: 142.9897 - acc: 0.9673 - val_loss: 69.9969 - val_acc: 0.9462\n",
      "Epoch 56/500\n",
      "948/948 [==============================] - 0s - loss: 131.6107 - acc: 0.9673 - val_loss: 70.5669 - val_acc: 0.9462\n",
      "Epoch 57/500\n",
      "948/948 [==============================] - 0s - loss: 129.8444 - acc: 0.9641 - val_loss: 68.3983 - val_acc: 0.9525\n",
      "Epoch 58/500\n",
      "948/948 [==============================] - 0s - loss: 132.0573 - acc: 0.9694 - val_loss: 66.6980 - val_acc: 0.9525\n",
      "Epoch 59/500\n",
      "948/948 [==============================] - 0s - loss: 134.9798 - acc: 0.9652 - val_loss: 68.0065 - val_acc: 0.9494\n",
      "Epoch 60/500\n",
      "948/948 [==============================] - 0s - loss: 137.5632 - acc: 0.9631 - val_loss: 66.9574 - val_acc: 0.9494\n",
      "Epoch 61/500\n",
      "948/948 [==============================] - 0s - loss: 136.6600 - acc: 0.9641 - val_loss: 65.8606 - val_acc: 0.9525\n",
      "Epoch 62/500\n",
      "948/948 [==============================] - 0s - loss: 131.1632 - acc: 0.9620 - val_loss: 68.0118 - val_acc: 0.9525\n",
      "Epoch 63/500\n",
      "948/948 [==============================] - 0s - loss: 135.0639 - acc: 0.9620 - val_loss: 64.8021 - val_acc: 0.9525\n",
      "Epoch 64/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 129.5045 - acc: 0.9715 - val_loss: 63.2661 - val_acc: 0.9525\n",
      "Epoch 65/500\n",
      "948/948 [==============================] - 0s - loss: 127.3811 - acc: 0.9662 - val_loss: 66.5015 - val_acc: 0.9557 0.965\n",
      "Epoch 66/500\n",
      "948/948 [==============================] - 0s - loss: 123.3053 - acc: 0.9631 - val_loss: 65.5712 - val_acc: 0.9589\n",
      "Epoch 67/500\n",
      "948/948 [==============================] - 0s - loss: 138.8782 - acc: 0.9747 - val_loss: 67.6528 - val_acc: 0.9589\n",
      "Epoch 68/500\n",
      "948/948 [==============================] - 0s - loss: 124.3628 - acc: 0.9694 - val_loss: 68.5531 - val_acc: 0.9589\n",
      "Epoch 69/500\n",
      "948/948 [==============================] - 0s - loss: 129.0448 - acc: 0.9641 - val_loss: 67.2483 - val_acc: 0.9557\n",
      "Epoch 70/500\n",
      "948/948 [==============================] - 0s - loss: 134.6971 - acc: 0.9673 - val_loss: 67.8405 - val_acc: 0.9557\n",
      "Epoch 71/500\n",
      "948/948 [==============================] - 0s - loss: 131.1207 - acc: 0.9662 - val_loss: 70.0503 - val_acc: 0.9557\n",
      "Epoch 72/500\n",
      "948/948 [==============================] - 0s - loss: 128.0545 - acc: 0.9694 - val_loss: 66.5963 - val_acc: 0.9557\n",
      "Epoch 73/500\n",
      "948/948 [==============================] - 0s - loss: 132.7433 - acc: 0.9652 - val_loss: 64.6403 - val_acc: 0.9589\n",
      "Epoch 74/500\n",
      "948/948 [==============================] - 0s - loss: 137.0331 - acc: 0.9641 - val_loss: 63.2494 - val_acc: 0.9589\n",
      "Epoch 75/500\n",
      "948/948 [==============================] - 0s - loss: 127.9031 - acc: 0.9662 - val_loss: 65.3115 - val_acc: 0.9525\n",
      "Epoch 76/500\n",
      "948/948 [==============================] - 0s - loss: 119.0615 - acc: 0.9652 - val_loss: 65.7729 - val_acc: 0.9589\n",
      "Epoch 77/500\n",
      "948/948 [==============================] - 0s - loss: 124.9086 - acc: 0.9673 - val_loss: 68.0120 - val_acc: 0.9589\n",
      "Epoch 78/500\n",
      "948/948 [==============================] - 0s - loss: 132.0296 - acc: 0.9673 - val_loss: 64.8463 - val_acc: 0.9589\n",
      "Epoch 79/500\n",
      "948/948 [==============================] - 0s - loss: 111.8684 - acc: 0.9662 - val_loss: 63.2779 - val_acc: 0.9589\n",
      "Epoch 80/500\n",
      "948/948 [==============================] - 0s - loss: 136.9279 - acc: 0.9631 - val_loss: 68.8983 - val_acc: 0.9557\n",
      "Epoch 81/500\n",
      "948/948 [==============================] - 0s - loss: 129.8666 - acc: 0.9736 - val_loss: 63.8722 - val_acc: 0.9557\n",
      "Epoch 82/500\n",
      "948/948 [==============================] - 0s - loss: 132.4463 - acc: 0.9726 - val_loss: 64.0890 - val_acc: 0.9589\n",
      "Epoch 83/500\n",
      "948/948 [==============================] - 0s - loss: 121.1474 - acc: 0.9673 - val_loss: 61.8238 - val_acc: 0.9589\n",
      "Epoch 84/500\n",
      "948/948 [==============================] - 0s - loss: 118.9342 - acc: 0.9747 - val_loss: 61.5857 - val_acc: 0.9557\n",
      "Epoch 85/500\n",
      "948/948 [==============================] - 0s - loss: 114.8407 - acc: 0.9705 - val_loss: 64.8176 - val_acc: 0.9557\n",
      "Epoch 86/500\n",
      "948/948 [==============================] - 0s - loss: 126.0001 - acc: 0.9694 - val_loss: 65.6332 - val_acc: 0.9557\n",
      "Epoch 87/500\n",
      "948/948 [==============================] - 0s - loss: 129.7929 - acc: 0.9673 - val_loss: 64.0894 - val_acc: 0.9557\n",
      "Epoch 88/500\n",
      "948/948 [==============================] - 0s - loss: 118.6350 - acc: 0.9673 - val_loss: 65.7156 - val_acc: 0.9557\n",
      "Epoch 89/500\n",
      "948/948 [==============================] - 0s - loss: 127.4291 - acc: 0.9631 - val_loss: 63.1927 - val_acc: 0.9557\n",
      "Epoch 90/500\n",
      "948/948 [==============================] - 0s - loss: 127.9066 - acc: 0.9694 - val_loss: 61.3884 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "948/948 [==============================] - 0s - loss: 117.1274 - acc: 0.9726 - val_loss: 59.8981 - val_acc: 0.9525\n",
      "Epoch 92/500\n",
      "948/948 [==============================] - 0s - loss: 123.3234 - acc: 0.9726 - val_loss: 61.7435 - val_acc: 0.9525\n",
      "Epoch 93/500\n",
      "948/948 [==============================] - 0s - loss: 120.4971 - acc: 0.9684 - val_loss: 61.7872 - val_acc: 0.9525\n",
      "Epoch 94/500\n",
      "948/948 [==============================] - 0s - loss: 116.2243 - acc: 0.9652 - val_loss: 65.7493 - val_acc: 0.9525\n",
      "Epoch 95/500\n",
      "948/948 [==============================] - 0s - loss: 121.9127 - acc: 0.9705 - val_loss: 62.1414 - val_acc: 0.9525\n",
      "Epoch 96/500\n",
      "948/948 [==============================] - 0s - loss: 126.8809 - acc: 0.9715 - val_loss: 59.0669 - val_acc: 0.9525\n",
      "Epoch 97/500\n",
      "948/948 [==============================] - 0s - loss: 119.3286 - acc: 0.9726 - val_loss: 62.7347 - val_acc: 0.9525\n",
      "Epoch 98/500\n",
      "948/948 [==============================] - 0s - loss: 125.8329 - acc: 0.9715 - val_loss: 60.5350 - val_acc: 0.9557\n",
      "Epoch 99/500\n",
      "948/948 [==============================] - 0s - loss: 115.1938 - acc: 0.9757 - val_loss: 63.4745 - val_acc: 0.9525\n",
      "Epoch 100/500\n",
      "948/948 [==============================] - 0s - loss: 116.3687 - acc: 0.9768 - val_loss: 61.9865 - val_acc: 0.9494\n",
      "Epoch 101/500\n",
      "948/948 [==============================] - 0s - loss: 121.7472 - acc: 0.9705 - val_loss: 59.6508 - val_acc: 0.9525\n",
      "Epoch 102/500\n",
      "948/948 [==============================] - 0s - loss: 119.3327 - acc: 0.9694 - val_loss: 60.9963 - val_acc: 0.9557\n",
      "Epoch 103/500\n",
      "948/948 [==============================] - 0s - loss: 115.5990 - acc: 0.9715 - val_loss: 62.1148 - val_acc: 0.9557\n",
      "Epoch 104/500\n",
      "948/948 [==============================] - 0s - loss: 118.5439 - acc: 0.9684 - val_loss: 61.9903 - val_acc: 0.9557\n",
      "Epoch 105/500\n",
      "948/948 [==============================] - 0s - loss: 117.1211 - acc: 0.9747 - val_loss: 61.5854 - val_acc: 0.9525\n",
      "Epoch 106/500\n",
      "948/948 [==============================] - 0s - loss: 118.2774 - acc: 0.9662 - val_loss: 60.8630 - val_acc: 0.9494\n",
      "Epoch 107/500\n",
      "948/948 [==============================] - 0s - loss: 115.2289 - acc: 0.9768 - val_loss: 59.0542 - val_acc: 0.9525\n",
      "Epoch 108/500\n",
      "948/948 [==============================] - 0s - loss: 106.5678 - acc: 0.9715 - val_loss: 58.2034 - val_acc: 0.9525\n",
      "Epoch 109/500\n",
      "948/948 [==============================] - 0s - loss: 117.8379 - acc: 0.9673 - val_loss: 60.5412 - val_acc: 0.9525\n",
      "Epoch 110/500\n",
      "948/948 [==============================] - 0s - loss: 116.4218 - acc: 0.9684 - val_loss: 58.9882 - val_acc: 0.9525\n",
      "Epoch 111/500\n",
      "948/948 [==============================] - 0s - loss: 114.9118 - acc: 0.9673 - val_loss: 63.5825 - val_acc: 0.9525\n",
      "Epoch 112/500\n",
      "948/948 [==============================] - 0s - loss: 123.1909 - acc: 0.9736 - val_loss: 58.6701 - val_acc: 0.9525\n",
      "Epoch 113/500\n",
      "948/948 [==============================] - 0s - loss: 116.1351 - acc: 0.9757 - val_loss: 61.1423 - val_acc: 0.9525\n",
      "Epoch 114/500\n",
      "948/948 [==============================] - 0s - loss: 124.3244 - acc: 0.9726 - val_loss: 60.8233 - val_acc: 0.9525\n",
      "Epoch 115/500\n",
      "948/948 [==============================] - 0s - loss: 115.3367 - acc: 0.9694 - val_loss: 64.1693 - val_acc: 0.9525\n",
      "Epoch 116/500\n",
      "948/948 [==============================] - 0s - loss: 116.0733 - acc: 0.9705 - val_loss: 66.6466 - val_acc: 0.9557\n",
      "Epoch 117/500\n",
      "948/948 [==============================] - 0s - loss: 122.4010 - acc: 0.9715 - val_loss: 61.7486 - val_acc: 0.9525\n",
      "Epoch 118/500\n",
      "948/948 [==============================] - 0s - loss: 112.9172 - acc: 0.9705 - val_loss: 60.6944 - val_acc: 0.9525\n",
      "Epoch 119/500\n",
      "948/948 [==============================] - 0s - loss: 121.2267 - acc: 0.9726 - val_loss: 61.0901 - val_acc: 0.9557\n",
      "Epoch 120/500\n",
      "948/948 [==============================] - 0s - loss: 125.0514 - acc: 0.9662 - val_loss: 68.4210 - val_acc: 0.9525\n",
      "Epoch 121/500\n",
      "948/948 [==============================] - 0s - loss: 114.6350 - acc: 0.9715 - val_loss: 61.8046 - val_acc: 0.9525\n",
      "Epoch 122/500\n",
      "948/948 [==============================] - 0s - loss: 112.2771 - acc: 0.9726 - val_loss: 60.1680 - val_acc: 0.9494\n",
      "Epoch 123/500\n",
      "948/948 [==============================] - 0s - loss: 111.8157 - acc: 0.9757 - val_loss: 58.6994 - val_acc: 0.9525\n",
      "Epoch 124/500\n",
      "948/948 [==============================] - 0s - loss: 120.1988 - acc: 0.9694 - val_loss: 61.0493 - val_acc: 0.9525\n",
      "Epoch 125/500\n",
      "948/948 [==============================] - 0s - loss: 116.6261 - acc: 0.9768 - val_loss: 61.0694 - val_acc: 0.9525\n",
      "Epoch 126/500\n",
      "948/948 [==============================] - 0s - loss: 105.7269 - acc: 0.9673 - val_loss: 60.7780 - val_acc: 0.9525\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 116.4704 - acc: 0.9673 - val_loss: 61.2159 - val_acc: 0.9525\n",
      "Epoch 128/500\n",
      "948/948 [==============================] - 0s - loss: 113.5059 - acc: 0.9747 - val_loss: 62.0036 - val_acc: 0.9525\n",
      "Epoch 129/500\n",
      "948/948 [==============================] - 0s - loss: 114.6464 - acc: 0.9726 - val_loss: 62.4180 - val_acc: 0.9525\n",
      "Epoch 130/500\n",
      "948/948 [==============================] - 0s - loss: 115.7620 - acc: 0.9673 - val_loss: 62.3867 - val_acc: 0.9525\n",
      "Epoch 131/500\n",
      "948/948 [==============================] - 0s - loss: 115.9976 - acc: 0.9684 - val_loss: 61.8138 - val_acc: 0.9525\n",
      "Epoch 132/500\n",
      "948/948 [==============================] - 0s - loss: 104.1473 - acc: 0.9715 - val_loss: 63.4327 - val_acc: 0.9494\n",
      "Epoch 133/500\n",
      "948/948 [==============================] - 0s - loss: 106.5430 - acc: 0.9715 - val_loss: 62.5652 - val_acc: 0.9494\n",
      "Epoch 134/500\n",
      "948/948 [==============================] - 0s - loss: 105.7789 - acc: 0.9726 - val_loss: 59.9639 - val_acc: 0.9462\n",
      "Epoch 135/500\n",
      "948/948 [==============================] - 0s - loss: 110.8600 - acc: 0.9757 - val_loss: 64.6963 - val_acc: 0.9462\n",
      "Epoch 136/500\n",
      "948/948 [==============================] - 0s - loss: 111.3699 - acc: 0.9652 - val_loss: 65.4972 - val_acc: 0.9525\n",
      "Epoch 137/500\n",
      "948/948 [==============================] - 0s - loss: 114.9484 - acc: 0.9747 - val_loss: 61.4474 - val_acc: 0.9525\n",
      "Epoch 138/500\n",
      "948/948 [==============================] - 0s - loss: 108.9177 - acc: 0.9652 - val_loss: 63.7878 - val_acc: 0.9525\n",
      "Epoch 139/500\n",
      "948/948 [==============================] - 0s - loss: 115.7798 - acc: 0.9736 - val_loss: 64.5635 - val_acc: 0.9525\n",
      "Epoch 140/500\n",
      "948/948 [==============================] - 0s - loss: 104.8849 - acc: 0.9747 - val_loss: 61.8078 - val_acc: 0.9525\n",
      "Epoch 141/500\n",
      "948/948 [==============================] - 0s - loss: 115.3125 - acc: 0.9694 - val_loss: 60.2982 - val_acc: 0.9525\n",
      "Epoch 142/500\n",
      "948/948 [==============================] - 0s - loss: 103.8628 - acc: 0.9705 - val_loss: 59.7268 - val_acc: 0.9557\n",
      "Epoch 143/500\n",
      "948/948 [==============================] - 0s - loss: 107.8822 - acc: 0.9705 - val_loss: 61.4393 - val_acc: 0.9525\n",
      "Epoch 144/500\n",
      "948/948 [==============================] - 0s - loss: 107.3622 - acc: 0.9726 - val_loss: 60.9894 - val_acc: 0.9525\n",
      "Epoch 145/500\n",
      "948/948 [==============================] - 0s - loss: 106.7654 - acc: 0.9652 - val_loss: 64.0883 - val_acc: 0.9525\n",
      "Epoch 146/500\n",
      "948/948 [==============================] - 0s - loss: 101.4013 - acc: 0.9726 - val_loss: 60.6026 - val_acc: 0.9525\n",
      "Epoch 147/500\n",
      "948/948 [==============================] - 0s - loss: 104.2431 - acc: 0.9747 - val_loss: 58.1018 - val_acc: 0.9525\n",
      "Epoch 148/500\n",
      "948/948 [==============================] - 0s - loss: 115.0616 - acc: 0.9778 - val_loss: 59.7682 - val_acc: 0.9462\n",
      "Epoch 149/500\n",
      "948/948 [==============================] - 0s - loss: 118.6908 - acc: 0.9726 - val_loss: 62.3073 - val_acc: 0.9525\n",
      "Epoch 150/500\n",
      "948/948 [==============================] - 0s - loss: 117.7670 - acc: 0.9726 - val_loss: 58.0063 - val_acc: 0.9525\n",
      "Epoch 151/500\n",
      "948/948 [==============================] - 0s - loss: 104.2476 - acc: 0.9800 - val_loss: 60.4917 - val_acc: 0.9462\n",
      "Epoch 152/500\n",
      "948/948 [==============================] - 0s - loss: 104.8300 - acc: 0.9736 - val_loss: 60.1709 - val_acc: 0.9525\n",
      "Epoch 153/500\n",
      "948/948 [==============================] - 0s - loss: 106.3476 - acc: 0.9821 - val_loss: 58.3388 - val_acc: 0.9525\n",
      "Epoch 154/500\n",
      "948/948 [==============================] - 0s - loss: 111.3146 - acc: 0.9705 - val_loss: 60.7754 - val_acc: 0.9525\n",
      "Epoch 155/500\n",
      "948/948 [==============================] - 0s - loss: 112.9222 - acc: 0.9757 - val_loss: 59.5028 - val_acc: 0.9525\n",
      "Epoch 156/500\n",
      "948/948 [==============================] - 0s - loss: 98.1052 - acc: 0.9715 - val_loss: 57.9397 - val_acc: 0.9525\n",
      "Epoch 157/500\n",
      "948/948 [==============================] - 0s - loss: 117.4581 - acc: 0.9705 - val_loss: 57.7498 - val_acc: 0.9525\n",
      "Epoch 158/500\n",
      "948/948 [==============================] - 0s - loss: 104.5864 - acc: 0.9757 - val_loss: 58.9758 - val_acc: 0.9525\n",
      "Epoch 159/500\n",
      "948/948 [==============================] - 0s - loss: 114.9342 - acc: 0.9757 - val_loss: 59.8489 - val_acc: 0.9462\n",
      "Epoch 160/500\n",
      "948/948 [==============================] - 0s - loss: 114.5721 - acc: 0.9757 - val_loss: 57.5680 - val_acc: 0.9462\n",
      "Epoch 161/500\n",
      "948/948 [==============================] - 0s - loss: 100.9007 - acc: 0.9757 - val_loss: 56.4953 - val_acc: 0.9525\n",
      "Epoch 162/500\n",
      "948/948 [==============================] - 0s - loss: 112.3162 - acc: 0.9652 - val_loss: 57.1552 - val_acc: 0.9525\n",
      "Epoch 163/500\n",
      "948/948 [==============================] - 0s - loss: 103.5055 - acc: 0.9747 - val_loss: 56.9476 - val_acc: 0.9525\n",
      "Epoch 164/500\n",
      "948/948 [==============================] - 0s - loss: 106.6637 - acc: 0.9726 - val_loss: 58.1514 - val_acc: 0.9525\n",
      "Epoch 165/500\n",
      "948/948 [==============================] - 0s - loss: 109.3985 - acc: 0.9768 - val_loss: 59.6460 - val_acc: 0.9525\n",
      "Epoch 166/500\n",
      "948/948 [==============================] - 0s - loss: 103.5364 - acc: 0.9705 - val_loss: 56.9578 - val_acc: 0.9525\n",
      "Epoch 167/500\n",
      "948/948 [==============================] - 0s - loss: 110.9770 - acc: 0.9684 - val_loss: 56.3728 - val_acc: 0.9525\n",
      "Epoch 168/500\n",
      "948/948 [==============================] - 0s - loss: 101.1866 - acc: 0.9852 - val_loss: 59.0064 - val_acc: 0.9525\n",
      "Epoch 169/500\n",
      "948/948 [==============================] - 0s - loss: 101.7265 - acc: 0.9726 - val_loss: 60.3517 - val_acc: 0.9525\n",
      "Epoch 170/500\n",
      "948/948 [==============================] - 0s - loss: 108.0653 - acc: 0.9726 - val_loss: 58.2459 - val_acc: 0.9525\n",
      "Epoch 171/500\n",
      "948/948 [==============================] - 0s - loss: 103.3289 - acc: 0.9757 - val_loss: 59.8602 - val_acc: 0.9525\n",
      "Epoch 172/500\n",
      "948/948 [==============================] - 0s - loss: 101.9516 - acc: 0.9736 - val_loss: 59.4901 - val_acc: 0.9525\n",
      "Epoch 173/500\n",
      "948/948 [==============================] - 0s - loss: 107.9716 - acc: 0.9726 - val_loss: 60.8535 - val_acc: 0.9525\n",
      "Epoch 174/500\n",
      "948/948 [==============================] - 0s - loss: 102.3148 - acc: 0.9757 - val_loss: 59.4741 - val_acc: 0.9525\n",
      "Epoch 175/500\n",
      "948/948 [==============================] - 0s - loss: 106.0720 - acc: 0.9778 - val_loss: 60.1596 - val_acc: 0.9525\n",
      "Epoch 176/500\n",
      "948/948 [==============================] - 0s - loss: 105.7180 - acc: 0.9673 - val_loss: 60.7713 - val_acc: 0.9525\n",
      "Epoch 177/500\n",
      "948/948 [==============================] - 0s - loss: 109.9400 - acc: 0.9747 - val_loss: 59.8850 - val_acc: 0.9525\n",
      "Epoch 178/500\n",
      "948/948 [==============================] - 0s - loss: 96.8006 - acc: 0.9810 - val_loss: 65.3940 - val_acc: 0.9525\n",
      "Epoch 179/500\n",
      "948/948 [==============================] - 0s - loss: 103.7282 - acc: 0.9747 - val_loss: 60.9376 - val_acc: 0.9494\n",
      "Epoch 180/500\n",
      "948/948 [==============================] - 0s - loss: 106.1368 - acc: 0.9768 - val_loss: 61.0307 - val_acc: 0.9525\n",
      "Epoch 181/500\n",
      "948/948 [==============================] - 0s - loss: 100.3715 - acc: 0.9768 - val_loss: 61.2359 - val_acc: 0.9525\n",
      "Epoch 182/500\n",
      "948/948 [==============================] - 0s - loss: 107.4588 - acc: 0.9778 - val_loss: 60.5016 - val_acc: 0.9525\n",
      "Epoch 183/500\n",
      "948/948 [==============================] - 0s - loss: 101.1925 - acc: 0.9778 - val_loss: 62.4912 - val_acc: 0.9525\n",
      "Epoch 184/500\n",
      "948/948 [==============================] - 0s - loss: 105.2549 - acc: 0.9736 - val_loss: 60.7748 - val_acc: 0.9525\n",
      "Epoch 185/500\n",
      "948/948 [==============================] - 0s - loss: 102.5261 - acc: 0.9747 - val_loss: 59.7588 - val_acc: 0.9525\n",
      "Epoch 186/500\n",
      "948/948 [==============================] - 0s - loss: 106.6961 - acc: 0.9736 - val_loss: 57.8731 - val_acc: 0.9525\n",
      "Epoch 187/500\n",
      "948/948 [==============================] - 0s - loss: 104.7757 - acc: 0.9768 - val_loss: 59.6663 - val_acc: 0.9525\n",
      "Epoch 188/500\n",
      "948/948 [==============================] - 0s - loss: 100.8386 - acc: 0.9736 - val_loss: 60.7451 - val_acc: 0.9525\n",
      "Epoch 189/500\n",
      "948/948 [==============================] - 0s - loss: 98.1216 - acc: 0.9705 - val_loss: 59.6418 - val_acc: 0.9494\n",
      "Epoch 190/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 101.7615 - acc: 0.9715 - val_loss: 59.6707 - val_acc: 0.9525\n",
      "Epoch 191/500\n",
      "948/948 [==============================] - 0s - loss: 98.3049 - acc: 0.9757 - val_loss: 57.5293 - val_acc: 0.9525\n",
      "Epoch 192/500\n",
      "948/948 [==============================] - 0s - loss: 108.7097 - acc: 0.9694 - val_loss: 57.5233 - val_acc: 0.9525\n",
      "Epoch 193/500\n",
      "948/948 [==============================] - 0s - loss: 98.8943 - acc: 0.9768 - val_loss: 58.5088 - val_acc: 0.9430\n",
      "Epoch 194/500\n",
      "948/948 [==============================] - 0s - loss: 102.1378 - acc: 0.9800 - val_loss: 58.3523 - val_acc: 0.9430\n",
      "Epoch 195/500\n",
      "948/948 [==============================] - 0s - loss: 95.1120 - acc: 0.9715 - val_loss: 57.3788 - val_acc: 0.9525\n",
      "Epoch 196/500\n",
      "948/948 [==============================] - 0s - loss: 98.6851 - acc: 0.9747 - val_loss: 56.5246 - val_acc: 0.9525\n",
      "Epoch 197/500\n",
      "948/948 [==============================] - 0s - loss: 100.2514 - acc: 0.9810 - val_loss: 58.0721 - val_acc: 0.9525\n",
      "Epoch 198/500\n",
      "948/948 [==============================] - 0s - loss: 100.2032 - acc: 0.9694 - val_loss: 62.7483 - val_acc: 0.9525\n",
      "Epoch 199/500\n",
      "948/948 [==============================] - 0s - loss: 103.1385 - acc: 0.9705 - val_loss: 60.7840 - val_acc: 0.9525\n",
      "Epoch 200/500\n",
      "948/948 [==============================] - 0s - loss: 100.3129 - acc: 0.9747 - val_loss: 58.6551 - val_acc: 0.9525\n",
      "Epoch 201/500\n",
      "948/948 [==============================] - 0s - loss: 105.6673 - acc: 0.9715 - val_loss: 58.9864 - val_acc: 0.9525\n",
      "Epoch 202/500\n",
      "948/948 [==============================] - 0s - loss: 114.7720 - acc: 0.9757 - val_loss: 58.6776 - val_acc: 0.9494\n",
      "Epoch 203/500\n",
      "948/948 [==============================] - 0s - loss: 107.6276 - acc: 0.9768 - val_loss: 57.1586 - val_acc: 0.9525\n",
      "Epoch 204/500\n",
      "948/948 [==============================] - 0s - loss: 98.9635 - acc: 0.9810 - val_loss: 56.6640 - val_acc: 0.9525\n",
      "Epoch 205/500\n",
      "948/948 [==============================] - 0s - loss: 105.1929 - acc: 0.9842 - val_loss: 58.0852 - val_acc: 0.9525\n",
      "Epoch 206/500\n",
      "948/948 [==============================] - 0s - loss: 100.3794 - acc: 0.9736 - val_loss: 56.9392 - val_acc: 0.9525\n",
      "Epoch 207/500\n",
      "948/948 [==============================] - 0s - loss: 106.4850 - acc: 0.9726 - val_loss: 57.4091 - val_acc: 0.9494\n",
      "Epoch 208/500\n",
      "948/948 [==============================] - 0s - loss: 102.5423 - acc: 0.9747 - val_loss: 62.5635 - val_acc: 0.9462\n",
      "Epoch 209/500\n",
      "948/948 [==============================] - 0s - loss: 94.8163 - acc: 0.9800 - val_loss: 62.0532 - val_acc: 0.9462\n",
      "Epoch 210/500\n",
      "948/948 [==============================] - 0s - loss: 94.1119 - acc: 0.9705 - val_loss: 60.8647 - val_acc: 0.9462\n",
      "Epoch 211/500\n",
      "948/948 [==============================] - 0s - loss: 97.1020 - acc: 0.9778 - val_loss: 61.1235 - val_acc: 0.9525\n",
      "Epoch 212/500\n",
      "948/948 [==============================] - 0s - loss: 100.5680 - acc: 0.9705 - val_loss: 62.2291 - val_acc: 0.9525\n",
      "Epoch 213/500\n",
      "948/948 [==============================] - 0s - loss: 106.2768 - acc: 0.9726 - val_loss: 60.1994 - val_acc: 0.9525\n",
      "Epoch 214/500\n",
      "948/948 [==============================] - 0s - loss: 94.4924 - acc: 0.9789 - val_loss: 60.7370 - val_acc: 0.9494\n",
      "Epoch 215/500\n",
      "948/948 [==============================] - 0s - loss: 102.2221 - acc: 0.9778 - val_loss: 62.7202 - val_acc: 0.9525\n",
      "Epoch 216/500\n",
      "948/948 [==============================] - 0s - loss: 93.9549 - acc: 0.9726 - val_loss: 59.7575 - val_acc: 0.9525\n",
      "Epoch 217/500\n",
      "948/948 [==============================] - 0s - loss: 100.2309 - acc: 0.9810 - val_loss: 60.7448 - val_acc: 0.9525\n",
      "Epoch 218/500\n",
      "948/948 [==============================] - 0s - loss: 96.9955 - acc: 0.9778 - val_loss: 60.9374 - val_acc: 0.9525\n",
      "Epoch 219/500\n",
      "948/948 [==============================] - 0s - loss: 102.3080 - acc: 0.9778 - val_loss: 59.3779 - val_acc: 0.9462\n",
      "Epoch 220/500\n",
      "948/948 [==============================] - 0s - loss: 98.2517 - acc: 0.9768 - val_loss: 59.1648 - val_acc: 0.9494\n",
      "Epoch 221/500\n",
      "948/948 [==============================] - 0s - loss: 105.9208 - acc: 0.9778 - val_loss: 63.6233 - val_acc: 0.9494\n",
      "Epoch 222/500\n",
      "948/948 [==============================] - 0s - loss: 98.7240 - acc: 0.9757 - val_loss: 61.3264 - val_acc: 0.9494\n",
      "Epoch 223/500\n",
      "948/948 [==============================] - 0s - loss: 100.2017 - acc: 0.9747 - val_loss: 63.0358 - val_acc: 0.9430\n",
      "Epoch 224/500\n",
      "948/948 [==============================] - 0s - loss: 94.4748 - acc: 0.9747 - val_loss: 60.9599 - val_acc: 0.9525\n",
      "Epoch 225/500\n",
      "948/948 [==============================] - 0s - loss: 104.8515 - acc: 0.9726 - val_loss: 61.1352 - val_acc: 0.9525\n",
      "Epoch 226/500\n",
      "948/948 [==============================] - 0s - loss: 107.1876 - acc: 0.9810 - val_loss: 61.3231 - val_acc: 0.9525\n",
      "Epoch 227/500\n",
      "948/948 [==============================] - 0s - loss: 107.8135 - acc: 0.9800 - val_loss: 61.0921 - val_acc: 0.9494\n",
      "Epoch 228/500\n",
      "948/948 [==============================] - 0s - loss: 97.6992 - acc: 0.9705 - val_loss: 60.2116 - val_acc: 0.9525\n"
     ]
    }
   ],
   "source": [
    "model1 = classifier1()\n",
    "model1.fit(train_X, train_Y, validation_data=(val_X, val_Y), nb_epoch=nb_epochs, callbacks=[earlyStopping, Model_best], batch_size=batch_size)\n",
    "pre_Y1 = model1.predict(train_x)\n",
    "pre_Y1_t = model1.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.73687460425\n"
     ]
    }
   ],
   "source": [
    "error_1, accuracy_1 = accuracy(pre_Y1_t, test_y)\n",
    "print(accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_dim=992, activation=\"relu\", use_bias=True)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, activation=\"relu\", use_bias=True)`\n",
      "  \"\"\"\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2, activation=\"linear\", use_bias=True)`\n",
      "  import sys\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 948 samples, validate on 316 samples\n",
      "Epoch 1/500\n",
      "948/948 [==============================] - 0s - loss: 4642.2260 - acc: 0.6160 - val_loss: 4489.7891 - val_acc: 0.9462\n",
      "Epoch 2/500\n",
      "948/948 [==============================] - 0s - loss: 4370.5042 - acc: 0.8639 - val_loss: 4035.9400 - val_acc: 0.9462\n",
      "Epoch 3/500\n",
      "948/948 [==============================] - 0s - loss: 3629.1826 - acc: 0.8914 - val_loss: 2995.7110 - val_acc: 0.9462\n",
      "Epoch 4/500\n",
      "948/948 [==============================] - 0s - loss: 2370.5744 - acc: 0.8861 - val_loss: 1686.4496 - val_acc: 0.9462\n",
      "Epoch 5/500\n",
      "948/948 [==============================] - 0s - loss: 1363.4110 - acc: 0.9335 - val_loss: 985.8098 - val_acc: 0.9462\n",
      "Epoch 6/500\n",
      "948/948 [==============================] - 0s - loss: 917.8789 - acc: 0.9430 - val_loss: 656.3494 - val_acc: 0.9462\n",
      "Epoch 7/500\n",
      "948/948 [==============================] - 0s - loss: 632.0813 - acc: 0.9430 - val_loss: 501.2610 - val_acc: 0.9462\n",
      "Epoch 8/500\n",
      "948/948 [==============================] - 0s - loss: 540.8696 - acc: 0.9430 - val_loss: 389.7577 - val_acc: 0.9462\n",
      "Epoch 9/500\n",
      "948/948 [==============================] - 0s - loss: 476.0533 - acc: 0.9430 - val_loss: 330.9035 - val_acc: 0.9462\n",
      "Epoch 10/500\n",
      "948/948 [==============================] - 0s - loss: 414.9945 - acc: 0.9430 - val_loss: 286.8450 - val_acc: 0.9462\n",
      "Epoch 11/500\n",
      "948/948 [==============================] - 0s - loss: 366.8243 - acc: 0.9430 - val_loss: 254.5543 - val_acc: 0.9462\n",
      "Epoch 12/500\n",
      "948/948 [==============================] - 0s - loss: 349.3258 - acc: 0.9430 - val_loss: 236.6534 - val_acc: 0.9462\n",
      "Epoch 13/500\n",
      "948/948 [==============================] - 0s - loss: 348.7370 - acc: 0.9430 - val_loss: 229.5152 - val_acc: 0.9462\n",
      "Epoch 14/500\n",
      "948/948 [==============================] - 0s - loss: 309.7748 - acc: 0.9430 - val_loss: 213.4755 - val_acc: 0.9462\n",
      "Epoch 15/500\n",
      "948/948 [==============================] - 0s - loss: 315.0898 - acc: 0.9430 - val_loss: 205.3952 - val_acc: 0.9462\n",
      "Epoch 16/500\n",
      "948/948 [==============================] - 0s - loss: 304.5327 - acc: 0.9430 - val_loss: 203.4394 - val_acc: 0.9462\n",
      "Epoch 17/500\n",
      "948/948 [==============================] - 0s - loss: 284.3218 - acc: 0.9430 - val_loss: 191.6889 - val_acc: 0.9462\n",
      "Epoch 18/500\n",
      "948/948 [==============================] - 0s - loss: 282.1597 - acc: 0.9430 - val_loss: 186.2904 - val_acc: 0.9462\n",
      "Epoch 19/500\n",
      "948/948 [==============================] - 0s - loss: 285.9256 - acc: 0.9430 - val_loss: 181.0709 - val_acc: 0.9462\n",
      "Epoch 20/500\n",
      "948/948 [==============================] - 0s - loss: 281.2442 - acc: 0.9430 - val_loss: 180.9927 - val_acc: 0.9462\n",
      "Epoch 21/500\n",
      "948/948 [==============================] - 0s - loss: 274.1834 - acc: 0.9430 - val_loss: 175.3418 - val_acc: 0.9462\n",
      "Epoch 22/500\n",
      "948/948 [==============================] - 0s - loss: 252.6124 - acc: 0.9430 - val_loss: 170.0168 - val_acc: 0.9462\n",
      "Epoch 23/500\n",
      "948/948 [==============================] - 0s - loss: 268.7964 - acc: 0.9430 - val_loss: 165.5870 - val_acc: 0.9462\n",
      "Epoch 24/500\n",
      "948/948 [==============================] - 0s - loss: 259.2558 - acc: 0.9430 - val_loss: 158.4485 - val_acc: 0.9462\n",
      "Epoch 25/500\n",
      "948/948 [==============================] - 0s - loss: 247.3124 - acc: 0.9430 - val_loss: 154.0111 - val_acc: 0.9462\n",
      "Epoch 26/500\n",
      "948/948 [==============================] - 0s - loss: 243.4316 - acc: 0.9430 - val_loss: 149.7409 - val_acc: 0.9462\n",
      "Epoch 27/500\n",
      "948/948 [==============================] - 0s - loss: 239.9543 - acc: 0.9430 - val_loss: 142.7184 - val_acc: 0.9462\n",
      "Epoch 28/500\n",
      "948/948 [==============================] - 0s - loss: 228.6694 - acc: 0.9430 - val_loss: 142.4368 - val_acc: 0.9462\n",
      "Epoch 29/500\n",
      "948/948 [==============================] - 0s - loss: 231.8461 - acc: 0.9430 - val_loss: 130.0577 - val_acc: 0.9462\n",
      "Epoch 30/500\n",
      "948/948 [==============================] - 0s - loss: 221.6180 - acc: 0.9430 - val_loss: 125.5725 - val_acc: 0.9462\n",
      "Epoch 31/500\n",
      "948/948 [==============================] - 0s - loss: 219.6040 - acc: 0.9451 - val_loss: 117.9207 - val_acc: 0.9462\n",
      "Epoch 32/500\n",
      "948/948 [==============================] - 0s - loss: 214.3636 - acc: 0.9462 - val_loss: 110.8286 - val_acc: 0.9462\n",
      "Epoch 33/500\n",
      "948/948 [==============================] - 0s - loss: 196.0679 - acc: 0.9494 - val_loss: 106.4194 - val_acc: 0.9462\n",
      "Epoch 34/500\n",
      "948/948 [==============================] - 0s - loss: 194.8886 - acc: 0.9536 - val_loss: 99.9040 - val_acc: 0.9462\n",
      "Epoch 35/500\n",
      "948/948 [==============================] - 0s - loss: 202.7009 - acc: 0.9515 - val_loss: 98.4716 - val_acc: 0.9462\n",
      "Epoch 36/500\n",
      "948/948 [==============================] - 0s - loss: 194.3378 - acc: 0.9546 - val_loss: 95.2331 - val_acc: 0.9462\n",
      "Epoch 37/500\n",
      "948/948 [==============================] - 0s - loss: 198.4887 - acc: 0.9473 - val_loss: 100.6015 - val_acc: 0.9430\n",
      "Epoch 38/500\n",
      "948/948 [==============================] - 0s - loss: 184.2078 - acc: 0.9525 - val_loss: 92.3611 - val_acc: 0.9430\n",
      "Epoch 39/500\n",
      "948/948 [==============================] - 0s - loss: 186.1184 - acc: 0.9483 - val_loss: 92.5104 - val_acc: 0.9430\n",
      "Epoch 40/500\n",
      "948/948 [==============================] - 0s - loss: 185.2581 - acc: 0.9568 - val_loss: 88.6881 - val_acc: 0.9399\n",
      "Epoch 41/500\n",
      "948/948 [==============================] - 0s - loss: 184.3068 - acc: 0.9515 - val_loss: 87.9606 - val_acc: 0.9462\n",
      "Epoch 42/500\n",
      "948/948 [==============================] - 0s - loss: 179.3148 - acc: 0.9578 - val_loss: 85.5097 - val_acc: 0.9494\n",
      "Epoch 43/500\n",
      "948/948 [==============================] - 0s - loss: 175.3400 - acc: 0.9557 - val_loss: 87.9251 - val_acc: 0.9494\n",
      "Epoch 44/500\n",
      "948/948 [==============================] - 0s - loss: 169.1460 - acc: 0.9557 - val_loss: 89.3187 - val_acc: 0.9494\n",
      "Epoch 45/500\n",
      "948/948 [==============================] - 0s - loss: 188.6111 - acc: 0.9536 - val_loss: 84.5779 - val_acc: 0.9462\n",
      "Epoch 46/500\n",
      "948/948 [==============================] - 0s - loss: 179.2062 - acc: 0.9557 - val_loss: 86.3142 - val_acc: 0.9494\n",
      "Epoch 47/500\n",
      "948/948 [==============================] - 0s - loss: 180.6683 - acc: 0.9620 - val_loss: 82.6640 - val_acc: 0.9494\n",
      "Epoch 48/500\n",
      "948/948 [==============================] - 0s - loss: 169.0118 - acc: 0.9631 - val_loss: 80.5330 - val_acc: 0.9494\n",
      "Epoch 49/500\n",
      "948/948 [==============================] - 0s - loss: 184.7011 - acc: 0.9536 - val_loss: 83.5716 - val_acc: 0.9494\n",
      "Epoch 50/500\n",
      "948/948 [==============================] - 0s - loss: 160.1051 - acc: 0.9599 - val_loss: 82.6218 - val_acc: 0.9494\n",
      "Epoch 51/500\n",
      "948/948 [==============================] - 0s - loss: 173.1654 - acc: 0.9589 - val_loss: 77.5085 - val_acc: 0.9494\n",
      "Epoch 52/500\n",
      "948/948 [==============================] - 0s - loss: 153.8226 - acc: 0.9568 - val_loss: 79.2209 - val_acc: 0.9494\n",
      "Epoch 53/500\n",
      "948/948 [==============================] - 0s - loss: 162.3436 - acc: 0.9546 - val_loss: 77.7574 - val_acc: 0.9494\n",
      "Epoch 54/500\n",
      "948/948 [==============================] - 0s - loss: 165.9363 - acc: 0.9610 - val_loss: 76.1436 - val_acc: 0.9462\n",
      "Epoch 55/500\n",
      "948/948 [==============================] - 0s - loss: 164.8578 - acc: 0.9610 - val_loss: 75.7873 - val_acc: 0.9494\n",
      "Epoch 56/500\n",
      "948/948 [==============================] - 0s - loss: 168.6070 - acc: 0.9568 - val_loss: 76.3995 - val_acc: 0.9494\n",
      "Epoch 57/500\n",
      "948/948 [==============================] - 0s - loss: 161.3555 - acc: 0.9589 - val_loss: 74.1014 - val_acc: 0.9462\n",
      "Epoch 58/500\n",
      "948/948 [==============================] - 0s - loss: 160.0104 - acc: 0.9620 - val_loss: 76.9885 - val_acc: 0.9462\n",
      "Epoch 59/500\n",
      "948/948 [==============================] - 0s - loss: 172.2152 - acc: 0.9610 - val_loss: 71.6863 - val_acc: 0.9494\n",
      "Epoch 60/500\n",
      "948/948 [==============================] - 0s - loss: 166.2301 - acc: 0.9641 - val_loss: 76.9762 - val_acc: 0.9462\n",
      "Epoch 61/500\n",
      "948/948 [==============================] - 0s - loss: 168.2006 - acc: 0.9610 - val_loss: 77.0413 - val_acc: 0.9462\n",
      "Epoch 62/500\n",
      "948/948 [==============================] - 0s - loss: 162.0906 - acc: 0.9568 - val_loss: 75.9593 - val_acc: 0.9462\n",
      "Epoch 63/500\n",
      "948/948 [==============================] - 0s - loss: 170.6856 - acc: 0.9641 - val_loss: 73.9624 - val_acc: 0.9462\n",
      "Epoch 64/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 168.2452 - acc: 0.9610 - val_loss: 75.2999 - val_acc: 0.9462\n",
      "Epoch 65/500\n",
      "948/948 [==============================] - 0s - loss: 147.7498 - acc: 0.9631 - val_loss: 71.0004 - val_acc: 0.9462\n",
      "Epoch 66/500\n",
      "948/948 [==============================] - 0s - loss: 154.8771 - acc: 0.9631 - val_loss: 69.5468 - val_acc: 0.9462\n",
      "Epoch 67/500\n",
      "948/948 [==============================] - 0s - loss: 152.0370 - acc: 0.9694 - val_loss: 73.7354 - val_acc: 0.9462\n",
      "Epoch 68/500\n",
      "948/948 [==============================] - 0s - loss: 159.1344 - acc: 0.9599 - val_loss: 70.6920 - val_acc: 0.9462\n",
      "Epoch 69/500\n",
      "948/948 [==============================] - 0s - loss: 168.2244 - acc: 0.9568 - val_loss: 73.8435 - val_acc: 0.9462\n",
      "Epoch 70/500\n",
      "948/948 [==============================] - 0s - loss: 150.1764 - acc: 0.9610 - val_loss: 68.8705 - val_acc: 0.9462\n",
      "Epoch 71/500\n",
      "948/948 [==============================] - 0s - loss: 141.7720 - acc: 0.9536 - val_loss: 70.8193 - val_acc: 0.9462\n",
      "Epoch 72/500\n",
      "948/948 [==============================] - 0s - loss: 162.1227 - acc: 0.9578 - val_loss: 70.5520 - val_acc: 0.9462\n",
      "Epoch 73/500\n",
      "948/948 [==============================] - 0s - loss: 152.4093 - acc: 0.9589 - val_loss: 70.6277 - val_acc: 0.9462\n",
      "Epoch 74/500\n",
      "948/948 [==============================] - 0s - loss: 150.8664 - acc: 0.9631 - val_loss: 74.2161 - val_acc: 0.9462\n",
      "Epoch 75/500\n",
      "948/948 [==============================] - 0s - loss: 149.6731 - acc: 0.9631 - val_loss: 70.9040 - val_acc: 0.9462\n",
      "Epoch 76/500\n",
      "948/948 [==============================] - 0s - loss: 145.6510 - acc: 0.9673 - val_loss: 69.3999 - val_acc: 0.9462\n",
      "Epoch 77/500\n",
      "948/948 [==============================] - 0s - loss: 144.1604 - acc: 0.9694 - val_loss: 69.2606 - val_acc: 0.9462\n",
      "Epoch 78/500\n",
      "948/948 [==============================] - 0s - loss: 151.8950 - acc: 0.9620 - val_loss: 69.9165 - val_acc: 0.9494\n",
      "Epoch 79/500\n",
      "948/948 [==============================] - 0s - loss: 149.6786 - acc: 0.9652 - val_loss: 69.1186 - val_acc: 0.9494\n",
      "Epoch 80/500\n",
      "948/948 [==============================] - 0s - loss: 153.7852 - acc: 0.9599 - val_loss: 68.5483 - val_acc: 0.9462\n",
      "Epoch 81/500\n",
      "948/948 [==============================] - 0s - loss: 149.3566 - acc: 0.9652 - val_loss: 67.8688 - val_acc: 0.9494\n",
      "Epoch 82/500\n",
      "948/948 [==============================] - 0s - loss: 142.4354 - acc: 0.9673 - val_loss: 66.1623 - val_acc: 0.9494\n",
      "Epoch 83/500\n",
      "948/948 [==============================] - 0s - loss: 148.6652 - acc: 0.9641 - val_loss: 67.5409 - val_acc: 0.9494\n",
      "Epoch 84/500\n",
      "948/948 [==============================] - 0s - loss: 147.2983 - acc: 0.9620 - val_loss: 69.3247 - val_acc: 0.9494\n",
      "Epoch 85/500\n",
      "948/948 [==============================] - 0s - loss: 150.1325 - acc: 0.9620 - val_loss: 66.2762 - val_acc: 0.9462\n",
      "Epoch 86/500\n",
      "948/948 [==============================] - 0s - loss: 140.8558 - acc: 0.9610 - val_loss: 67.1228 - val_acc: 0.9525\n",
      "Epoch 87/500\n",
      "948/948 [==============================] - 0s - loss: 141.7545 - acc: 0.9715 - val_loss: 66.8646 - val_acc: 0.9525\n",
      "Epoch 88/500\n",
      "948/948 [==============================] - 0s - loss: 144.2052 - acc: 0.9620 - val_loss: 63.8364 - val_acc: 0.9525\n",
      "Epoch 89/500\n",
      "948/948 [==============================] - 0s - loss: 144.2380 - acc: 0.9631 - val_loss: 65.4966 - val_acc: 0.9525\n",
      "Epoch 90/500\n",
      "948/948 [==============================] - 0s - loss: 148.2607 - acc: 0.9641 - val_loss: 65.9197 - val_acc: 0.9525\n",
      "Epoch 91/500\n",
      "948/948 [==============================] - 0s - loss: 148.0582 - acc: 0.9673 - val_loss: 69.6040 - val_acc: 0.9494\n",
      "Epoch 92/500\n",
      "948/948 [==============================] - 0s - loss: 148.3976 - acc: 0.9610 - val_loss: 67.9929 - val_acc: 0.9525\n",
      "Epoch 93/500\n",
      "948/948 [==============================] - 0s - loss: 143.3857 - acc: 0.9662 - val_loss: 65.0442 - val_acc: 0.9525\n",
      "Epoch 94/500\n",
      "948/948 [==============================] - 0s - loss: 141.0527 - acc: 0.9568 - val_loss: 66.4974 - val_acc: 0.9525\n",
      "Epoch 95/500\n",
      "948/948 [==============================] - 0s - loss: 147.3361 - acc: 0.9715 - val_loss: 63.5501 - val_acc: 0.9589\n",
      "Epoch 96/500\n",
      "948/948 [==============================] - ETA: 0s - loss: 145.7916 - acc: 0.957 - 0s - loss: 141.2443 - acc: 0.9610 - val_loss: 67.3821 - val_acc: 0.9525\n",
      "Epoch 97/500\n",
      "948/948 [==============================] - 0s - loss: 130.0654 - acc: 0.9620 - val_loss: 63.4953 - val_acc: 0.9525\n",
      "Epoch 98/500\n",
      "948/948 [==============================] - 0s - loss: 136.4461 - acc: 0.9620 - val_loss: 64.3952 - val_acc: 0.9525\n",
      "Epoch 99/500\n",
      "948/948 [==============================] - 0s - loss: 138.5184 - acc: 0.9578 - val_loss: 63.3381 - val_acc: 0.9525\n",
      "Epoch 100/500\n",
      "948/948 [==============================] - 0s - loss: 133.0900 - acc: 0.9673 - val_loss: 64.2126 - val_acc: 0.9525\n",
      "Epoch 101/500\n",
      "948/948 [==============================] - 0s - loss: 131.6397 - acc: 0.9620 - val_loss: 62.5654 - val_acc: 0.9525\n",
      "Epoch 102/500\n",
      "948/948 [==============================] - 0s - loss: 141.4504 - acc: 0.9694 - val_loss: 63.8008 - val_acc: 0.9525\n",
      "Epoch 103/500\n",
      "948/948 [==============================] - 0s - loss: 144.0325 - acc: 0.9652 - val_loss: 65.2471 - val_acc: 0.9525\n",
      "Epoch 104/500\n",
      "948/948 [==============================] - 0s - loss: 134.3121 - acc: 0.9694 - val_loss: 63.5425 - val_acc: 0.9525\n",
      "Epoch 105/500\n",
      "948/948 [==============================] - 0s - loss: 136.7617 - acc: 0.9684 - val_loss: 61.8574 - val_acc: 0.9525\n",
      "Epoch 106/500\n",
      "948/948 [==============================] - 0s - loss: 138.4282 - acc: 0.9673 - val_loss: 61.9474 - val_acc: 0.9525\n",
      "Epoch 107/500\n",
      "948/948 [==============================] - 0s - loss: 138.2153 - acc: 0.9705 - val_loss: 64.1736 - val_acc: 0.9525\n",
      "Epoch 108/500\n",
      "948/948 [==============================] - 0s - loss: 136.8903 - acc: 0.9684 - val_loss: 61.7199 - val_acc: 0.9525\n",
      "Epoch 109/500\n",
      "948/948 [==============================] - 0s - loss: 142.9305 - acc: 0.9684 - val_loss: 66.2875 - val_acc: 0.9525\n",
      "Epoch 110/500\n",
      "948/948 [==============================] - 0s - loss: 138.2027 - acc: 0.9662 - val_loss: 63.1154 - val_acc: 0.9589\n",
      "Epoch 111/500\n",
      "948/948 [==============================] - 0s - loss: 134.6228 - acc: 0.9641 - val_loss: 63.2203 - val_acc: 0.9589\n",
      "Epoch 112/500\n",
      "948/948 [==============================] - 0s - loss: 130.9381 - acc: 0.9673 - val_loss: 64.9565 - val_acc: 0.9525\n",
      "Epoch 113/500\n",
      "948/948 [==============================] - 0s - loss: 145.5722 - acc: 0.9610 - val_loss: 65.4464 - val_acc: 0.9525\n",
      "Epoch 114/500\n",
      "948/948 [==============================] - 0s - loss: 135.7038 - acc: 0.9652 - val_loss: 65.2628 - val_acc: 0.9525\n",
      "Epoch 115/500\n",
      "948/948 [==============================] - 0s - loss: 132.0854 - acc: 0.9631 - val_loss: 63.4952 - val_acc: 0.9589\n",
      "Epoch 116/500\n",
      "948/948 [==============================] - 0s - loss: 130.1246 - acc: 0.9705 - val_loss: 63.6800 - val_acc: 0.9525\n",
      "Epoch 117/500\n",
      "948/948 [==============================] - 0s - loss: 136.1644 - acc: 0.9684 - val_loss: 64.3440 - val_acc: 0.9557\n",
      "Epoch 118/500\n",
      "948/948 [==============================] - 0s - loss: 125.5376 - acc: 0.9673 - val_loss: 62.3114 - val_acc: 0.9494\n",
      "Epoch 119/500\n",
      "948/948 [==============================] - 0s - loss: 137.9262 - acc: 0.9684 - val_loss: 62.6802 - val_acc: 0.9525\n",
      "Epoch 120/500\n",
      "948/948 [==============================] - 0s - loss: 129.6036 - acc: 0.9747 - val_loss: 61.7287 - val_acc: 0.9525\n",
      "Epoch 121/500\n",
      "948/948 [==============================] - 0s - loss: 142.7741 - acc: 0.9641 - val_loss: 63.3475 - val_acc: 0.9525\n",
      "Epoch 122/500\n",
      "948/948 [==============================] - 0s - loss: 134.9982 - acc: 0.9715 - val_loss: 68.8082 - val_acc: 0.9525\n",
      "Epoch 123/500\n",
      "948/948 [==============================] - 0s - loss: 133.4476 - acc: 0.9694 - val_loss: 63.5167 - val_acc: 0.9525\n",
      "Epoch 124/500\n",
      "948/948 [==============================] - 0s - loss: 136.2399 - acc: 0.9641 - val_loss: 61.1041 - val_acc: 0.9525\n",
      "Epoch 125/500\n",
      "948/948 [==============================] - 0s - loss: 128.2249 - acc: 0.9684 - val_loss: 59.8653 - val_acc: 0.9589\n",
      "Epoch 126/500\n",
      "948/948 [==============================] - 0s - loss: 142.5982 - acc: 0.9778 - val_loss: 61.1420 - val_acc: 0.9589\n",
      "Epoch 127/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 129.8482 - acc: 0.9694 - val_loss: 63.1754 - val_acc: 0.9557\n",
      "Epoch 128/500\n",
      "948/948 [==============================] - 0s - loss: 135.2481 - acc: 0.9652 - val_loss: 61.4648 - val_acc: 0.9557\n",
      "Epoch 129/500\n",
      "948/948 [==============================] - 0s - loss: 135.2965 - acc: 0.9705 - val_loss: 60.4882 - val_acc: 0.9557\n",
      "Epoch 130/500\n",
      "948/948 [==============================] - 0s - loss: 125.6938 - acc: 0.9673 - val_loss: 63.6677 - val_acc: 0.9589\n",
      "Epoch 131/500\n",
      "948/948 [==============================] - 0s - loss: 124.1869 - acc: 0.9673 - val_loss: 61.4164 - val_acc: 0.9589\n",
      "Epoch 132/500\n",
      "948/948 [==============================] - 0s - loss: 124.2938 - acc: 0.9736 - val_loss: 61.8671 - val_acc: 0.9589\n",
      "Epoch 133/500\n",
      "948/948 [==============================] - 0s - loss: 129.2165 - acc: 0.9589 - val_loss: 62.8396 - val_acc: 0.9589\n",
      "Epoch 134/500\n",
      "948/948 [==============================] - 0s - loss: 118.9821 - acc: 0.9705 - val_loss: 63.1981 - val_acc: 0.9589\n",
      "Epoch 135/500\n",
      "948/948 [==============================] - 0s - loss: 126.2240 - acc: 0.9662 - val_loss: 64.1214 - val_acc: 0.9589\n",
      "Epoch 136/500\n",
      "948/948 [==============================] - 0s - loss: 123.8364 - acc: 0.9747 - val_loss: 60.7101 - val_acc: 0.9589\n",
      "Epoch 137/500\n",
      "948/948 [==============================] - 0s - loss: 131.2869 - acc: 0.9694 - val_loss: 59.5475 - val_acc: 0.9589\n",
      "Epoch 138/500\n",
      "948/948 [==============================] - 0s - loss: 126.1607 - acc: 0.9694 - val_loss: 61.0247 - val_acc: 0.9525\n",
      "Epoch 139/500\n",
      "948/948 [==============================] - 0s - loss: 125.8863 - acc: 0.9726 - val_loss: 58.8854 - val_acc: 0.9589\n",
      "Epoch 140/500\n",
      "948/948 [==============================] - 0s - loss: 130.7385 - acc: 0.9673 - val_loss: 57.5421 - val_acc: 0.9525\n",
      "Epoch 141/500\n",
      "948/948 [==============================] - 0s - loss: 128.5073 - acc: 0.9715 - val_loss: 62.0278 - val_acc: 0.9589\n",
      "Epoch 142/500\n",
      "948/948 [==============================] - 0s - loss: 131.3196 - acc: 0.9715 - val_loss: 60.6675 - val_acc: 0.9557\n",
      "Epoch 143/500\n",
      "948/948 [==============================] - 0s - loss: 122.2047 - acc: 0.9705 - val_loss: 61.0781 - val_acc: 0.9557\n",
      "Epoch 144/500\n",
      "948/948 [==============================] - 0s - loss: 118.3852 - acc: 0.9757 - val_loss: 64.5579 - val_acc: 0.9557\n",
      "Epoch 145/500\n",
      "948/948 [==============================] - 0s - loss: 115.8611 - acc: 0.9684 - val_loss: 61.5970 - val_acc: 0.9589\n",
      "Epoch 146/500\n",
      "948/948 [==============================] - 0s - loss: 116.0156 - acc: 0.9694 - val_loss: 59.6496 - val_acc: 0.9557\n",
      "Epoch 147/500\n",
      "948/948 [==============================] - 0s - loss: 128.6649 - acc: 0.9778 - val_loss: 58.0974 - val_acc: 0.9557\n",
      "Epoch 148/500\n",
      "948/948 [==============================] - 0s - loss: 122.8161 - acc: 0.9726 - val_loss: 57.1875 - val_acc: 0.9525\n",
      "Epoch 149/500\n",
      "948/948 [==============================] - 0s - loss: 134.8150 - acc: 0.9747 - val_loss: 58.8844 - val_acc: 0.9525\n",
      "Epoch 150/500\n",
      "948/948 [==============================] - 0s - loss: 115.9892 - acc: 0.9747 - val_loss: 58.9250 - val_acc: 0.9525\n",
      "Epoch 151/500\n",
      "948/948 [==============================] - 0s - loss: 126.3726 - acc: 0.9726 - val_loss: 59.8175 - val_acc: 0.9525\n",
      "Epoch 152/500\n",
      "948/948 [==============================] - 0s - loss: 131.7145 - acc: 0.9673 - val_loss: 62.1001 - val_acc: 0.9557\n",
      "Epoch 153/500\n",
      "948/948 [==============================] - 0s - loss: 128.9588 - acc: 0.9694 - val_loss: 58.4923 - val_acc: 0.9557\n",
      "Epoch 154/500\n",
      "948/948 [==============================] - 0s - loss: 121.1102 - acc: 0.9747 - val_loss: 59.6789 - val_acc: 0.9525\n",
      "Epoch 155/500\n",
      "948/948 [==============================] - 0s - loss: 121.8334 - acc: 0.9705 - val_loss: 61.5168 - val_acc: 0.9557\n",
      "Epoch 156/500\n",
      "948/948 [==============================] - 0s - loss: 124.2692 - acc: 0.9757 - val_loss: 59.1474 - val_acc: 0.9525\n",
      "Epoch 157/500\n",
      "948/948 [==============================] - 0s - loss: 112.8581 - acc: 0.9768 - val_loss: 59.0903 - val_acc: 0.9557\n",
      "Epoch 158/500\n",
      "948/948 [==============================] - 0s - loss: 124.9424 - acc: 0.9694 - val_loss: 63.0759 - val_acc: 0.9557\n",
      "Epoch 159/500\n",
      "948/948 [==============================] - 0s - loss: 125.5276 - acc: 0.9694 - val_loss: 63.1629 - val_acc: 0.9557\n",
      "Epoch 160/500\n",
      "948/948 [==============================] - 0s - loss: 120.1603 - acc: 0.9715 - val_loss: 61.5218 - val_acc: 0.9589\n",
      "Epoch 161/500\n",
      "948/948 [==============================] - 0s - loss: 111.1343 - acc: 0.9715 - val_loss: 63.0009 - val_acc: 0.9557\n",
      "Epoch 162/500\n",
      "948/948 [==============================] - 0s - loss: 116.6419 - acc: 0.9715 - val_loss: 58.9376 - val_acc: 0.9525\n",
      "Epoch 163/500\n",
      "948/948 [==============================] - 0s - loss: 121.7468 - acc: 0.9747 - val_loss: 58.2228 - val_acc: 0.9557\n",
      "Epoch 164/500\n",
      "948/948 [==============================] - 0s - loss: 127.9610 - acc: 0.9673 - val_loss: 59.1961 - val_acc: 0.9557\n",
      "Epoch 165/500\n",
      "948/948 [==============================] - 0s - loss: 119.2018 - acc: 0.9747 - val_loss: 61.6678 - val_acc: 0.9525\n",
      "Epoch 166/500\n",
      "948/948 [==============================] - 0s - loss: 107.7057 - acc: 0.9747 - val_loss: 60.0683 - val_acc: 0.9494\n",
      "Epoch 167/500\n",
      "948/948 [==============================] - 0s - loss: 121.2348 - acc: 0.9747 - val_loss: 58.8632 - val_acc: 0.9557\n",
      "Epoch 168/500\n",
      "948/948 [==============================] - 0s - loss: 125.1584 - acc: 0.9747 - val_loss: 62.1064 - val_acc: 0.9557\n",
      "Epoch 169/500\n",
      "948/948 [==============================] - 0s - loss: 110.4169 - acc: 0.9673 - val_loss: 61.0534 - val_acc: 0.9525\n",
      "Epoch 170/500\n",
      "948/948 [==============================] - 0s - loss: 121.3479 - acc: 0.9684 - val_loss: 61.0113 - val_acc: 0.9557\n",
      "Epoch 171/500\n",
      "948/948 [==============================] - 0s - loss: 118.5169 - acc: 0.9715 - val_loss: 60.5237 - val_acc: 0.9494\n",
      "Epoch 172/500\n",
      "948/948 [==============================] - 0s - loss: 117.8594 - acc: 0.9652 - val_loss: 60.3874 - val_acc: 0.9557\n",
      "Epoch 173/500\n",
      "948/948 [==============================] - 0s - loss: 119.1028 - acc: 0.9694 - val_loss: 59.3933 - val_acc: 0.9557\n",
      "Epoch 174/500\n",
      "948/948 [==============================] - 0s - loss: 112.9225 - acc: 0.9736 - val_loss: 58.7236 - val_acc: 0.9557\n",
      "Epoch 175/500\n",
      "948/948 [==============================] - 0s - loss: 118.4671 - acc: 0.9673 - val_loss: 60.0384 - val_acc: 0.9589\n",
      "Epoch 176/500\n",
      "948/948 [==============================] - 0s - loss: 121.8501 - acc: 0.9705 - val_loss: 59.0815 - val_acc: 0.9494\n",
      "Epoch 177/500\n",
      "948/948 [==============================] - 0s - loss: 120.2498 - acc: 0.9662 - val_loss: 61.9507 - val_acc: 0.9494\n",
      "Epoch 178/500\n",
      "948/948 [==============================] - 0s - loss: 114.4595 - acc: 0.9715 - val_loss: 60.8108 - val_acc: 0.9494\n",
      "Epoch 179/500\n",
      "948/948 [==============================] - 0s - loss: 115.7318 - acc: 0.9747 - val_loss: 60.5345 - val_acc: 0.9494\n",
      "Epoch 180/500\n",
      "948/948 [==============================] - 0s - loss: 120.0938 - acc: 0.9673 - val_loss: 59.3991 - val_acc: 0.9494\n",
      "Epoch 181/500\n",
      "948/948 [==============================] - 0s - loss: 117.3291 - acc: 0.9694 - val_loss: 61.3651 - val_acc: 0.9525\n",
      "Epoch 182/500\n",
      "948/948 [==============================] - 0s - loss: 116.1533 - acc: 0.9715 - val_loss: 60.2933 - val_acc: 0.9557\n",
      "Epoch 183/500\n",
      "948/948 [==============================] - 0s - loss: 107.7283 - acc: 0.9726 - val_loss: 60.4424 - val_acc: 0.9494\n",
      "Epoch 184/500\n",
      "948/948 [==============================] - 0s - loss: 116.4662 - acc: 0.9662 - val_loss: 60.3742 - val_acc: 0.9494\n",
      "Epoch 185/500\n",
      "948/948 [==============================] - 0s - loss: 115.9781 - acc: 0.9705 - val_loss: 61.0674 - val_acc: 0.9494\n",
      "Epoch 186/500\n",
      "948/948 [==============================] - 0s - loss: 122.3090 - acc: 0.9694 - val_loss: 58.1614 - val_acc: 0.9525\n",
      "Epoch 187/500\n",
      "948/948 [==============================] - 0s - loss: 113.2498 - acc: 0.9694 - val_loss: 57.8284 - val_acc: 0.9557\n",
      "Epoch 188/500\n",
      "948/948 [==============================] - 0s - loss: 106.8450 - acc: 0.9736 - val_loss: 57.9865 - val_acc: 0.9525\n",
      "Epoch 189/500\n",
      "948/948 [==============================] - 0s - loss: 124.6647 - acc: 0.9715 - val_loss: 59.9181 - val_acc: 0.9557\n",
      "Epoch 190/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 124.9798 - acc: 0.9768 - val_loss: 59.3616 - val_acc: 0.9525\n",
      "Epoch 191/500\n",
      "948/948 [==============================] - 0s - loss: 118.1002 - acc: 0.9736 - val_loss: 58.6316 - val_acc: 0.9525\n",
      "Epoch 192/500\n",
      "948/948 [==============================] - 0s - loss: 108.4422 - acc: 0.9641 - val_loss: 60.5217 - val_acc: 0.9525\n",
      "Epoch 193/500\n",
      "948/948 [==============================] - 0s - loss: 112.8020 - acc: 0.9778 - val_loss: 60.7847 - val_acc: 0.9525\n",
      "Epoch 194/500\n",
      "948/948 [==============================] - 0s - loss: 114.2784 - acc: 0.9694 - val_loss: 61.7491 - val_acc: 0.9557\n",
      "Epoch 195/500\n",
      "948/948 [==============================] - 0s - loss: 115.8027 - acc: 0.9705 - val_loss: 57.9623 - val_acc: 0.9525\n",
      "Epoch 196/500\n",
      "948/948 [==============================] - 0s - loss: 128.6765 - acc: 0.9705 - val_loss: 59.8222 - val_acc: 0.9525\n",
      "Epoch 197/500\n",
      "948/948 [==============================] - 0s - loss: 105.3581 - acc: 0.9768 - val_loss: 58.5185 - val_acc: 0.9494\n",
      "Epoch 198/500\n",
      "948/948 [==============================] - 0s - loss: 111.5497 - acc: 0.9736 - val_loss: 58.7497 - val_acc: 0.9525\n",
      "Epoch 199/500\n",
      "948/948 [==============================] - 0s - loss: 114.9186 - acc: 0.9778 - val_loss: 61.0102 - val_acc: 0.9525\n",
      "Epoch 200/500\n",
      "948/948 [==============================] - 0s - loss: 118.3949 - acc: 0.9726 - val_loss: 60.5195 - val_acc: 0.9494\n",
      "Epoch 201/500\n",
      "948/948 [==============================] - 0s - loss: 116.0140 - acc: 0.9673 - val_loss: 64.0249 - val_acc: 0.9494\n",
      "Epoch 202/500\n",
      "948/948 [==============================] - 0s - loss: 119.9267 - acc: 0.9747 - val_loss: 57.7169 - val_acc: 0.9494\n",
      "Epoch 203/500\n",
      "948/948 [==============================] - 0s - loss: 115.6758 - acc: 0.9726 - val_loss: 57.1161 - val_acc: 0.9525\n",
      "Epoch 204/500\n",
      "948/948 [==============================] - 0s - loss: 108.0157 - acc: 0.9768 - val_loss: 58.6583 - val_acc: 0.9557\n",
      "Epoch 205/500\n",
      "948/948 [==============================] - 0s - loss: 115.2287 - acc: 0.9747 - val_loss: 59.2953 - val_acc: 0.9494\n",
      "Epoch 206/500\n",
      "948/948 [==============================] - 0s - loss: 109.5831 - acc: 0.9726 - val_loss: 59.7543 - val_acc: 0.9494\n",
      "Epoch 207/500\n",
      "948/948 [==============================] - 0s - loss: 116.3836 - acc: 0.9736 - val_loss: 59.7621 - val_acc: 0.9494\n",
      "Epoch 208/500\n",
      "948/948 [==============================] - 0s - loss: 107.7796 - acc: 0.9789 - val_loss: 62.9479 - val_acc: 0.9494\n",
      "Epoch 209/500\n",
      "948/948 [==============================] - 0s - loss: 112.3321 - acc: 0.9789 - val_loss: 59.8519 - val_acc: 0.9494\n",
      "Epoch 210/500\n",
      "948/948 [==============================] - 0s - loss: 108.5705 - acc: 0.9747 - val_loss: 63.4623 - val_acc: 0.9462\n",
      "Epoch 211/500\n",
      "948/948 [==============================] - 0s - loss: 108.0201 - acc: 0.9694 - val_loss: 59.6329 - val_acc: 0.9525\n",
      "Epoch 212/500\n",
      "948/948 [==============================] - 0s - loss: 105.1254 - acc: 0.9768 - val_loss: 62.7630 - val_acc: 0.9462\n",
      "Epoch 213/500\n",
      "948/948 [==============================] - 0s - loss: 104.0352 - acc: 0.9705 - val_loss: 59.0446 - val_acc: 0.9494\n",
      "Epoch 214/500\n",
      "948/948 [==============================] - 0s - loss: 109.1677 - acc: 0.9736 - val_loss: 58.2619 - val_acc: 0.9525\n",
      "Epoch 215/500\n",
      "948/948 [==============================] - 0s - loss: 114.2409 - acc: 0.9726 - val_loss: 57.8421 - val_acc: 0.9525\n",
      "Epoch 216/500\n",
      "948/948 [==============================] - 0s - loss: 111.8777 - acc: 0.9757 - val_loss: 60.4590 - val_acc: 0.9494\n",
      "Epoch 217/500\n",
      "948/948 [==============================] - 0s - loss: 104.6054 - acc: 0.9747 - val_loss: 60.2833 - val_acc: 0.9494\n",
      "Epoch 218/500\n",
      "948/948 [==============================] - 0s - loss: 114.1664 - acc: 0.9778 - val_loss: 58.7902 - val_acc: 0.9525\n",
      "Epoch 219/500\n",
      "948/948 [==============================] - 0s - loss: 110.9815 - acc: 0.9705 - val_loss: 59.9514 - val_acc: 0.9525\n",
      "Epoch 220/500\n",
      "948/948 [==============================] - 0s - loss: 107.9572 - acc: 0.9694 - val_loss: 60.4060 - val_acc: 0.9525\n",
      "Epoch 221/500\n",
      "948/948 [==============================] - 0s - loss: 106.8510 - acc: 0.9705 - val_loss: 58.9149 - val_acc: 0.9525\n",
      "Epoch 222/500\n",
      "948/948 [==============================] - 0s - loss: 111.9984 - acc: 0.9684 - val_loss: 59.9845 - val_acc: 0.9494\n",
      "Epoch 223/500\n",
      "948/948 [==============================] - 0s - loss: 113.3237 - acc: 0.9694 - val_loss: 60.1659 - val_acc: 0.9494\n",
      "Epoch 224/500\n",
      "948/948 [==============================] - 0s - loss: 100.3994 - acc: 0.9747 - val_loss: 58.7293 - val_acc: 0.9525\n",
      "Epoch 225/500\n",
      "948/948 [==============================] - 0s - loss: 110.0732 - acc: 0.9662 - val_loss: 60.3220 - val_acc: 0.9525\n",
      "Epoch 226/500\n",
      "948/948 [==============================] - 0s - loss: 112.1669 - acc: 0.9747 - val_loss: 57.7647 - val_acc: 0.9494\n",
      "Epoch 227/500\n",
      "948/948 [==============================] - 0s - loss: 104.2148 - acc: 0.9747 - val_loss: 57.4312 - val_acc: 0.9462\n",
      "Epoch 228/500\n",
      "948/948 [==============================] - 0s - loss: 106.2261 - acc: 0.9757 - val_loss: 57.0819 - val_acc: 0.9462\n",
      "Epoch 229/500\n",
      "948/948 [==============================] - 0s - loss: 106.5798 - acc: 0.9736 - val_loss: 59.0635 - val_acc: 0.9430\n",
      "Epoch 230/500\n",
      "948/948 [==============================] - 0s - loss: 106.1326 - acc: 0.9747 - val_loss: 57.6023 - val_acc: 0.9494\n",
      "Epoch 231/500\n",
      "948/948 [==============================] - 0s - loss: 109.1750 - acc: 0.9800 - val_loss: 58.9800 - val_acc: 0.9462\n",
      "Epoch 232/500\n",
      "948/948 [==============================] - 0s - loss: 106.8038 - acc: 0.9736 - val_loss: 59.9742 - val_acc: 0.9494\n",
      "Epoch 233/500\n",
      "948/948 [==============================] - 0s - loss: 108.1878 - acc: 0.9757 - val_loss: 58.5638 - val_acc: 0.9525\n",
      "Epoch 234/500\n",
      "948/948 [==============================] - 0s - loss: 106.1133 - acc: 0.9694 - val_loss: 59.3098 - val_acc: 0.9525\n",
      "Epoch 235/500\n",
      "948/948 [==============================] - 0s - loss: 109.1906 - acc: 0.9705 - val_loss: 61.3121 - val_acc: 0.9494\n",
      "Epoch 236/500\n",
      "948/948 [==============================] - 0s - loss: 103.2844 - acc: 0.9768 - val_loss: 60.6177 - val_acc: 0.9525\n",
      "Epoch 237/500\n",
      "948/948 [==============================] - 0s - loss: 105.1284 - acc: 0.9715 - val_loss: 56.8243 - val_acc: 0.9462\n",
      "Epoch 238/500\n",
      "948/948 [==============================] - 0s - loss: 105.7594 - acc: 0.9800 - val_loss: 56.4965 - val_acc: 0.9430\n",
      "Epoch 239/500\n",
      "948/948 [==============================] - 0s - loss: 110.8176 - acc: 0.9694 - val_loss: 56.6975 - val_acc: 0.9430\n",
      "Epoch 240/500\n",
      "948/948 [==============================] - 0s - loss: 104.6308 - acc: 0.9736 - val_loss: 56.2585 - val_acc: 0.9525\n",
      "Epoch 241/500\n",
      "948/948 [==============================] - 0s - loss: 104.2467 - acc: 0.9778 - val_loss: 58.0973 - val_acc: 0.9494\n",
      "Epoch 242/500\n",
      "948/948 [==============================] - 0s - loss: 99.2618 - acc: 0.9736 - val_loss: 58.3477 - val_acc: 0.9494\n",
      "Epoch 243/500\n",
      "948/948 [==============================] - 0s - loss: 99.4651 - acc: 0.9736 - val_loss: 58.5467 - val_acc: 0.9494\n",
      "Epoch 244/500\n",
      "948/948 [==============================] - 0s - loss: 106.5404 - acc: 0.9747 - val_loss: 58.0000 - val_acc: 0.9494\n",
      "Epoch 245/500\n",
      "948/948 [==============================] - 0s - loss: 109.3763 - acc: 0.9736 - val_loss: 58.3475 - val_acc: 0.9494\n",
      "Epoch 246/500\n",
      "948/948 [==============================] - 0s - loss: 100.5404 - acc: 0.9747 - val_loss: 57.1553 - val_acc: 0.9494\n",
      "Epoch 247/500\n",
      "948/948 [==============================] - 0s - loss: 98.4836 - acc: 0.9747 - val_loss: 56.4169 - val_acc: 0.9494\n",
      "Epoch 248/500\n",
      "948/948 [==============================] - 0s - loss: 104.0577 - acc: 0.9768 - val_loss: 58.0001 - val_acc: 0.9494\n",
      "Epoch 249/500\n",
      "948/948 [==============================] - 0s - loss: 113.2680 - acc: 0.9757 - val_loss: 59.6527 - val_acc: 0.9525\n",
      "Epoch 250/500\n",
      "948/948 [==============================] - 0s - loss: 99.6995 - acc: 0.9694 - val_loss: 59.0092 - val_acc: 0.9494\n",
      "Epoch 251/500\n",
      "948/948 [==============================] - 0s - loss: 108.8273 - acc: 0.9715 - val_loss: 58.0159 - val_acc: 0.9525\n",
      "Epoch 252/500\n",
      "948/948 [==============================] - 0s - loss: 103.4229 - acc: 0.9736 - val_loss: 58.0824 - val_acc: 0.9494\n",
      "Epoch 253/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 100.5621 - acc: 0.9789 - val_loss: 59.0453 - val_acc: 0.9494\n",
      "Epoch 254/500\n",
      "948/948 [==============================] - 0s - loss: 107.6154 - acc: 0.9705 - val_loss: 59.2722 - val_acc: 0.9494\n",
      "Epoch 255/500\n",
      "948/948 [==============================] - 0s - loss: 109.5540 - acc: 0.9726 - val_loss: 59.9458 - val_acc: 0.9494\n",
      "Epoch 256/500\n",
      "948/948 [==============================] - 0s - loss: 117.4239 - acc: 0.9715 - val_loss: 57.9557 - val_acc: 0.9462\n",
      "Epoch 257/500\n",
      "948/948 [==============================] - 0s - loss: 107.2348 - acc: 0.9736 - val_loss: 59.6306 - val_acc: 0.9494\n",
      "Epoch 258/500\n",
      "948/948 [==============================] - 0s - loss: 102.8763 - acc: 0.9768 - val_loss: 62.8506 - val_acc: 0.9462\n",
      "Epoch 259/500\n",
      "948/948 [==============================] - 0s - loss: 109.3042 - acc: 0.9641 - val_loss: 59.6759 - val_acc: 0.9462\n",
      "Epoch 260/500\n",
      "948/948 [==============================] - 0s - loss: 100.4994 - acc: 0.9705 - val_loss: 63.9510 - val_acc: 0.9462\n",
      "Epoch 261/500\n",
      "948/948 [==============================] - 0s - loss: 101.7291 - acc: 0.9705 - val_loss: 61.9133 - val_acc: 0.9462\n",
      "Epoch 262/500\n",
      "948/948 [==============================] - 0s - loss: 99.7591 - acc: 0.9757 - val_loss: 60.6606 - val_acc: 0.9430\n",
      "Epoch 263/500\n",
      "948/948 [==============================] - 0s - loss: 98.9686 - acc: 0.9789 - val_loss: 59.2956 - val_acc: 0.9494\n",
      "Epoch 264/500\n",
      "948/948 [==============================] - 0s - loss: 106.1790 - acc: 0.9821 - val_loss: 59.0189 - val_acc: 0.9430\n",
      "Epoch 265/500\n",
      "948/948 [==============================] - 0s - loss: 107.4049 - acc: 0.9768 - val_loss: 57.4723 - val_acc: 0.9430\n",
      "Epoch 266/500\n",
      "948/948 [==============================] - 0s - loss: 103.5547 - acc: 0.9747 - val_loss: 57.9277 - val_acc: 0.9525\n",
      "Epoch 267/500\n",
      "948/948 [==============================] - 0s - loss: 112.8461 - acc: 0.9789 - val_loss: 59.4851 - val_acc: 0.9494\n",
      "Epoch 268/500\n",
      "948/948 [==============================] - 0s - loss: 109.1331 - acc: 0.9705 - val_loss: 58.6468 - val_acc: 0.9494\n",
      "Epoch 269/500\n",
      "948/948 [==============================] - 0s - loss: 101.6334 - acc: 0.9705 - val_loss: 59.4240 - val_acc: 0.9494\n",
      "Epoch 270/500\n",
      "948/948 [==============================] - 0s - loss: 104.7656 - acc: 0.9705 - val_loss: 58.7765 - val_acc: 0.9494\n",
      "Epoch 271/500\n",
      "948/948 [==============================] - 0s - loss: 110.8958 - acc: 0.9736 - val_loss: 58.0486 - val_acc: 0.9494\n",
      "Epoch 272/500\n",
      "948/948 [==============================] - 0s - loss: 98.9191 - acc: 0.9726 - val_loss: 60.8465 - val_acc: 0.9494\n",
      "Epoch 273/500\n",
      "948/948 [==============================] - 0s - loss: 105.6000 - acc: 0.9747 - val_loss: 58.4355 - val_acc: 0.9494\n",
      "Epoch 274/500\n",
      "948/948 [==============================] - 0s - loss: 98.9300 - acc: 0.9705 - val_loss: 58.0840 - val_acc: 0.9494\n",
      "Epoch 275/500\n",
      "948/948 [==============================] - 0s - loss: 105.2948 - acc: 0.9757 - val_loss: 58.7351 - val_acc: 0.9494\n",
      "Epoch 276/500\n",
      "948/948 [==============================] - 0s - loss: 101.7072 - acc: 0.9757 - val_loss: 57.7297 - val_acc: 0.9525\n",
      "Epoch 277/500\n",
      "948/948 [==============================] - 0s - loss: 104.7150 - acc: 0.9778 - val_loss: 58.2530 - val_acc: 0.9494\n",
      "Epoch 278/500\n",
      "948/948 [==============================] - 0s - loss: 105.0290 - acc: 0.9747 - val_loss: 57.3030 - val_acc: 0.9494\n",
      "Epoch 279/500\n",
      "948/948 [==============================] - 0s - loss: 98.9194 - acc: 0.9768 - val_loss: 59.8306 - val_acc: 0.9462\n",
      "Epoch 280/500\n",
      "948/948 [==============================] - 0s - loss: 102.5665 - acc: 0.9705 - val_loss: 61.0841 - val_acc: 0.9494\n",
      "Epoch 281/500\n",
      "948/948 [==============================] - 0s - loss: 97.2245 - acc: 0.9757 - val_loss: 62.2430 - val_acc: 0.9494\n",
      "Epoch 282/500\n",
      "948/948 [==============================] - 0s - loss: 92.7921 - acc: 0.9768 - val_loss: 62.5055 - val_acc: 0.9462\n",
      "Epoch 283/500\n",
      "948/948 [==============================] - 0s - loss: 97.4049 - acc: 0.9673 - val_loss: 59.6164 - val_acc: 0.9494\n",
      "Epoch 284/500\n",
      "948/948 [==============================] - 0s - loss: 101.0933 - acc: 0.9768 - val_loss: 60.5600 - val_acc: 0.9494\n",
      "Epoch 285/500\n",
      "948/948 [==============================] - 0s - loss: 101.5646 - acc: 0.9747 - val_loss: 58.2988 - val_acc: 0.9494\n",
      "Epoch 286/500\n",
      "948/948 [==============================] - 0s - loss: 99.9164 - acc: 0.9726 - val_loss: 60.5652 - val_acc: 0.9494\n",
      "Epoch 287/500\n",
      "948/948 [==============================] - 0s - loss: 101.5399 - acc: 0.9757 - val_loss: 58.5736 - val_acc: 0.9462\n",
      "Epoch 288/500\n",
      "948/948 [==============================] - 0s - loss: 94.4822 - acc: 0.9768 - val_loss: 58.6272 - val_acc: 0.9494\n",
      "Epoch 289/500\n",
      "948/948 [==============================] - 0s - loss: 95.5933 - acc: 0.9778 - val_loss: 59.3486 - val_acc: 0.9494\n",
      "Epoch 290/500\n",
      "948/948 [==============================] - 0s - loss: 93.5950 - acc: 0.9821 - val_loss: 59.9087 - val_acc: 0.9494\n",
      "Epoch 291/500\n",
      "948/948 [==============================] - 0s - loss: 98.4996 - acc: 0.9652 - val_loss: 61.2929 - val_acc: 0.9494\n",
      "Epoch 292/500\n",
      "948/948 [==============================] - 0s - loss: 100.5501 - acc: 0.9705 - val_loss: 60.5440 - val_acc: 0.9462\n",
      "Epoch 293/500\n",
      "948/948 [==============================] - 0s - loss: 91.9034 - acc: 0.9768 - val_loss: 58.9204 - val_acc: 0.9462\n",
      "Epoch 294/500\n",
      "948/948 [==============================] - 0s - loss: 106.9378 - acc: 0.9747 - val_loss: 58.2796 - val_acc: 0.9462\n",
      "Epoch 295/500\n",
      "948/948 [==============================] - 0s - loss: 98.8548 - acc: 0.9800 - val_loss: 59.0140 - val_acc: 0.9462\n",
      "Epoch 296/500\n",
      "948/948 [==============================] - 0s - loss: 94.7555 - acc: 0.9684 - val_loss: 59.9555 - val_acc: 0.9462\n",
      "Epoch 297/500\n",
      "948/948 [==============================] - 0s - loss: 95.6502 - acc: 0.9789 - val_loss: 60.5674 - val_acc: 0.9462\n",
      "Epoch 298/500\n",
      "948/948 [==============================] - 0s - loss: 105.0089 - acc: 0.9705 - val_loss: 59.9282 - val_acc: 0.9462\n",
      "Epoch 299/500\n",
      "948/948 [==============================] - 0s - loss: 96.2877 - acc: 0.9652 - val_loss: 60.0737 - val_acc: 0.9494\n",
      "Epoch 300/500\n",
      "948/948 [==============================] - 0s - loss: 91.8675 - acc: 0.9705 - val_loss: 59.0071 - val_acc: 0.9462\n",
      "Epoch 301/500\n",
      "948/948 [==============================] - 0s - loss: 89.3153 - acc: 0.9726 - val_loss: 57.8437 - val_acc: 0.9430\n",
      "Epoch 302/500\n",
      "948/948 [==============================] - 0s - loss: 99.0504 - acc: 0.9715 - val_loss: 58.9200 - val_acc: 0.9462\n",
      "Epoch 303/500\n",
      "948/948 [==============================] - 0s - loss: 100.6835 - acc: 0.9736 - val_loss: 57.0139 - val_acc: 0.9430\n",
      "Epoch 304/500\n",
      "948/948 [==============================] - 0s - loss: 91.5306 - acc: 0.9715 - val_loss: 58.2347 - val_acc: 0.9462\n",
      "Epoch 305/500\n",
      "948/948 [==============================] - 0s - loss: 103.2013 - acc: 0.9768 - val_loss: 59.0052 - val_acc: 0.9494\n",
      "Epoch 306/500\n",
      "948/948 [==============================] - 0s - loss: 92.3930 - acc: 0.9821 - val_loss: 59.3352 - val_acc: 0.9399\n",
      "Epoch 307/500\n",
      "948/948 [==============================] - 0s - loss: 88.6026 - acc: 0.9768 - val_loss: 62.0241 - val_acc: 0.9462\n",
      "Epoch 308/500\n",
      "948/948 [==============================] - 0s - loss: 96.6330 - acc: 0.9789 - val_loss: 60.0943 - val_acc: 0.9462\n",
      "Epoch 309/500\n",
      "948/948 [==============================] - 0s - loss: 96.5662 - acc: 0.9736 - val_loss: 58.5785 - val_acc: 0.9462\n",
      "Epoch 310/500\n",
      "948/948 [==============================] - 0s - loss: 90.8466 - acc: 0.9800 - val_loss: 57.5001 - val_acc: 0.9462\n",
      "Epoch 311/500\n",
      "948/948 [==============================] - 0s - loss: 96.7708 - acc: 0.9673 - val_loss: 58.0470 - val_acc: 0.9462\n",
      "Epoch 312/500\n",
      "948/948 [==============================] - 0s - loss: 93.6858 - acc: 0.9705 - val_loss: 58.0309 - val_acc: 0.9494\n",
      "Epoch 313/500\n",
      "948/948 [==============================] - 0s - loss: 94.4472 - acc: 0.9747 - val_loss: 56.1094 - val_acc: 0.9494\n",
      "Epoch 314/500\n",
      "948/948 [==============================] - 0s - loss: 93.6953 - acc: 0.9726 - val_loss: 56.9719 - val_acc: 0.9462\n",
      "Epoch 315/500\n",
      "948/948 [==============================] - 0s - loss: 91.8980 - acc: 0.9757 - val_loss: 57.8926 - val_acc: 0.9462\n",
      "Epoch 316/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 93.5735 - acc: 0.9736 - val_loss: 58.0941 - val_acc: 0.9399\n",
      "Epoch 317/500\n",
      "948/948 [==============================] - 0s - loss: 95.4296 - acc: 0.9694 - val_loss: 59.9871 - val_acc: 0.9430\n",
      "Epoch 318/500\n",
      "948/948 [==============================] - 0s - loss: 85.0875 - acc: 0.9789 - val_loss: 59.1491 - val_acc: 0.9462\n",
      "Epoch 319/500\n",
      "948/948 [==============================] - 0s - loss: 93.9261 - acc: 0.9852 - val_loss: 59.4629 - val_acc: 0.9462\n",
      "Epoch 320/500\n",
      "948/948 [==============================] - 0s - loss: 94.4860 - acc: 0.9726 - val_loss: 58.3682 - val_acc: 0.9462\n",
      "Epoch 321/500\n",
      "948/948 [==============================] - 0s - loss: 91.2505 - acc: 0.9800 - val_loss: 58.3312 - val_acc: 0.9462\n",
      "Epoch 322/500\n",
      "948/948 [==============================] - 0s - loss: 91.7068 - acc: 0.9673 - val_loss: 57.7426 - val_acc: 0.9462\n",
      "Epoch 323/500\n",
      "948/948 [==============================] - 0s - loss: 100.4199 - acc: 0.9747 - val_loss: 56.7031 - val_acc: 0.9462\n",
      "Epoch 324/500\n",
      "948/948 [==============================] - 0s - loss: 103.0600 - acc: 0.9694 - val_loss: 58.7130 - val_acc: 0.9462\n",
      "Epoch 325/500\n",
      "948/948 [==============================] - 0s - loss: 94.2515 - acc: 0.9705 - val_loss: 62.0241 - val_acc: 0.9462\n",
      "Epoch 326/500\n",
      "948/948 [==============================] - 0s - loss: 94.9047 - acc: 0.9694 - val_loss: 61.9871 - val_acc: 0.9462\n",
      "Epoch 327/500\n",
      "948/948 [==============================] - 0s - loss: 96.6290 - acc: 0.9715 - val_loss: 59.3581 - val_acc: 0.9462\n",
      "Epoch 328/500\n",
      "948/948 [==============================] - 0s - loss: 103.7083 - acc: 0.9705 - val_loss: 58.2854 - val_acc: 0.9462\n",
      "Epoch 329/500\n",
      "948/948 [==============================] - 0s - loss: 90.3892 - acc: 0.9789 - val_loss: 59.7887 - val_acc: 0.9462\n",
      "Epoch 330/500\n",
      "948/948 [==============================] - 0s - loss: 99.7141 - acc: 0.9768 - val_loss: 58.5432 - val_acc: 0.9430\n",
      "Epoch 331/500\n",
      "948/948 [==============================] - 0s - loss: 95.5331 - acc: 0.9715 - val_loss: 59.5251 - val_acc: 0.9430\n",
      "Epoch 332/500\n",
      "948/948 [==============================] - 0s - loss: 98.8208 - acc: 0.9757 - val_loss: 58.5526 - val_acc: 0.9367\n",
      "Epoch 333/500\n",
      "948/948 [==============================] - 0s - loss: 95.4738 - acc: 0.9705 - val_loss: 58.2313 - val_acc: 0.9430\n",
      "Epoch 334/500\n",
      "948/948 [==============================] - 0s - loss: 94.5681 - acc: 0.9673 - val_loss: 58.4751 - val_acc: 0.9462\n",
      "Epoch 335/500\n",
      "948/948 [==============================] - 0s - loss: 91.9786 - acc: 0.9726 - val_loss: 57.2027 - val_acc: 0.9462\n",
      "Epoch 336/500\n",
      "948/948 [==============================] - 0s - loss: 92.4802 - acc: 0.9736 - val_loss: 58.0180 - val_acc: 0.9462\n",
      "Epoch 337/500\n",
      "948/948 [==============================] - 0s - loss: 90.0578 - acc: 0.9673 - val_loss: 57.9933 - val_acc: 0.9462\n",
      "Epoch 338/500\n",
      "948/948 [==============================] - 0s - loss: 100.1094 - acc: 0.9726 - val_loss: 56.9067 - val_acc: 0.9462\n",
      "Epoch 339/500\n",
      "948/948 [==============================] - 0s - loss: 97.2996 - acc: 0.9673 - val_loss: 57.1883 - val_acc: 0.9430\n",
      "Epoch 340/500\n",
      "948/948 [==============================] - 0s - loss: 93.4294 - acc: 0.9789 - val_loss: 56.9885 - val_acc: 0.9399\n",
      "Epoch 341/500\n",
      "948/948 [==============================] - 0s - loss: 90.5236 - acc: 0.9705 - val_loss: 58.6966 - val_acc: 0.9462\n",
      "Epoch 342/500\n",
      "948/948 [==============================] - 0s - loss: 92.0921 - acc: 0.9757 - val_loss: 60.4284 - val_acc: 0.9462\n",
      "Epoch 343/500\n",
      "948/948 [==============================] - 0s - loss: 93.2533 - acc: 0.9705 - val_loss: 60.7015 - val_acc: 0.9494\n",
      "Epoch 344/500\n",
      "948/948 [==============================] - 0s - loss: 94.4735 - acc: 0.9736 - val_loss: 60.1505 - val_acc: 0.9462\n",
      "Epoch 345/500\n",
      "948/948 [==============================] - 0s - loss: 83.8047 - acc: 0.9694 - val_loss: 60.6566 - val_acc: 0.9494\n",
      "Epoch 346/500\n",
      "948/948 [==============================] - 0s - loss: 98.0439 - acc: 0.9757 - val_loss: 59.8181 - val_acc: 0.9494\n",
      "Epoch 347/500\n",
      "948/948 [==============================] - 0s - loss: 90.9759 - acc: 0.9726 - val_loss: 59.2806 - val_acc: 0.9430\n",
      "Epoch 348/500\n",
      "948/948 [==============================] - 0s - loss: 88.7440 - acc: 0.9715 - val_loss: 57.5717 - val_acc: 0.9399\n",
      "Epoch 349/500\n",
      "948/948 [==============================] - 0s - loss: 88.3442 - acc: 0.9747 - val_loss: 58.9537 - val_acc: 0.9399\n",
      "Epoch 350/500\n",
      "948/948 [==============================] - 0s - loss: 97.3451 - acc: 0.9705 - val_loss: 59.3686 - val_acc: 0.9399\n",
      "Epoch 351/500\n",
      "948/948 [==============================] - 0s - loss: 90.8697 - acc: 0.9757 - val_loss: 60.4171 - val_acc: 0.9462\n",
      "Epoch 352/500\n",
      "948/948 [==============================] - 0s - loss: 93.2216 - acc: 0.9694 - val_loss: 60.0180 - val_acc: 0.9399\n",
      "Epoch 353/500\n",
      "948/948 [==============================] - 0s - loss: 105.2750 - acc: 0.9800 - val_loss: 61.3836 - val_acc: 0.9462\n",
      "Epoch 354/500\n",
      "948/948 [==============================] - 0s - loss: 85.0874 - acc: 0.9810 - val_loss: 57.8364 - val_acc: 0.9430\n",
      "Epoch 355/500\n",
      "948/948 [==============================] - 0s - loss: 91.8706 - acc: 0.9789 - val_loss: 59.3947 - val_acc: 0.9462\n",
      "Epoch 356/500\n",
      "948/948 [==============================] - 0s - loss: 96.4423 - acc: 0.9726 - val_loss: 59.4379 - val_acc: 0.9494\n",
      "Epoch 357/500\n",
      "948/948 [==============================] - 0s - loss: 90.7990 - acc: 0.9810 - val_loss: 59.8501 - val_acc: 0.9462\n",
      "Epoch 358/500\n",
      "948/948 [==============================] - 0s - loss: 89.5003 - acc: 0.9757 - val_loss: 57.0921 - val_acc: 0.9525\n",
      "Epoch 359/500\n",
      "948/948 [==============================] - 0s - loss: 87.4206 - acc: 0.9778 - val_loss: 57.4250 - val_acc: 0.9525\n",
      "Epoch 360/500\n",
      "948/948 [==============================] - 0s - loss: 88.3801 - acc: 0.9631 - val_loss: 57.1142 - val_acc: 0.9430\n",
      "Epoch 361/500\n",
      "948/948 [==============================] - 0s - loss: 92.5876 - acc: 0.9715 - val_loss: 60.0203 - val_acc: 0.9462\n",
      "Epoch 362/500\n",
      "948/948 [==============================] - 0s - loss: 92.8318 - acc: 0.9884 - val_loss: 63.1303 - val_acc: 0.9525\n",
      "Epoch 363/500\n",
      "948/948 [==============================] - 0s - loss: 93.8479 - acc: 0.9662 - val_loss: 61.9184 - val_acc: 0.9399\n",
      "Epoch 364/500\n",
      "948/948 [==============================] - 0s - loss: 95.7259 - acc: 0.9757 - val_loss: 59.9338 - val_acc: 0.9462\n",
      "Epoch 365/500\n",
      "948/948 [==============================] - 0s - loss: 86.1560 - acc: 0.9831 - val_loss: 59.0685 - val_acc: 0.9430\n",
      "Epoch 366/500\n",
      "948/948 [==============================] - 0s - loss: 87.2084 - acc: 0.9726 - val_loss: 59.2565 - val_acc: 0.9430\n",
      "Epoch 367/500\n",
      "948/948 [==============================] - 0s - loss: 96.8425 - acc: 0.9684 - val_loss: 59.1916 - val_acc: 0.9367\n",
      "Epoch 368/500\n",
      "948/948 [==============================] - 0s - loss: 90.4146 - acc: 0.9747 - val_loss: 59.7645 - val_acc: 0.9399\n",
      "Epoch 369/500\n",
      "948/948 [==============================] - 0s - loss: 94.2800 - acc: 0.9800 - val_loss: 60.0034 - val_acc: 0.9367\n",
      "Epoch 370/500\n",
      "948/948 [==============================] - 0s - loss: 88.0192 - acc: 0.9757 - val_loss: 59.3207 - val_acc: 0.9399\n",
      "Epoch 371/500\n",
      "948/948 [==============================] - 0s - loss: 94.4374 - acc: 0.9778 - val_loss: 59.9789 - val_acc: 0.9462\n",
      "Epoch 372/500\n",
      "948/948 [==============================] - 0s - loss: 96.9435 - acc: 0.9747 - val_loss: 62.9184 - val_acc: 0.9462\n",
      "Epoch 373/500\n",
      "948/948 [==============================] - 0s - loss: 89.1744 - acc: 0.9736 - val_loss: 62.7959 - val_acc: 0.9462\n",
      "Epoch 374/500\n",
      "948/948 [==============================] - 0s - loss: 85.8794 - acc: 0.9800 - val_loss: 60.5437 - val_acc: 0.9430\n",
      "Epoch 375/500\n",
      "948/948 [==============================] - 0s - loss: 90.1336 - acc: 0.9747 - val_loss: 63.3487 - val_acc: 0.9430\n",
      "Epoch 376/500\n",
      "948/948 [==============================] - 0s - loss: 88.8896 - acc: 0.9768 - val_loss: 60.8011 - val_acc: 0.9399\n",
      "Epoch 377/500\n",
      "948/948 [==============================] - 0s - loss: 86.3765 - acc: 0.9768 - val_loss: 60.8425 - val_acc: 0.9399\n",
      "Epoch 378/500\n",
      "948/948 [==============================] - 0s - loss: 88.5457 - acc: 0.9736 - val_loss: 60.4721 - val_acc: 0.9399\n",
      "Epoch 379/500\n",
      "948/948 [==============================] - 0s - loss: 91.4144 - acc: 0.9757 - val_loss: 60.6054 - val_acc: 0.9399\n",
      "Epoch 380/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 84.1323 - acc: 0.9778 - val_loss: 60.9388 - val_acc: 0.9399\n",
      "Epoch 381/500\n",
      "948/948 [==============================] - 0s - loss: 88.2297 - acc: 0.9726 - val_loss: 59.0529 - val_acc: 0.9367\n",
      "Epoch 382/500\n",
      "948/948 [==============================] - 0s - loss: 86.6554 - acc: 0.9705 - val_loss: 58.5257 - val_acc: 0.9335\n",
      "Epoch 383/500\n",
      "948/948 [==============================] - 0s - loss: 86.9475 - acc: 0.9757 - val_loss: 62.2358 - val_acc: 0.9430\n",
      "Epoch 384/500\n",
      "948/948 [==============================] - 0s - loss: 98.1890 - acc: 0.9757 - val_loss: 61.5234 - val_acc: 0.9399\n",
      "Epoch 385/500\n",
      "948/948 [==============================] - 0s - loss: 85.4238 - acc: 0.9705 - val_loss: 61.7172 - val_acc: 0.9462\n",
      "Epoch 386/500\n",
      "948/948 [==============================] - 0s - loss: 87.9454 - acc: 0.9684 - val_loss: 60.1322 - val_acc: 0.9335\n",
      "Epoch 387/500\n",
      "948/948 [==============================] - 0s - loss: 93.3247 - acc: 0.9726 - val_loss: 59.6055 - val_acc: 0.9430\n",
      "Epoch 388/500\n",
      "948/948 [==============================] - 0s - loss: 86.3664 - acc: 0.9684 - val_loss: 60.1305 - val_acc: 0.9430\n",
      "Epoch 389/500\n",
      "948/948 [==============================] - 0s - loss: 88.4837 - acc: 0.9821 - val_loss: 59.6669 - val_acc: 0.9430\n",
      "Epoch 390/500\n",
      "948/948 [==============================] - 0s - loss: 87.1490 - acc: 0.9768 - val_loss: 60.9833 - val_acc: 0.9367\n",
      "Epoch 391/500\n",
      "948/948 [==============================] - 0s - loss: 84.4145 - acc: 0.9715 - val_loss: 60.3767 - val_acc: 0.9367\n",
      "Epoch 392/500\n",
      "948/948 [==============================] - 0s - loss: 95.4332 - acc: 0.9694 - val_loss: 59.9908 - val_acc: 0.9430\n",
      "Epoch 393/500\n",
      "948/948 [==============================] - 0s - loss: 85.2801 - acc: 0.9694 - val_loss: 58.0706 - val_acc: 0.9430\n",
      "Epoch 394/500\n",
      "948/948 [==============================] - 0s - loss: 87.3563 - acc: 0.9800 - val_loss: 59.2806 - val_acc: 0.9462\n",
      "Epoch 395/500\n",
      "948/948 [==============================] - 0s - loss: 94.3519 - acc: 0.9684 - val_loss: 58.3037 - val_acc: 0.9494\n",
      "Epoch 396/500\n",
      "948/948 [==============================] - 0s - loss: 85.2301 - acc: 0.9778 - val_loss: 59.8625 - val_acc: 0.9494\n",
      "Epoch 397/500\n",
      "948/948 [==============================] - 0s - loss: 88.0877 - acc: 0.9778 - val_loss: 61.7400 - val_acc: 0.9430\n",
      "Epoch 398/500\n",
      "948/948 [==============================] - 0s - loss: 89.0772 - acc: 0.9726 - val_loss: 57.9202 - val_acc: 0.9462\n",
      "Epoch 399/500\n",
      "948/948 [==============================] - 0s - loss: 85.9502 - acc: 0.9684 - val_loss: 59.1178 - val_acc: 0.9462\n",
      "Epoch 400/500\n",
      "948/948 [==============================] - 0s - loss: 85.7346 - acc: 0.9736 - val_loss: 60.1649 - val_acc: 0.9494\n",
      "Epoch 401/500\n",
      "948/948 [==============================] - 0s - loss: 82.8128 - acc: 0.9800 - val_loss: 61.3590 - val_acc: 0.9430\n",
      "Epoch 402/500\n",
      "948/948 [==============================] - 0s - loss: 86.6378 - acc: 0.9789 - val_loss: 61.3217 - val_acc: 0.9462\n",
      "Epoch 403/500\n",
      "948/948 [==============================] - 0s - loss: 89.1788 - acc: 0.9705 - val_loss: 59.1905 - val_acc: 0.9430\n",
      "Epoch 404/500\n",
      "948/948 [==============================] - 0s - loss: 86.9436 - acc: 0.9747 - val_loss: 59.1081 - val_acc: 0.9462\n",
      "Epoch 405/500\n",
      "948/948 [==============================] - 0s - loss: 88.3568 - acc: 0.9736 - val_loss: 58.4240 - val_acc: 0.9430\n",
      "Epoch 406/500\n",
      "948/948 [==============================] - 0s - loss: 93.2949 - acc: 0.9747 - val_loss: 58.6956 - val_acc: 0.9430\n",
      "Epoch 407/500\n",
      "948/948 [==============================] - 0s - loss: 91.2283 - acc: 0.9757 - val_loss: 62.9379 - val_acc: 0.9462\n",
      "Epoch 408/500\n",
      "948/948 [==============================] - 0s - loss: 94.4141 - acc: 0.9715 - val_loss: 60.4311 - val_acc: 0.9462\n",
      "Epoch 409/500\n",
      "948/948 [==============================] - 0s - loss: 88.1287 - acc: 0.9641 - val_loss: 58.0243 - val_acc: 0.9494\n",
      "Epoch 410/500\n",
      "948/948 [==============================] - 0s - loss: 95.7054 - acc: 0.9810 - val_loss: 59.9927 - val_acc: 0.9462\n",
      "Epoch 411/500\n",
      "948/948 [==============================] - 0s - loss: 86.4399 - acc: 0.9726 - val_loss: 61.8693 - val_acc: 0.9367\n",
      "Epoch 412/500\n",
      "948/948 [==============================] - 0s - loss: 84.1770 - acc: 0.9726 - val_loss: 60.4133 - val_acc: 0.9430\n",
      "Epoch 413/500\n",
      "948/948 [==============================] - 0s - loss: 81.8546 - acc: 0.9747 - val_loss: 61.3909 - val_acc: 0.9462\n",
      "Epoch 414/500\n",
      "948/948 [==============================] - 0s - loss: 86.5881 - acc: 0.9789 - val_loss: 60.8663 - val_acc: 0.9494\n",
      "Epoch 415/500\n",
      "948/948 [==============================] - 0s - loss: 91.9140 - acc: 0.9789 - val_loss: 61.1681 - val_acc: 0.9494\n",
      "Epoch 416/500\n",
      "948/948 [==============================] - 0s - loss: 87.7886 - acc: 0.9778 - val_loss: 61.5892 - val_acc: 0.9430\n",
      "Epoch 417/500\n",
      "948/948 [==============================] - 0s - loss: 91.2199 - acc: 0.9800 - val_loss: 58.4326 - val_acc: 0.9462\n",
      "Epoch 418/500\n",
      "948/948 [==============================] - 0s - loss: 90.2446 - acc: 0.9715 - val_loss: 58.0045 - val_acc: 0.9367\n",
      "Epoch 419/500\n",
      "948/948 [==============================] - 0s - loss: 86.6388 - acc: 0.9715 - val_loss: 57.6404 - val_acc: 0.9462\n",
      "Epoch 420/500\n",
      "948/948 [==============================] - 0s - loss: 96.3334 - acc: 0.9694 - val_loss: 58.5767 - val_acc: 0.9494\n",
      "Epoch 421/500\n",
      "948/948 [==============================] - 0s - loss: 81.9283 - acc: 0.9768 - val_loss: 57.9011 - val_acc: 0.9462\n",
      "Epoch 422/500\n",
      "948/948 [==============================] - 0s - loss: 84.4466 - acc: 0.9726 - val_loss: 58.3761 - val_acc: 0.9430\n",
      "Epoch 423/500\n",
      "948/948 [==============================] - 0s - loss: 91.9777 - acc: 0.9747 - val_loss: 59.4096 - val_acc: 0.9462\n",
      "Epoch 424/500\n",
      "948/948 [==============================] - 0s - loss: 92.4634 - acc: 0.9768 - val_loss: 58.8532 - val_acc: 0.9462\n",
      "Epoch 425/500\n",
      "948/948 [==============================] - 0s - loss: 85.2517 - acc: 0.9694 - val_loss: 60.0188 - val_acc: 0.9399\n",
      "Epoch 426/500\n",
      "948/948 [==============================] - 0s - loss: 87.7317 - acc: 0.9778 - val_loss: 60.5509 - val_acc: 0.9430\n",
      "Epoch 427/500\n",
      "948/948 [==============================] - 0s - loss: 82.4193 - acc: 0.9705 - val_loss: 61.8363 - val_acc: 0.9525\n",
      "Epoch 428/500\n",
      "948/948 [==============================] - 0s - loss: 83.0043 - acc: 0.9715 - val_loss: 61.1614 - val_acc: 0.9462\n",
      "Epoch 429/500\n",
      "948/948 [==============================] - 0s - loss: 85.9606 - acc: 0.9789 - val_loss: 60.8069 - val_acc: 0.9525\n",
      "Epoch 430/500\n",
      "948/948 [==============================] - 0s - loss: 84.9032 - acc: 0.9768 - val_loss: 62.0688 - val_acc: 0.9525\n",
      "Epoch 431/500\n",
      "948/948 [==============================] - 0s - loss: 87.5920 - acc: 0.9778 - val_loss: 64.0659 - val_acc: 0.9430\n",
      "Epoch 432/500\n",
      "948/948 [==============================] - 0s - loss: 86.2718 - acc: 0.9726 - val_loss: 64.8728 - val_acc: 0.9462\n",
      "Epoch 433/500\n",
      "948/948 [==============================] - 0s - loss: 89.9594 - acc: 0.9736 - val_loss: 61.7538 - val_acc: 0.9430\n",
      "Epoch 434/500\n",
      "948/948 [==============================] - 0s - loss: 89.6247 - acc: 0.9652 - val_loss: 63.1647 - val_acc: 0.9462\n",
      "Epoch 435/500\n",
      "948/948 [==============================] - 0s - loss: 83.1912 - acc: 0.9662 - val_loss: 62.2469 - val_acc: 0.9462\n",
      "Epoch 436/500\n",
      "948/948 [==============================] - 0s - loss: 78.0984 - acc: 0.9726 - val_loss: 61.8916 - val_acc: 0.9367\n",
      "Epoch 437/500\n",
      "948/948 [==============================] - 0s - loss: 89.0352 - acc: 0.9810 - val_loss: 61.8991 - val_acc: 0.9367\n",
      "Epoch 438/500\n",
      "948/948 [==============================] - 0s - loss: 85.7585 - acc: 0.9778 - val_loss: 63.6708 - val_acc: 0.9367\n",
      "Epoch 439/500\n",
      "948/948 [==============================] - 0s - loss: 86.3185 - acc: 0.9705 - val_loss: 64.9747 - val_acc: 0.9430\n",
      "Epoch 440/500\n",
      "948/948 [==============================] - 0s - loss: 86.6456 - acc: 0.9789 - val_loss: 64.2637 - val_acc: 0.9367\n",
      "Epoch 441/500\n",
      "948/948 [==============================] - 0s - loss: 85.2998 - acc: 0.9726 - val_loss: 62.9472 - val_acc: 0.9430\n",
      "Epoch 442/500\n",
      "948/948 [==============================] - 0s - loss: 87.2476 - acc: 0.9789 - val_loss: 61.5168 - val_acc: 0.9462\n",
      "Epoch 443/500\n",
      "948/948 [==============================] - 0s - loss: 86.2386 - acc: 0.9726 - val_loss: 62.4278 - val_acc: 0.9462\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948/948 [==============================] - 0s - loss: 87.7647 - acc: 0.9747 - val_loss: 66.1670 - val_acc: 0.9494\n",
      "Epoch 445/500\n",
      "948/948 [==============================] - 0s - loss: 82.5262 - acc: 0.9757 - val_loss: 65.3281 - val_acc: 0.9462\n",
      "Epoch 446/500\n",
      "948/948 [==============================] - 0s - loss: 84.4161 - acc: 0.9726 - val_loss: 61.4632 - val_acc: 0.9462\n",
      "Epoch 447/500\n",
      "948/948 [==============================] - 0s - loss: 79.4684 - acc: 0.9705 - val_loss: 61.0028 - val_acc: 0.9462\n",
      "Epoch 448/500\n",
      "948/948 [==============================] - 0s - loss: 79.8269 - acc: 0.9747 - val_loss: 59.1620 - val_acc: 0.9525\n",
      "Epoch 449/500\n",
      "948/948 [==============================] - 0s - loss: 91.7221 - acc: 0.9789 - val_loss: 61.2229 - val_acc: 0.9494\n",
      "Epoch 450/500\n",
      "948/948 [==============================] - 0s - loss: 83.3417 - acc: 0.9757 - val_loss: 62.7631 - val_acc: 0.9462\n",
      "Epoch 451/500\n",
      "948/948 [==============================] - 0s - loss: 80.9804 - acc: 0.9715 - val_loss: 61.9156 - val_acc: 0.9462\n",
      "Epoch 452/500\n",
      "948/948 [==============================] - 0s - loss: 86.3859 - acc: 0.9736 - val_loss: 60.0272 - val_acc: 0.9367\n",
      "Epoch 453/500\n",
      "948/948 [==============================] - 0s - loss: 86.7802 - acc: 0.9694 - val_loss: 61.7771 - val_acc: 0.9399\n",
      "Epoch 454/500\n",
      "948/948 [==============================] - 0s - loss: 79.5289 - acc: 0.9810 - val_loss: 62.5120 - val_acc: 0.9462\n",
      "Epoch 455/500\n",
      "948/948 [==============================] - 0s - loss: 86.3980 - acc: 0.9715 - val_loss: 60.0628 - val_acc: 0.9430\n",
      "Epoch 456/500\n",
      "948/948 [==============================] - 0s - loss: 87.4115 - acc: 0.9673 - val_loss: 61.2740 - val_acc: 0.9399\n",
      "Epoch 457/500\n",
      "948/948 [==============================] - 0s - loss: 87.1862 - acc: 0.9747 - val_loss: 62.8933 - val_acc: 0.9462\n",
      "Epoch 458/500\n",
      "948/948 [==============================] - 0s - loss: 83.0974 - acc: 0.9726 - val_loss: 63.4897 - val_acc: 0.9430\n",
      "Epoch 459/500\n",
      "948/948 [==============================] - 0s - loss: 90.7081 - acc: 0.9715 - val_loss: 60.6303 - val_acc: 0.9430\n",
      "Epoch 460/500\n",
      "948/948 [==============================] - 0s - loss: 85.1924 - acc: 0.9747 - val_loss: 60.0470 - val_acc: 0.9430\n",
      "Epoch 461/500\n",
      "948/948 [==============================] - 0s - loss: 85.3902 - acc: 0.9800 - val_loss: 60.0408 - val_acc: 0.9462\n",
      "Epoch 462/500\n",
      "948/948 [==============================] - 0s - loss: 89.4855 - acc: 0.9768 - val_loss: 59.4175 - val_acc: 0.9430\n",
      "Epoch 463/500\n",
      "948/948 [==============================] - 0s - loss: 85.5335 - acc: 0.9662 - val_loss: 59.8297 - val_acc: 0.9399\n",
      "Epoch 464/500\n",
      "948/948 [==============================] - 0s - loss: 79.9333 - acc: 0.9747 - val_loss: 60.4590 - val_acc: 0.9462\n",
      "Epoch 465/500\n",
      "948/948 [==============================] - 0s - loss: 89.1868 - acc: 0.9705 - val_loss: 61.1736 - val_acc: 0.9430\n",
      "Epoch 466/500\n",
      "948/948 [==============================] - 0s - loss: 88.2032 - acc: 0.9768 - val_loss: 59.3484 - val_acc: 0.9430\n",
      "Epoch 467/500\n",
      "948/948 [==============================] - 0s - loss: 82.2674 - acc: 0.9715 - val_loss: 58.2564 - val_acc: 0.9462\n",
      "Epoch 468/500\n",
      "948/948 [==============================] - 0s - loss: 84.5850 - acc: 0.9789 - val_loss: 58.8626 - val_acc: 0.9462\n",
      "Epoch 469/500\n",
      "948/948 [==============================] - 0s - loss: 81.1453 - acc: 0.9736 - val_loss: 61.5066 - val_acc: 0.9462\n",
      "Epoch 470/500\n",
      "948/948 [==============================] - 0s - loss: 84.7924 - acc: 0.9800 - val_loss: 61.2526 - val_acc: 0.9430\n",
      "Epoch 471/500\n",
      "948/948 [==============================] - 0s - loss: 86.3666 - acc: 0.9736 - val_loss: 60.6505 - val_acc: 0.9462\n",
      "Epoch 472/500\n",
      "948/948 [==============================] - 0s - loss: 89.4138 - acc: 0.9694 - val_loss: 58.9832 - val_acc: 0.9430\n",
      "Epoch 473/500\n",
      "948/948 [==============================] - 0s - loss: 80.1509 - acc: 0.9768 - val_loss: 59.7302 - val_acc: 0.9430\n",
      "Epoch 474/500\n",
      "948/948 [==============================] - 0s - loss: 88.7417 - acc: 0.9694 - val_loss: 58.5033 - val_acc: 0.9462\n",
      "Epoch 475/500\n",
      "948/948 [==============================] - 0s - loss: 81.6979 - acc: 0.9789 - val_loss: 58.4249 - val_acc: 0.9462\n",
      "Epoch 476/500\n",
      "948/948 [==============================] - 0s - loss: 87.6685 - acc: 0.9757 - val_loss: 62.2801 - val_acc: 0.9430\n",
      "Epoch 477/500\n",
      "948/948 [==============================] - 0s - loss: 86.9662 - acc: 0.9726 - val_loss: 60.3802 - val_acc: 0.9462\n",
      "Epoch 478/500\n",
      "948/948 [==============================] - 0s - loss: 87.8499 - acc: 0.9684 - val_loss: 59.7075 - val_acc: 0.9367\n",
      "Epoch 479/500\n",
      "948/948 [==============================] - 0s - loss: 79.6906 - acc: 0.9842 - val_loss: 59.8594 - val_acc: 0.9367\n",
      "Epoch 480/500\n",
      "948/948 [==============================] - 0s - loss: 81.1146 - acc: 0.9768 - val_loss: 56.9747 - val_acc: 0.9462\n",
      "Epoch 481/500\n",
      "948/948 [==============================] - 0s - loss: 87.6184 - acc: 0.9747 - val_loss: 57.4588 - val_acc: 0.9462\n",
      "Epoch 482/500\n",
      "948/948 [==============================] - 0s - loss: 82.9493 - acc: 0.9736 - val_loss: 58.6061 - val_acc: 0.9462\n",
      "Epoch 483/500\n",
      "948/948 [==============================] - 0s - loss: 86.8064 - acc: 0.9757 - val_loss: 58.6812 - val_acc: 0.9494\n",
      "Epoch 484/500\n",
      "948/948 [==============================] - 0s - loss: 84.0647 - acc: 0.9768 - val_loss: 56.6789 - val_acc: 0.9525\n",
      "Epoch 485/500\n",
      "948/948 [==============================] - 0s - loss: 86.8758 - acc: 0.9810 - val_loss: 56.6191 - val_acc: 0.9525\n",
      "Epoch 486/500\n",
      "948/948 [==============================] - 0s - loss: 85.5050 - acc: 0.9705 - val_loss: 59.1943 - val_acc: 0.9494\n",
      "Epoch 487/500\n",
      "948/948 [==============================] - 0s - loss: 82.8194 - acc: 0.9736 - val_loss: 57.1648 - val_acc: 0.9462\n",
      "Epoch 488/500\n",
      "948/948 [==============================] - 0s - loss: 86.4826 - acc: 0.9694 - val_loss: 55.9743 - val_acc: 0.9462\n",
      "Epoch 489/500\n",
      "948/948 [==============================] - 0s - loss: 81.0426 - acc: 0.9726 - val_loss: 56.3010 - val_acc: 0.9462\n",
      "Epoch 490/500\n",
      "948/948 [==============================] - 0s - loss: 85.2649 - acc: 0.9715 - val_loss: 56.7228 - val_acc: 0.9462\n",
      "Epoch 491/500\n",
      "948/948 [==============================] - 0s - loss: 83.0949 - acc: 0.9736 - val_loss: 56.7192 - val_acc: 0.9462\n",
      "Epoch 492/500\n",
      "948/948 [==============================] - 0s - loss: 81.2667 - acc: 0.9778 - val_loss: 58.0657 - val_acc: 0.9462\n",
      "Epoch 493/500\n",
      "948/948 [==============================] - 0s - loss: 80.8380 - acc: 0.9778 - val_loss: 57.0350 - val_acc: 0.9430\n",
      "Epoch 494/500\n",
      "948/948 [==============================] - 0s - loss: 85.5381 - acc: 0.9747 - val_loss: 59.3172 - val_acc: 0.9462\n",
      "Epoch 495/500\n",
      "948/948 [==============================] - 0s - loss: 90.8837 - acc: 0.9810 - val_loss: 59.0041 - val_acc: 0.9494\n",
      "Epoch 496/500\n",
      "948/948 [==============================] - 0s - loss: 89.9587 - acc: 0.9736 - val_loss: 57.4032 - val_acc: 0.9462\n",
      "Epoch 497/500\n",
      "948/948 [==============================] - 0s - loss: 86.5861 - acc: 0.9757 - val_loss: 58.3824 - val_acc: 0.9462\n",
      "Epoch 498/500\n",
      "948/948 [==============================] - 0s - loss: 83.9150 - acc: 0.9747 - val_loss: 60.5656 - val_acc: 0.9430\n",
      "Epoch 499/500\n",
      "948/948 [==============================] - 0s - loss: 85.6259 - acc: 0.9768 - val_loss: 60.1891 - val_acc: 0.9430\n",
      "Epoch 500/500\n",
      "948/948 [==============================] - 0s - loss: 86.3563 - acc: 0.9694 - val_loss: 58.3236 - val_acc: 0.9399\n"
     ]
    }
   ],
   "source": [
    "model2 = classifier2()\n",
    "model2.fit(train_X, train_Y, validation_data=(val_X, val_Y), nb_epoch=nb_epochs, batch_size=batch_size)\n",
    "pre_Y2 = model2.predict(train_x)\n",
    "pre_Y2_t = model2.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.34040122807\n"
     ]
    }
   ],
   "source": [
    "error_2, accuracy_2 = accuracy(pre_Y2_t, test_y)\n",
    "print(accuracy_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=992, activation=\"relu\", use_bias=True)`\n",
      "  model.add(Dense(512, input_dim=input_size, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:25: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"relu\", use_bias=True)`\n",
      "  model.add(Dense(256, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:31: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, input_dim=256, activation=\"relu\", use_bias=True)`\n",
      "  e.add(Dense(512, input_dim=256, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(992, activation=\"relu\", use_bias=True)`\n",
      "  e.add(Dense(input_size, activation='relu', bias=True))\n",
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\keras\\models.py:844: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 2/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 3/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 4/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 5/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 6/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 7/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 8/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 9/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 10/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 11/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 12/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 13/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 14/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 15/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 16/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 17/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 18/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 19/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 20/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 21/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 22/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 23/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 24/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 25/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 26/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 27/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 28/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 29/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 30/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 31/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 32/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 33/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 34/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 35/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 36/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 37/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 38/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 39/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 40/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 41/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 42/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 43/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 44/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 45/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 46/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 47/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 48/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 49/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 50/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 51/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 52/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 53/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 54/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 55/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 56/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 57/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 58/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 59/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 60/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 61/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 62/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 63/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 64/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 65/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 66/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 67/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 68/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 69/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 70/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 71/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 72/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 73/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 74/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 75/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 76/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 77/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 78/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 79/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 80/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 81/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 82/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 83/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 84/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 85/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 86/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 87/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 88/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 89/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 90/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 91/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 92/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 93/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 94/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 95/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 96/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 97/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 98/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 99/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Epoch 100/100\n",
      "1011/1011 [==============================] - 0s - loss: 0.0010     \n",
      "Train on 1011 samples, validate on 253 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011/1011 [==============================] - 0s - loss: 4474.3262 - val_loss: 3880.5954\n",
      "Epoch 2/100\n",
      "1011/1011 [==============================] - 0s - loss: 1821.1584 - val_loss: 649.9554\n",
      "Epoch 3/100\n",
      "1011/1011 [==============================] - 0s - loss: 571.3432 - val_loss: 284.3620\n",
      "Epoch 4/100\n",
      "1011/1011 [==============================] - 0s - loss: 338.6924 - val_loss: 227.5849\n",
      "Epoch 5/100\n",
      "1011/1011 [==============================] - 0s - loss: 297.8874 - val_loss: 206.4954\n",
      "Epoch 6/100\n",
      "1011/1011 [==============================] - 0s - loss: 278.0182 - val_loss: 194.1532\n",
      "Epoch 7/100\n",
      "1011/1011 [==============================] - 0s - loss: 262.3271 - val_loss: 191.9393\n",
      "Epoch 8/100\n",
      "1011/1011 [==============================] - 0s - loss: 263.8454 - val_loss: 175.7105\n",
      "Epoch 9/100\n",
      "1011/1011 [==============================] - 0s - loss: 240.0097 - val_loss: 174.7396\n",
      "Epoch 10/100\n",
      "1011/1011 [==============================] - 0s - loss: 232.7842 - val_loss: 179.2035\n",
      "Epoch 11/100\n",
      "1011/1011 [==============================] - 0s - loss: 231.1609 - val_loss: 160.6417\n",
      "Epoch 12/100\n",
      "1011/1011 [==============================] - 0s - loss: 219.8315 - val_loss: 168.3576\n",
      "Epoch 13/100\n",
      "1011/1011 [==============================] - 0s - loss: 210.6646 - val_loss: 180.7952\n",
      "Epoch 14/100\n",
      "1011/1011 [==============================] - 0s - loss: 212.6820 - val_loss: 139.0642\n",
      "Epoch 15/100\n",
      "1011/1011 [==============================] - 0s - loss: 179.5398 - val_loss: 133.2834\n",
      "Epoch 16/100\n",
      "1011/1011 [==============================] - 0s - loss: 157.5710 - val_loss: 130.4835\n",
      "Epoch 17/100\n",
      "1011/1011 [==============================] - 0s - loss: 147.4773 - val_loss: 87.9519\n",
      "Epoch 18/100\n",
      "1011/1011 [==============================] - 0s - loss: 138.4519 - val_loss: 92.4518\n",
      "Epoch 19/100\n",
      "1011/1011 [==============================] - 0s - loss: 131.3033 - val_loss: 79.7618\n",
      "Epoch 20/100\n",
      "1011/1011 [==============================] - 0s - loss: 128.4590 - val_loss: 74.7696\n",
      "Epoch 21/100\n",
      "1011/1011 [==============================] - 0s - loss: 122.1510 - val_loss: 107.1967\n",
      "Epoch 22/100\n",
      "1011/1011 [==============================] - 0s - loss: 115.8613 - val_loss: 85.2415\n",
      "Epoch 23/100\n",
      "1011/1011 [==============================] - 0s - loss: 109.4193 - val_loss: 76.6417\n",
      "Epoch 24/100\n",
      "1011/1011 [==============================] - 0s - loss: 105.9166 - val_loss: 77.9972\n",
      "Epoch 25/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.5832 - val_loss: 73.7870\n",
      "Epoch 26/100\n",
      "1011/1011 [==============================] - 0s - loss: 106.6189 - val_loss: 103.8239\n",
      "Epoch 27/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.9575 - val_loss: 79.3691\n",
      "Epoch 28/100\n",
      "1011/1011 [==============================] - 0s - loss: 108.4511 - val_loss: 99.4405\n",
      "Epoch 29/100\n",
      "1011/1011 [==============================] - 0s - loss: 111.2533 - val_loss: 90.0506\n",
      "Epoch 30/100\n",
      "1011/1011 [==============================] - 0s - loss: 106.1584 - val_loss: 71.9150\n",
      "Epoch 31/100\n",
      "1011/1011 [==============================] - 0s - loss: 107.4159 - val_loss: 79.5797\n",
      "Epoch 32/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.9902 - val_loss: 102.1024\n",
      "Epoch 33/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.8635 - val_loss: 70.1583\n",
      "Epoch 34/100\n",
      "1011/1011 [==============================] - 0s - loss: 102.6015 - val_loss: 78.5147\n",
      "Epoch 35/100\n",
      "1011/1011 [==============================] - 0s - loss: 100.7414 - val_loss: 100.6274\n",
      "Epoch 36/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.2235 - val_loss: 71.2878\n",
      "Epoch 37/100\n",
      "1011/1011 [==============================] - 0s - loss: 100.9876 - val_loss: 75.5796\n",
      "Epoch 38/100\n",
      "1011/1011 [==============================] - 0s - loss: 91.4318 - val_loss: 82.0382\n",
      "Epoch 39/100\n",
      "1011/1011 [==============================] - 0s - loss: 93.0727 - val_loss: 71.6865\n",
      "Epoch 40/100\n",
      "1011/1011 [==============================] - 0s - loss: 92.7081 - val_loss: 95.4925\n",
      "Epoch 41/100\n",
      "1011/1011 [==============================] - 0s - loss: 98.2830 - val_loss: 81.2996\n",
      "Epoch 42/100\n",
      "1011/1011 [==============================] - 0s - loss: 88.6960 - val_loss: 77.4189\n",
      "Epoch 43/100\n",
      "1011/1011 [==============================] - 0s - loss: 98.8275 - val_loss: 77.5593\n",
      "Epoch 44/100\n",
      "1011/1011 [==============================] - 0s - loss: 93.7769 - val_loss: 82.8451\n",
      "Epoch 45/100\n",
      "1011/1011 [==============================] - 0s - loss: 86.7737 - val_loss: 80.0005\n",
      "Epoch 46/100\n",
      "1011/1011 [==============================] - 0s - loss: 92.3521 - val_loss: 76.0354\n",
      "Epoch 47/100\n",
      "1011/1011 [==============================] - 0s - loss: 91.5814 - val_loss: 72.0187\n",
      "Epoch 48/100\n",
      "1011/1011 [==============================] - 0s - loss: 99.8585 - val_loss: 77.1783\n",
      "Epoch 49/100\n",
      "1011/1011 [==============================] - 0s - loss: 89.0409 - val_loss: 76.2235\n",
      "Epoch 50/100\n",
      "1011/1011 [==============================] - 0s - loss: 93.1688 - val_loss: 77.6638\n",
      "Epoch 51/100\n",
      "1011/1011 [==============================] - 0s - loss: 94.7580 - val_loss: 81.3360\n",
      "Epoch 52/100\n",
      "1011/1011 [==============================] - 0s - loss: 88.5794 - val_loss: 72.4446\n",
      "Epoch 53/100\n",
      "1011/1011 [==============================] - 0s - loss: 90.9997 - val_loss: 70.9698\n",
      "Epoch 54/100\n",
      "1011/1011 [==============================] - 0s - loss: 94.5248 - val_loss: 77.7950\n",
      "Epoch 55/100\n",
      "1011/1011 [==============================] - 0s - loss: 89.8682 - val_loss: 71.6456\n",
      "Epoch 56/100\n",
      "1011/1011 [==============================] - 0s - loss: 91.2025 - val_loss: 63.7261\n",
      "Epoch 57/100\n",
      "1011/1011 [==============================] - 0s - loss: 93.1397 - val_loss: 83.1343\n",
      "Epoch 58/100\n",
      "1011/1011 [==============================] - 0s - loss: 84.9218 - val_loss: 59.8023\n",
      "Epoch 59/100\n",
      "1011/1011 [==============================] - 0s - loss: 90.8306 - val_loss: 67.5779\n",
      "Epoch 60/100\n",
      "1011/1011 [==============================] - 0s - loss: 81.2748 - val_loss: 68.7372\n",
      "Epoch 61/100\n",
      "1011/1011 [==============================] - 0s - loss: 82.1346 - val_loss: 69.8149\n",
      "Epoch 62/100\n",
      "1011/1011 [==============================] - 0s - loss: 86.2249 - val_loss: 66.3957\n",
      "Epoch 63/100\n",
      "1011/1011 [==============================] - 0s - loss: 80.8626 - val_loss: 77.7904\n",
      "Epoch 64/100\n",
      "1011/1011 [==============================] - 0s - loss: 85.4599 - val_loss: 74.9446\n",
      "Epoch 65/100\n",
      "1011/1011 [==============================] - 0s - loss: 91.2456 - val_loss: 72.3643\n",
      "Epoch 66/100\n",
      "1011/1011 [==============================] - 0s - loss: 84.3795 - val_loss: 73.2238\n",
      "Epoch 67/100\n",
      "1011/1011 [==============================] - 0s - loss: 92.5539 - val_loss: 72.6520\n",
      "Epoch 68/100\n",
      "1011/1011 [==============================] - 0s - loss: 83.9100 - val_loss: 67.7685\n",
      "Epoch 69/100\n",
      "1011/1011 [==============================] - 0s - loss: 89.6079 - val_loss: 83.3941\n",
      "Epoch 70/100\n",
      "1011/1011 [==============================] - 0s - loss: 93.0798 - val_loss: 99.4222\n",
      "Epoch 71/100\n",
      "1011/1011 [==============================] - 0s - loss: 85.3709 - val_loss: 74.1020\n",
      "Epoch 72/100\n",
      "1011/1011 [==============================] - 0s - loss: 89.1108 - val_loss: 70.7772\n",
      "Epoch 73/100\n",
      "1011/1011 [==============================] - 0s - loss: 89.4223 - val_loss: 66.3620\n",
      "Epoch 74/100\n",
      "1011/1011 [==============================] - 0s - loss: 81.7331 - val_loss: 84.0644\n",
      "Epoch 75/100\n",
      "1011/1011 [==============================] - 0s - loss: 83.7243 - val_loss: 77.5894\n",
      "Epoch 76/100\n",
      "1011/1011 [==============================] - 0s - loss: 88.5403 - val_loss: 71.8392\n",
      "Epoch 77/100\n",
      "1011/1011 [==============================] - 0s - loss: 81.7123 - val_loss: 65.7433\n",
      "Epoch 78/100\n",
      "1011/1011 [==============================] - 0s - loss: 84.6198 - val_loss: 73.7854\n",
      "Epoch 79/100\n",
      "1011/1011 [==============================] - 0s - loss: 80.0856 - val_loss: 81.3264\n",
      "Epoch 80/100\n",
      "1011/1011 [==============================] - 0s - loss: 79.0152 - val_loss: 72.3054\n",
      "Epoch 81/100\n",
      "1011/1011 [==============================] - 0s - loss: 83.5918 - val_loss: 71.9301\n",
      "Epoch 82/100\n",
      "1011/1011 [==============================] - 0s - loss: 82.7427 - val_loss: 82.4040\n",
      "Epoch 83/100\n",
      "1011/1011 [==============================] - 0s - loss: 86.0943 - val_loss: 81.9703\n",
      "Epoch 84/100\n",
      "1011/1011 [==============================] - 0s - loss: 84.2618 - val_loss: 80.1052\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011/1011 [==============================] - 0s - loss: 88.4487 - val_loss: 94.6456\n",
      "Epoch 86/100\n",
      "1011/1011 [==============================] - 0s - loss: 83.0008 - val_loss: 68.4630\n",
      "Epoch 87/100\n",
      "1011/1011 [==============================] - 0s - loss: 85.9297 - val_loss: 57.7866\n",
      "Epoch 88/100\n",
      "1011/1011 [==============================] - 0s - loss: 84.8273 - val_loss: 63.2416\n",
      "Epoch 89/100\n",
      "1011/1011 [==============================] - 0s - loss: 82.2490 - val_loss: 74.0204\n",
      "Epoch 90/100\n",
      "1011/1011 [==============================] - 0s - loss: 81.0959 - val_loss: 81.8597\n",
      "Epoch 91/100\n",
      "1011/1011 [==============================] - 0s - loss: 77.8809 - val_loss: 70.5886\n",
      "Epoch 92/100\n",
      "1011/1011 [==============================] - 0s - loss: 80.4724 - val_loss: 71.6384\n",
      "Epoch 93/100\n",
      "1011/1011 [==============================] - 0s - loss: 75.9013 - val_loss: 75.8092\n",
      "Epoch 94/100\n",
      "1011/1011 [==============================] - 0s - loss: 80.7248 - val_loss: 69.8083\n",
      "Epoch 95/100\n",
      "1011/1011 [==============================] - 0s - loss: 78.5750 - val_loss: 67.6583\n",
      "Epoch 96/100\n",
      "1011/1011 [==============================] - 0s - loss: 84.2825 - val_loss: 89.6384\n",
      "Epoch 97/100\n",
      "1011/1011 [==============================] - 0s - loss: 92.5745 - val_loss: 88.9710\n",
      "Epoch 98/100\n",
      "1011/1011 [==============================] - 0s - loss: 83.0475 - val_loss: 69.2873\n",
      "Epoch 99/100\n",
      "1011/1011 [==============================] - 0s - loss: 78.4023 - val_loss: 72.1030\n",
      "Epoch 100/100\n",
      "1011/1011 [==============================] - 0s - loss: 81.2737 - val_loss: 80.2119\n"
     ]
    }
   ],
   "source": [
    "import auto_regression as ar\n",
    "model3=ar.regression(train_x, train_y)\n",
    "pre_Y3=model3.predict(train_x)\n",
    "pre_Y3_t = model3.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.5921279692\n"
     ]
    }
   ],
   "source": [
    "error_3, accuracy_3 = accuracy(pre_Y3_t, test_y)\n",
    "print(accuracy_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1011 samples, validate on 253 samples\n",
      "Epoch 1/100\n",
      "1011/1011 [==============================] - 0s - loss: 4638.6789 - val_loss: 4646.4831\n",
      "Epoch 2/100\n",
      "1011/1011 [==============================] - 0s - loss: 4364.9839 - val_loss: 4131.2581\n",
      "Epoch 3/100\n",
      "1011/1011 [==============================] - 0s - loss: 3552.7515 - val_loss: 2897.1790\n",
      "Epoch 4/100\n",
      "1011/1011 [==============================] - 0s - loss: 2176.7558 - val_loss: 1442.9888\n",
      "Epoch 5/100\n",
      "1011/1011 [==============================] - 0s - loss: 1178.7201 - val_loss: 829.7022\n",
      "Epoch 6/100\n",
      "1011/1011 [==============================] - 0s - loss: 768.4152 - val_loss: 573.6358\n",
      "Epoch 7/100\n",
      "1011/1011 [==============================] - 0s - loss: 586.9927 - val_loss: 444.3046\n",
      "Epoch 8/100\n",
      "1011/1011 [==============================] - 0s - loss: 468.6223 - val_loss: 368.0005\n",
      "Epoch 9/100\n",
      "1011/1011 [==============================] - 0s - loss: 398.3818 - val_loss: 323.8407\n",
      "Epoch 10/100\n",
      "1011/1011 [==============================] - 0s - loss: 355.7782 - val_loss: 292.4463\n",
      "Epoch 11/100\n",
      "1011/1011 [==============================] - 0s - loss: 332.4577 - val_loss: 268.7904\n",
      "Epoch 12/100\n",
      "1011/1011 [==============================] - 0s - loss: 306.1429 - val_loss: 250.1153\n",
      "Epoch 13/100\n",
      "1011/1011 [==============================] - 0s - loss: 284.5917 - val_loss: 237.1542\n",
      "Epoch 14/100\n",
      "1011/1011 [==============================] - 0s - loss: 284.9548 - val_loss: 222.8397\n",
      "Epoch 15/100\n",
      "1011/1011 [==============================] - 0s - loss: 270.2523 - val_loss: 212.3824\n",
      "Epoch 16/100\n",
      "1011/1011 [==============================] - 0s - loss: 255.8470 - val_loss: 201.7142\n",
      "Epoch 17/100\n",
      "1011/1011 [==============================] - 0s - loss: 244.4929 - val_loss: 186.9941\n",
      "Epoch 18/100\n",
      "1011/1011 [==============================] - 0s - loss: 224.3294 - val_loss: 175.4715\n",
      "Epoch 19/100\n",
      "1011/1011 [==============================] - 0s - loss: 227.4284 - val_loss: 163.1634\n",
      "Epoch 20/100\n",
      "1011/1011 [==============================] - 0s - loss: 211.6253 - val_loss: 151.9906\n",
      "Epoch 21/100\n",
      "1011/1011 [==============================] - 0s - loss: 190.7560 - val_loss: 144.9431\n",
      "Epoch 22/100\n",
      "1011/1011 [==============================] - 0s - loss: 187.2682 - val_loss: 134.7405\n",
      "Epoch 23/100\n",
      "1011/1011 [==============================] - 0s - loss: 183.8073 - val_loss: 131.2885\n",
      "Epoch 24/100\n",
      "1011/1011 [==============================] - 0s - loss: 179.9778 - val_loss: 127.3062\n",
      "Epoch 25/100\n",
      "1011/1011 [==============================] - 0s - loss: 166.5010 - val_loss: 122.1675\n",
      "Epoch 26/100\n",
      "1011/1011 [==============================] - 0s - loss: 170.1316 - val_loss: 120.7703\n",
      "Epoch 27/100\n",
      "1011/1011 [==============================] - 0s - loss: 164.4076 - val_loss: 117.9375\n",
      "Epoch 28/100\n",
      "1011/1011 [==============================] - 0s - loss: 161.0133 - val_loss: 114.9987\n",
      "Epoch 29/100\n",
      "1011/1011 [==============================] - 0s - loss: 152.1436 - val_loss: 112.9870\n",
      "Epoch 30/100\n",
      "1011/1011 [==============================] - 0s - loss: 151.5765 - val_loss: 111.1487\n",
      "Epoch 31/100\n",
      "1011/1011 [==============================] - 0s - loss: 154.6798 - val_loss: 111.3338\n",
      "Epoch 32/100\n",
      "1011/1011 [==============================] - 0s - loss: 152.7872 - val_loss: 108.1272\n",
      "Epoch 33/100\n",
      "1011/1011 [==============================] - 0s - loss: 151.4116 - val_loss: 107.7779\n",
      "Epoch 34/100\n",
      "1011/1011 [==============================] - 0s - loss: 136.7399 - val_loss: 105.3810\n",
      "Epoch 35/100\n",
      "1011/1011 [==============================] - 0s - loss: 142.3419 - val_loss: 105.4489\n",
      "Epoch 36/100\n",
      "1011/1011 [==============================] - 0s - loss: 149.2462 - val_loss: 105.3872\n",
      "Epoch 37/100\n",
      "1011/1011 [==============================] - 0s - loss: 144.3858 - val_loss: 102.8544\n",
      "Epoch 38/100\n",
      "1011/1011 [==============================] - 0s - loss: 136.3846 - val_loss: 107.5493\n",
      "Epoch 39/100\n",
      "1011/1011 [==============================] - 0s - loss: 141.9596 - val_loss: 101.0949\n",
      "Epoch 40/100\n",
      "1011/1011 [==============================] - 0s - loss: 142.2263 - val_loss: 100.7716\n",
      "Epoch 41/100\n",
      "1011/1011 [==============================] - 0s - loss: 137.8178 - val_loss: 99.0033\n",
      "Epoch 42/100\n",
      "1011/1011 [==============================] - 0s - loss: 138.6105 - val_loss: 97.2239\n",
      "Epoch 43/100\n",
      "1011/1011 [==============================] - 0s - loss: 128.5407 - val_loss: 96.4261\n",
      "Epoch 44/100\n",
      "1011/1011 [==============================] - 0s - loss: 138.2416 - val_loss: 95.2060\n",
      "Epoch 45/100\n",
      "1011/1011 [==============================] - 0s - loss: 130.6016 - val_loss: 94.5532\n",
      "Epoch 46/100\n",
      "1011/1011 [==============================] - 0s - loss: 135.5114 - val_loss: 94.0553\n",
      "Epoch 47/100\n",
      "1011/1011 [==============================] - 0s - loss: 132.6843 - val_loss: 92.1176\n",
      "Epoch 48/100\n",
      "1011/1011 [==============================] - 0s - loss: 131.2029 - val_loss: 91.8173\n",
      "Epoch 49/100\n",
      "1011/1011 [==============================] - 0s - loss: 126.3162 - val_loss: 91.5923\n",
      "Epoch 50/100\n",
      "1011/1011 [==============================] - 0s - loss: 122.4472 - val_loss: 90.3864\n",
      "Epoch 51/100\n",
      "1011/1011 [==============================] - 0s - loss: 120.7057 - val_loss: 90.7807\n",
      "Epoch 52/100\n",
      "1011/1011 [==============================] - 0s - loss: 124.4613 - val_loss: 89.6854\n",
      "Epoch 53/100\n",
      "1011/1011 [==============================] - 0s - loss: 123.9313 - val_loss: 90.3354\n",
      "Epoch 54/100\n",
      "1011/1011 [==============================] - 0s - loss: 120.4118 - val_loss: 89.6579\n",
      "Epoch 55/100\n",
      "1011/1011 [==============================] - 0s - loss: 124.7742 - val_loss: 88.5732\n",
      "Epoch 56/100\n",
      "1011/1011 [==============================] - 0s - loss: 124.5020 - val_loss: 86.8700\n",
      "Epoch 57/100\n",
      "1011/1011 [==============================] - 0s - loss: 122.9409 - val_loss: 87.5161\n",
      "Epoch 58/100\n",
      "1011/1011 [==============================] - 0s - loss: 119.1000 - val_loss: 87.4781\n",
      "Epoch 59/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.6460 - val_loss: 85.9186\n",
      "Epoch 60/100\n",
      "1011/1011 [==============================] - 0s - loss: 112.9696 - val_loss: 85.0018\n",
      "Epoch 61/100\n",
      "1011/1011 [==============================] - 0s - loss: 117.6574 - val_loss: 84.7029\n",
      "Epoch 62/100\n",
      "1011/1011 [==============================] - 0s - loss: 118.0871 - val_loss: 86.2431\n",
      "Epoch 63/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.1404 - val_loss: 84.2168\n",
      "Epoch 64/100\n",
      "1011/1011 [==============================] - 0s - loss: 117.5588 - val_loss: 86.0110\n",
      "Epoch 65/100\n",
      "1011/1011 [==============================] - 0s - loss: 109.3344 - val_loss: 84.7889\n",
      "Epoch 66/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.0638 - val_loss: 85.2052\n",
      "Epoch 67/100\n",
      "1011/1011 [==============================] - 0s - loss: 115.7456 - val_loss: 87.7337\n",
      "Epoch 68/100\n",
      "1011/1011 [==============================] - 0s - loss: 113.0533 - val_loss: 82.3508\n",
      "Epoch 69/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.5397 - val_loss: 84.6362\n",
      "Epoch 70/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.0789 - val_loss: 80.5083\n",
      "Epoch 71/100\n",
      "1011/1011 [==============================] - 0s - loss: 114.6897 - val_loss: 80.9269\n",
      "Epoch 72/100\n",
      "1011/1011 [==============================] - 0s - loss: 106.0949 - val_loss: 79.5478\n",
      "Epoch 73/100\n",
      "1011/1011 [==============================] - 0s - loss: 110.1800 - val_loss: 80.0538\n",
      "Epoch 74/100\n",
      "1011/1011 [==============================] - 0s - loss: 113.1048 - val_loss: 79.9347\n",
      "Epoch 75/100\n",
      "1011/1011 [==============================] - 0s - loss: 108.1933 - val_loss: 79.1514\n",
      "Epoch 76/100\n",
      "1011/1011 [==============================] - 0s - loss: 103.7018 - val_loss: 80.1854\n",
      "Epoch 77/100\n",
      "1011/1011 [==============================] - 0s - loss: 115.1276 - val_loss: 78.5049\n",
      "Epoch 78/100\n",
      "1011/1011 [==============================] - 0s - loss: 106.0341 - val_loss: 80.5727\n",
      "Epoch 79/100\n",
      "1011/1011 [==============================] - 0s - loss: 108.4772 - val_loss: 77.9068\n",
      "Epoch 80/100\n",
      "1011/1011 [==============================] - 0s - loss: 105.1595 - val_loss: 77.3645\n",
      "Epoch 81/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.1258 - val_loss: 79.0902\n",
      "Epoch 82/100\n",
      "1011/1011 [==============================] - 0s - loss: 110.4260 - val_loss: 77.2138\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011/1011 [==============================] - 0s - loss: 105.3318 - val_loss: 78.8088\n",
      "Epoch 84/100\n",
      "1011/1011 [==============================] - 0s - loss: 105.2315 - val_loss: 78.7226\n",
      "Epoch 85/100\n",
      "1011/1011 [==============================] - 0s - loss: 106.4993 - val_loss: 78.0017\n",
      "Epoch 86/100\n",
      "1011/1011 [==============================] - 0s - loss: 106.4010 - val_loss: 78.2716\n",
      "Epoch 87/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.2241 - val_loss: 76.6005\n",
      "Epoch 88/100\n",
      "1011/1011 [==============================] - 0s - loss: 102.9368 - val_loss: 76.2872\n",
      "Epoch 89/100\n",
      "1011/1011 [==============================] - 0s - loss: 102.8661 - val_loss: 76.8902\n",
      "Epoch 90/100\n",
      "1011/1011 [==============================] - 0s - loss: 100.5191 - val_loss: 76.8852\n",
      "Epoch 91/100\n",
      "1011/1011 [==============================] - 0s - loss: 102.2093 - val_loss: 76.0928\n",
      "Epoch 92/100\n",
      "1011/1011 [==============================] - 0s - loss: 104.9019 - val_loss: 77.7626\n",
      "Epoch 93/100\n",
      "1011/1011 [==============================] - 0s - loss: 101.4258 - val_loss: 77.2242\n",
      "Epoch 94/100\n",
      "1011/1011 [==============================] - 0s - loss: 107.3967 - val_loss: 76.6519\n",
      "Epoch 95/100\n",
      "1011/1011 [==============================] - 0s - loss: 100.4352 - val_loss: 75.0737\n",
      "Epoch 96/100\n",
      "1011/1011 [==============================] - 0s - loss: 103.5857 - val_loss: 73.9429\n",
      "Epoch 97/100\n",
      "1011/1011 [==============================] - 0s - loss: 104.9869 - val_loss: 73.8077\n",
      "Epoch 98/100\n",
      "1011/1011 [==============================] - 0s - loss: 98.4464 - val_loss: 75.2417\n",
      "Epoch 99/100\n",
      "1011/1011 [==============================] - 0s - loss: 100.9385 - val_loss: 72.2157\n",
      "Epoch 100/100\n",
      "1011/1011 [==============================] - 0s - loss: 97.5133 - val_loss: 72.8363\n"
     ]
    }
   ],
   "source": [
    "import regular_regression as rr\n",
    "model4 = rr.train_model(train_x, train_y)\n",
    "pre_Y4=model4.predict(train_x)\n",
    "pre_Y4_t = model4.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.012434598\n"
     ]
    }
   ],
   "source": [
    "error_4, accuracy_4 = accuracy(pre_Y4_t, test_y)\n",
    "print(accuracy_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=4)\n",
    "neigh.fit(train_x, train_y)\n",
    "pre_Y5 = neigh.predict(train_x)\n",
    "pre_Y5_t = neigh.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.52908763973\n"
     ]
    }
   ],
   "source": [
    "error_5, accuracy_5 = accuracy(pre_Y5_t, test_y)\n",
    "print(accuracy_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaped(predict):\n",
    "    size = predict.shape[0]\n",
    "    j = predict.reshape((2*size, 1))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_Y11 = reshaped(pre_Y1)\n",
    "pre_Y21 = reshaped(pre_Y2)\n",
    "pre_Y31 = reshaped(pre_Y3)\n",
    "pre_Y41 = reshaped(pre_Y4)\n",
    "pre_Y51 = reshaped(pre_Y5)\n",
    "pre = np.column_stack((pre_Y11, pre_Y21, pre_Y31, pre_Y41,pre_Y51))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_t = np.column_stack(((reshaped(pre_Y1_t), reshaped(pre_Y2_t),reshaped(pre_Y3_t), reshaped(pre_Y4_t),reshaped(pre_Y5_t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2528"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 104.24365997,  104.87120819,  104.42606354,  104.42606354,\n",
       "         110.71757507,  103.8625    ],\n",
       "       [  41.49926758,   39.51929474,   32.27218628,   32.27218628,\n",
       "          41.85041046,   39.84818   ],\n",
       "       [  92.22106171,   87.23154449,   74.16387939,   74.16387939,\n",
       "          73.80802917,   78.711     ],\n",
       "       ..., \n",
       "       [  35.3372879 ,   33.22958755,   33.81820679,   33.81820679,\n",
       "          37.03022766,   36.90943   ],\n",
       "       [  91.88299561,   95.22770691,   88.02386475,   88.02386475,\n",
       "          93.83798218,   85.954     ],\n",
       "       [  29.46458435,   28.65709496,   24.58782005,   24.58782005,\n",
       "          28.64374352,   17.23793   ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (452,6) and (5,1) not aligned: 6 (dim 1) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-5a5d55f235cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshaped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpred1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m226\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \"\"\"\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m--> 253\u001b[1;33m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (452,6) and (5,1) not aligned: 6 (dim 1) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "# stacking model 1\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(pre, reshaped(train_y))\n",
    "pred1 = lr.predict(pre_t)\n",
    "e, a = accuracy(pred1.reshape((226,2)), test_y)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 105.62419678   38.99786781]\n",
      " [  81.2270999    25.89475089]\n",
      " [ 116.2762937    34.12244628]\n",
      " [ 130.81536328   27.59213693]\n",
      " [  53.42100974   15.97249547]\n",
      " [  91.34338184   45.0831464 ]\n",
      " [  32.16121355   18.96290255]\n",
      " [ 105.2199939    24.98634357]\n",
      " [  51.87518779   17.01497619]\n",
      " [  64.15566646   29.0076044 ]\n",
      " [  73.02900205   20.10207016]\n",
      " [  68.36074048   20.78693101]\n",
      " [  70.74947197   23.53102262]\n",
      " [  52.63991785   23.73156586]\n",
      " [ 122.10410449   37.00175707]\n",
      " [ 115.09697168   66.32476574]\n",
      " [ 126.85948926   60.29914733]\n",
      " [  93.14928887   30.01999276]\n",
      " [ 132.55691699   27.1258681 ]\n",
      " [  28.58134711   22.13776842]\n",
      " [  59.93068369   32.72369904]\n",
      " [  73.7562002    19.30075001]\n",
      " [ 105.28207861   21.04736198]\n",
      " [  90.2177688    57.78709064]\n",
      " [ 119.56389941   62.46445543]\n",
      " [  53.21831912   23.63803623]\n",
      " [  61.40895967   18.28232599]\n",
      " [  99.08906152   33.03187916]\n",
      " [  56.81542424   14.76982914]\n",
      " [  80.16533276   36.43007663]\n",
      " [  91.37498584   28.69053822]\n",
      " [  89.91155103   51.82657357]\n",
      " [ 138.16559668   32.94233493]\n",
      " [  85.18561729   27.09790961]\n",
      " [ 102.85965015   29.69640199]\n",
      " [  87.32633325   56.39076137]\n",
      " [ 103.07498926   20.01025538]\n",
      " [  58.17828477   18.97231498]\n",
      " [  92.51397148   26.20359298]\n",
      " [  77.98898682   24.09748018]\n",
      " [  59.71987532   63.54782848]\n",
      " [  79.04170254   24.43464664]\n",
      " [ 112.14979102   17.12531108]\n",
      " [ 118.31656079   68.45914606]\n",
      " [  76.73761953   23.84567719]\n",
      " [ 124.94347412   67.75452047]\n",
      " [  55.95279043   32.80374625]\n",
      " [  26.10824939   22.8380015 ]\n",
      " [ 102.07411157   19.35068576]\n",
      " [  84.09818203   19.00266353]\n",
      " [ 169.94365186   53.66926434]\n",
      " [  64.46746743   26.53378412]\n",
      " [  63.01904023   62.44186412]\n",
      " [  64.39511797   59.33373649]\n",
      " [  62.68763225   56.55587848]\n",
      " [ 127.11576367   31.95466276]\n",
      " [  63.66275649   44.01996142]\n",
      " [ 107.86951025   17.99316902]\n",
      " [ 127.28779053   35.95935946]\n",
      " [  64.39498042   25.65025614]\n",
      " [ 123.60687354   30.0234303 ]\n",
      " [  92.49826636   34.52549171]\n",
      " [ 104.81810303   20.02786662]\n",
      " [  47.99787036   15.30577334]\n",
      " [  70.79831963   23.98268904]\n",
      " [  90.64371655   33.98101163]\n",
      " [  96.40826025   26.64431666]\n",
      " [  61.45823225   39.28814039]\n",
      " [  43.876195     24.11117675]\n",
      " [  74.70801309   26.10123743]\n",
      " [  91.37960986   28.71284257]\n",
      " [ 104.62596729   22.71236698]\n",
      " [  81.57164102   18.77882676]\n",
      " [ 154.98158105   45.73593812]\n",
      " [  80.43164761   37.33682562]\n",
      " [  82.93686172   24.34393637]\n",
      " [  39.98163853   24.06564918]\n",
      " [ 108.30787231   22.40542123]\n",
      " [ 104.56659814   22.21370586]\n",
      " [ 172.04897607   54.11813209]\n",
      " [ 140.86032129   30.93993219]\n",
      " [  65.9199415    23.71458912]\n",
      " [  35.53667778   52.39040504]\n",
      " [  76.92464233   25.95465744]\n",
      " [  98.66192334   19.12453808]\n",
      " [  94.94268384   38.02251283]\n",
      " [  84.15740918   39.20819733]\n",
      " [ 109.39128955   17.56445984]\n",
      " [  60.84415142   65.02758912]\n",
      " [  57.21427844   16.99145521]\n",
      " [  61.0858605    26.85370509]\n",
      " [  27.18888079   23.3078112 ]\n",
      " [  91.26856299   49.72551952]\n",
      " [  25.67154833   23.08137992]\n",
      " [ 108.56853345   49.41846195]\n",
      " [  58.67548591   23.41958274]\n",
      " [  84.4471145    37.05558649]\n",
      " [  45.14232827   21.29386452]\n",
      " [  22.13273123   22.11221187]\n",
      " [  93.63647646   36.24229745]\n",
      " [  64.05319141   28.06903471]\n",
      " [  68.2153209    35.80353439]\n",
      " [  91.54790552   52.37826796]\n",
      " [  97.2813374    32.88660292]\n",
      " [  24.24161511   19.21523519]\n",
      " [ 118.98429297   34.67583637]\n",
      " [  99.93021973   47.52067699]\n",
      " [  61.21413594   20.79326633]\n",
      " [  50.3300634    15.29554091]\n",
      " [  82.81522456   20.0329256 ]\n",
      " [ 110.48403467   30.29399562]\n",
      " [  92.75564697   53.07106461]\n",
      " [  60.85106877   18.46282421]\n",
      " [ 119.30149316   47.244415  ]\n",
      " [  92.57902979   52.1525537 ]\n",
      " [  99.17326782   28.74310596]\n",
      " [  71.31502642   42.64722548]\n",
      " [  95.39316382   27.662784  ]\n",
      " [  99.86686084   19.31251774]\n",
      " [  65.72591299   52.47334635]\n",
      " [  30.48345435   20.10496011]\n",
      " [  99.74042651   19.02417698]\n",
      " [  65.98427617   23.95496503]\n",
      " [  79.72861699   18.51086723]\n",
      " [  45.54907778   23.02326868]\n",
      " [  84.25174087   19.45450224]\n",
      " [  47.67464158   31.06687128]\n",
      " [ 137.91007959   36.98100272]\n",
      " [ 114.09097217   32.00137869]\n",
      " [  74.64420845   30.12533556]\n",
      " [  53.39914033   16.89162882]\n",
      " [  65.32888203   31.03947711]\n",
      " [  89.23937104   40.29552782]\n",
      " [  81.64946235   21.7436629 ]\n",
      " [  86.90035391   21.97381654]\n",
      " [  95.52661719   28.13530182]\n",
      " [  82.02836865   18.64728776]\n",
      " [ 118.10618286   41.26913744]\n",
      " [ 113.7328335    38.7766591 ]\n",
      " [  62.03612646   29.10803843]\n",
      " [  59.71938325   23.99982269]\n",
      " [  95.26011719   26.0524473 ]\n",
      " [ 100.64766748   47.54768825]\n",
      " [  63.17878052   58.04233134]\n",
      " [  63.06893303   41.61690006]\n",
      " [ 100.79043848   42.86711183]\n",
      " [  90.24418789   40.20356271]\n",
      " [  32.86694958   21.61927113]\n",
      " [  30.48820249   43.11659193]\n",
      " [ 115.47952783   71.14181339]\n",
      " [ 104.33957349   32.40703253]\n",
      " [  25.98572485   20.93705036]\n",
      " [ 106.17959375   39.29102013]\n",
      " [  19.17691205   42.31012216]\n",
      " [  68.68837788   62.60823287]\n",
      " [ 124.26141455   26.69929562]\n",
      " [  91.92790186   33.90340316]\n",
      " [ 103.04760181   40.25067133]\n",
      " [  47.62347354   23.693482  ]\n",
      " [ 122.18520923   28.02698884]\n",
      " [ 109.93594995   28.09545763]\n",
      " [ 119.14194702   22.53895237]\n",
      " [ 110.63024536   21.23338945]\n",
      " [  68.08007109   25.72328318]\n",
      " [  74.13240693   18.43210336]\n",
      " [  75.55697173   35.49655914]\n",
      " [  85.21089785   15.56401383]\n",
      " [  67.10254331   49.94059315]\n",
      " [  93.81412593   29.7854619 ]\n",
      " [ 143.77927246   43.48934567]\n",
      " [  53.87762786   14.06367551]\n",
      " [  67.09207583   27.71849048]\n",
      " [ 104.35392822   20.99600466]\n",
      " [ 109.29601538   35.02420414]\n",
      " [ 103.88233105   21.60800436]\n",
      " [  85.98289189   62.60029345]\n",
      " [  48.61325542   20.06124288]\n",
      " [  49.42642205   14.03873673]\n",
      " [  86.75424072   22.62932465]\n",
      " [  64.62035503   23.04647048]\n",
      " [  57.72567712   25.0144777 ]\n",
      " [  48.04732981   14.88588651]\n",
      " [ 137.37464746   32.26485006]\n",
      " [  31.20638096   20.89842394]\n",
      " [ 108.80675527   29.92685437]\n",
      " [ 119.21939307   50.5564632 ]\n",
      " [  79.1470522    34.05131151]\n",
      " [  87.29584155   42.44437665]\n",
      " [  41.6138134    17.20047975]\n",
      " [  84.2845916    18.52442725]\n",
      " [  77.02022085   25.82970459]\n",
      " [  91.71593604   48.14120722]\n",
      " [  64.19686807   62.99101185]\n",
      " [  84.07220347   64.28808068]\n",
      " [ 114.30611255   43.32560438]\n",
      " [ 125.92359717   30.50618582]\n",
      " [  98.97503174   27.26640935]\n",
      " [  96.03786377   25.9441572 ]\n",
      " [  98.57664229   25.62010974]\n",
      " [  65.0252001    21.47963342]\n",
      " [  87.15817285   52.35302211]\n",
      " [  57.55759648   17.91802968]\n",
      " [  84.49258682   18.88285726]\n",
      " [ 122.9384043    30.43676443]\n",
      " [  84.18180596   17.58661609]\n",
      " [  76.36045156   30.08004002]\n",
      " [  90.76315527   65.95525402]\n",
      " [  57.36870859   19.35169583]\n",
      " [  68.30309155   29.2432579 ]\n",
      " [  86.60877119   19.65997623]\n",
      " [  70.47974858   36.15098554]\n",
      " [  57.48825039   54.52737858]\n",
      " [ 131.55637598   28.00888572]\n",
      " [  52.07613774   19.98758312]\n",
      " [ 123.42432642   64.53667648]\n",
      " [  83.07189424   26.62623679]\n",
      " [  82.39084453   66.91203131]\n",
      " [  39.75393083   22.89612703]\n",
      " [  58.58300476   39.85919542]\n",
      " [  73.4903397    27.31360772]\n",
      " [  80.21076528   19.32737859]\n",
      " [ 123.68402881   68.57920763]\n",
      " [  89.33617114   62.92931542]\n",
      " [  97.10521689   27.95217184]\n",
      " [  97.99702148   35.26494874]\n",
      " [  90.98531294   25.71823572]]\n",
      "8.34109845289\n"
     ]
    }
   ],
   "source": [
    "b = (pre_Y1_t+pre_Y2_t+pre_Y3_t+pre_Y4_t+pre_Y5_t)/5\n",
    "print(b)\n",
    "mean_e, mean_a = accuracy(b, test_y)\n",
    "print(mean_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  6]\n",
      " [ 8 10]]\n"
     ]
    }
   ],
   "source": [
    "c=np.array([[2,2],[7,8]])\n",
    "d=np.array([[2,4],[1,2]])\n",
    "k=c+d\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.64381909306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py:526: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# stacking model 2\n",
    "from sklearn.svm import LinearSVR\n",
    "Lsvm = LinearSVR()\n",
    "Lsvm.fit(pre, reshaped(train_y))\n",
    "pred2 = Lsvm.predict(pre_t)\n",
    "e2, a2 = accuracy(pred2.reshape((226,2)), test_y)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.70886094388\n"
     ]
    }
   ],
   "source": [
    "# stacking model 3\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=4)\n",
    "neigh.fit(pre, reshaped(train_y))\n",
    "pred3 = neigh.predict(pre_t)\n",
    "e3, a3 = accuracy(pred3.reshape((226,2)), test_y)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.96807629376\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkjhLiISQgp3EZEQUriLiITQ/weACQ0TGwodwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18d14a208d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.median(e))\n",
    "error_sorted= np.sort(e)\n",
    "p = 1. *np.arange(len(e))/(len(e)-1)\n",
    "plt.plot(error_sorted, p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
