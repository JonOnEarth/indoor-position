{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import regularizers \n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import auto_regression as ar\n",
    "import regular_regression as rr\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "#test\n",
    "#load data\n",
    "path = 'C:/Users/mys12/Desktop/northeastern/summer2018/data_Robust_Fingerprinting-master/DISTRIBUTED_OPENSOURCE/FINGERPRINTING_DB'\n",
    "\n",
    "test_rss = pd.read_csv(path + '/Training_rss_21Aug17.csv', header = 0)\n",
    "test_coord = pd.read_csv(path + '/Training_coordinates_21Aug17.csv', header = 0)\n",
    "test_rss = test_rss.replace(100, 0)\n",
    "\n",
    "train_rss = pd.read_csv(path + '/Test_rss_21Aug17.csv', header = 0)\n",
    "train_coord = pd.read_csv(path + '/Test_coordinates_21Aug17.csv', header = 0)\n",
    "train_rss = train_rss.replace(100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "992\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([train_rss, train_coord], axis=1, ignore_index=True)\n",
    "test = pd.concat([test_rss, test_coord], axis=1, ignore_index=True)\n",
    "train = np.asarray(train)\n",
    "test = np.asarray(test)\n",
    "\n",
    "# first floor\n",
    "train1 = train[train[:,-1]==0.0]\n",
    "normalizer = preprocessing.Normalizer().fit(train1[:,:-3])\n",
    "train1_r=normalizer.transform(train1[:,:-3])\n",
    "train1_c=train1[:,-3:-1]\n",
    "print(train1_r.shape[1])\n",
    "\n",
    "test1 = test[test[:,-1]==0.0]\n",
    "test1_r=normalizer.transform(test1[:,:-3])\n",
    "test1_c=test1[:,-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata(rss, locations):\n",
    "    # the origin of the room\n",
    "    origin = np.amin(locations,axis=0)\n",
    "    #size of the room\n",
    "    room_size = np.amax(locations, axis=0)-origin\n",
    "    # position respect to origin\n",
    "    train_Yy = locations - origin\n",
    "    train_Xx = np.asarray(rss, dtype=np.float64)\n",
    "    return train_Xx, train_Yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_val(rss, locations):\n",
    "#     train_Xx, train_Yy = predata(rss, locations)\n",
    "#     train_x, val_x, train_y, val_y = train_test_split(train_Xx, train_Yy, test_size=0.2)\n",
    "#     return train_x, val_x, train_y, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    error = np.sqrt(np.sum((predictions - labels)**2, 1))\n",
    "    return error, np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = predata(train1_r, train1_c)\n",
    "testX, testY = predata(test1_r, test1_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=rr.train_model\n",
    "clf2=ar.regression\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors=4)\n",
    "clf3 = neigh.fit\n",
    "\n",
    "dr = DecisionTreeRegressor(max_depth=7)\n",
    "clf4 = dr.fit\n",
    "\n",
    "er = ExtraTreeRegressor(max_depth=7)\n",
    "clf5 = er.fit\n",
    "\n",
    "clfs = [clf1,clf2,clf3,clf4,clf5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=2018)\n",
    "\n",
    "#               a = np.zeros(shape=(5,4,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshaped(predict):\n",
    "    size = predict.shape[0]\n",
    "    j = predict.reshape((2*size, 1))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _BaseKFold.split at 0x0000028E565143B8>\n"
     ]
    }
   ],
   "source": [
    "print(skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, X_train, y_train, X_test):\n",
    "#     X_train = reshaped(X_train)\n",
    "#     y_train = reshaped(y_train)\n",
    "#     X_test = reshaped(X_test)\n",
    "\n",
    "    blend_train = np.zeros((y_train.shape[0],2))\n",
    "    blend_test = np.zeros((X_test.shape[0],2))\n",
    "    blend_test_skf = np.zeros((X_test.shape[0],2,5)) \n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(list(kf.split(X_train))):\n",
    "        print(\"Fold\", i)   \n",
    "\n",
    "        kf_X_train = X_train[train_index]\n",
    "        kf_y_train = y_train[train_index]\n",
    "        kf_X_test = X_train[test_index]\n",
    "        kf_y_test = y_train[test_index]\n",
    "        \n",
    "        model = clf(kf_X_train,kf_y_train)\n",
    "        \n",
    "        blend_train[test_index]=model.predict(kf_X_test)  # 992*2\n",
    "        \n",
    "        blend_test_skf[:,:,i] = model.predict(X_test)   # 1*292*2\n",
    "    \n",
    "    blend_test[:,:]=blend_test_skf.mean(axis=2)\n",
    "    return blend_train, blend_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n"
     ]
    }
   ],
   "source": [
    "blend_train3, blend_test3 = get_oof(clf3, trainX, trainY, testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 103.203   ,   37.66458 ],\n",
       "       [  82.40365 ,   27.53373 ],\n",
       "       [ 114.986   ,   37.99638 ],\n",
       "       [ 132.448   ,   28.57548 ],\n",
       "       [  55.4249  ,   15.64583 ],\n",
       "       [  94.796   ,   53.47178 ],\n",
       "       [  43.45965 ,   19.14968 ],\n",
       "       [ 105.108   ,   24.80828 ],\n",
       "       [  53.98    ,   16.07823 ],\n",
       "       [  66.5732  ,   31.89973 ],\n",
       "       [  77.25165 ,   22.00058 ],\n",
       "       [  69.73575 ,   24.38083 ],\n",
       "       [  67.9996  ,   23.37908 ],\n",
       "       [  53.8442  ,   25.46698 ],\n",
       "       [ 132.      ,   38.07608 ],\n",
       "       [ 119.89    ,   69.55053 ],\n",
       "       [ 133.158   ,   46.06573 ],\n",
       "       [  95.0293  ,   35.93773 ],\n",
       "       [ 139.5045  ,   28.26263 ],\n",
       "       [  23.47205 ,   24.09053 ],\n",
       "       [  59.3365  ,   38.46588 ],\n",
       "       [  80.73075 ,   18.97433 ],\n",
       "       [ 107.9495  ,   18.32258 ],\n",
       "       [  93.3715  ,   62.86078 ],\n",
       "       [ 121.742   ,   66.81118 ],\n",
       "       [  50.91115 ,   24.21203 ],\n",
       "       [  53.66365 ,   16.965875],\n",
       "       [  99.4035  ,   37.05533 ],\n",
       "       [  55.6733  ,   15.04993 ],\n",
       "       [  80.44565 ,   35.13448 ],\n",
       "       [ 105.946   ,   30.64963 ],\n",
       "       [  95.5035  ,   56.77608 ],\n",
       "       [ 140.1745  ,   32.65023 ],\n",
       "       [  86.0861  ,   28.47718 ],\n",
       "       [  97.045   ,   25.55613 ],\n",
       "       [  91.31    ,   67.11598 ],\n",
       "       [ 108.192   ,   19.92863 ],\n",
       "       [  59.18855 ,   16.72813 ],\n",
       "       [  93.6258  ,   28.65898 ],\n",
       "       [  87.147   ,   28.23988 ],\n",
       "       [  58.86105 ,   67.00298 ],\n",
       "       [  83.4294  ,   24.40193 ],\n",
       "       [ 119.0515  ,   15.85505 ],\n",
       "       [ 120.592   ,   70.11523 ],\n",
       "       [  75.74265 ,   24.92688 ],\n",
       "       [ 120.339   ,   67.93643 ],\n",
       "       [  46.0224  ,   29.83498 ],\n",
       "       [  23.98045 ,   21.73963 ],\n",
       "       [ 107.3665  ,   19.71243 ],\n",
       "       [  83.81935 ,   16.94948 ],\n",
       "       [ 167.0805  ,   53.69358 ],\n",
       "       [  66.8767  ,   29.01123 ],\n",
       "       [  61.42175 ,   61.13543 ],\n",
       "       [  61.66475 ,   60.82213 ],\n",
       "       [  65.4572  ,   52.66523 ],\n",
       "       [ 139.192   ,   37.27838 ],\n",
       "       [  64.6813  ,   39.37738 ],\n",
       "       [ 111.7715  ,   16.23503 ],\n",
       "       [ 130.1085  ,   37.81398 ],\n",
       "       [  64.05415 ,   25.52198 ],\n",
       "       [ 120.5695  ,   29.65138 ],\n",
       "       [  92.535   ,   37.88638 ],\n",
       "       [ 108.1535  ,   22.96703 ],\n",
       "       [  53.7186  ,   15.80243 ],\n",
       "       [  72.8145  ,   24.35993 ],\n",
       "       [  92.702   ,   35.20523 ],\n",
       "       [  96.45135 ,   25.81983 ],\n",
       "       [  66.3013  ,   38.90863 ],\n",
       "       [  38.1808  ,   25.15393 ],\n",
       "       [  77.8025  ,   27.52133 ],\n",
       "       [  92.628   ,   29.67583 ],\n",
       "       [  92.885   ,   31.86118 ],\n",
       "       [  81.81255 ,   18.19063 ],\n",
       "       [ 148.986   ,   46.05153 ],\n",
       "       [  83.261   ,   37.74878 ],\n",
       "       [  80.5948  ,   22.71338 ],\n",
       "       [  31.8318  ,   23.78293 ],\n",
       "       [ 111.3245  ,   21.88668 ],\n",
       "       [ 103.239   ,   22.07788 ],\n",
       "       [ 171.2365  ,   55.26523 ],\n",
       "       [ 136.961   ,   26.93518 ],\n",
       "       [  73.8446  ,   26.92423 ],\n",
       "       [  33.4953  ,   52.95548 ],\n",
       "       [  77.7311  ,   27.26073 ],\n",
       "       [ 105.141   ,   21.40898 ],\n",
       "       [  97.868   ,   33.80928 ],\n",
       "       [  96.623   ,   44.91438 ],\n",
       "       [ 111.51    ,   15.04548 ],\n",
       "       [  59.5124  ,   67.85333 ],\n",
       "       [  56.6484  ,   17.29233 ],\n",
       "       [  64.33085 ,   30.72638 ],\n",
       "       [  25.3268  ,   22.67198 ],\n",
       "       [  96.352   ,   55.47383 ],\n",
       "       [  25.3268  ,   22.67198 ],\n",
       "       [ 104.69    ,   42.10408 ],\n",
       "       [  59.4638  ,   24.70708 ],\n",
       "       [  83.90035 ,   41.45273 ],\n",
       "       [  48.8514  ,   23.80433 ],\n",
       "       [  23.13415 ,   21.81743 ],\n",
       "       [  94.4414  ,   35.95873 ],\n",
       "       [  63.9793  ,   28.67143 ],\n",
       "       [  66.4     ,   39.79603 ],\n",
       "       [  95.873   ,   56.07508 ],\n",
       "       [  99.002   ,   38.67548 ],\n",
       "       [  17.9082  ,   21.06398 ],\n",
       "       [ 127.1265  ,   33.64188 ],\n",
       "       [  98.455   ,   48.98038 ],\n",
       "       [  62.05605 ,   22.03193 ],\n",
       "       [  51.56295 ,   16.10623 ],\n",
       "       [  82.4658  ,   20.58298 ],\n",
       "       [ 112.9515  ,   34.27363 ],\n",
       "       [  93.825   ,   55.66258 ],\n",
       "       [  64.32245 ,   21.06558 ],\n",
       "       [ 117.155   ,   45.70738 ],\n",
       "       [  95.7488  ,   53.14778 ],\n",
       "       [ 113.2775  ,   31.91173 ],\n",
       "       [  75.1012  ,   40.64918 ],\n",
       "       [  98.671   ,   29.16458 ],\n",
       "       [ 105.911   ,   17.42573 ],\n",
       "       [  63.4082  ,   57.69708 ],\n",
       "       [  28.99925 ,   22.60738 ],\n",
       "       [ 105.684   ,   18.39478 ],\n",
       "       [  65.42295 ,   25.84623 ],\n",
       "       [  81.8362  ,   18.96468 ],\n",
       "       [  43.91705 ,   24.43593 ],\n",
       "       [  86.1076  ,   20.43898 ],\n",
       "       [  41.753   ,   26.70818 ],\n",
       "       [ 130.543   ,   40.16788 ],\n",
       "       [ 114.038   ,   36.12573 ],\n",
       "       [  78.45655 ,   32.68833 ],\n",
       "       [  54.62155 ,   16.68968 ],\n",
       "       [  69.76985 ,   35.20798 ],\n",
       "       [  89.7997  ,   37.44448 ],\n",
       "       [  80.8542  ,   22.93918 ],\n",
       "       [  86.64665 ,   22.64303 ],\n",
       "       [  96.565   ,   30.30523 ],\n",
       "       [  83.4736  ,   17.93908 ],\n",
       "       [ 111.388   ,   38.03468 ],\n",
       "       [ 111.918   ,   33.71298 ],\n",
       "       [  64.7795  ,   33.02998 ],\n",
       "       [  59.31085 ,   23.99758 ],\n",
       "       [  94.289   ,   25.03413 ],\n",
       "       [  99.701   ,   48.65388 ],\n",
       "       [  62.41535 ,   59.19318 ],\n",
       "       [  62.5286  ,   38.24408 ],\n",
       "       [ 101.295   ,   43.18628 ],\n",
       "       [  93.06505 ,   42.81338 ],\n",
       "       [  22.3881  ,   22.94128 ],\n",
       "       [  33.7244  ,   50.97368 ],\n",
       "       [ 119.44    ,   71.03298 ],\n",
       "       [ 105.5905  ,   33.13653 ],\n",
       "       [  22.6809  ,   21.63213 ],\n",
       "       [ 116.677   ,   38.61593 ],\n",
       "       [  10.70205 ,   49.01038 ],\n",
       "       [  59.1662  ,   66.19978 ],\n",
       "       [ 119.6175  ,   29.58103 ],\n",
       "       [  92.494   ,   32.89208 ],\n",
       "       [  97.7365  ,   38.22018 ],\n",
       "       [  43.62525 ,   24.39553 ],\n",
       "       [ 118.3835  ,   33.73153 ],\n",
       "       [ 110.2455  ,   28.55383 ],\n",
       "       [ 126.6745  ,   21.82698 ],\n",
       "       [ 113.2975  ,   16.42043 ],\n",
       "       [  74.92555 ,   27.35808 ],\n",
       "       [  84.36025 ,   21.99188 ],\n",
       "       [  82.91015 ,   33.59333 ],\n",
       "       [  86.86375 ,   14.74639 ],\n",
       "       [  64.3843  ,   55.53938 ],\n",
       "       [  91.2497  ,   32.01518 ],\n",
       "       [ 144.5115  ,   44.15473 ],\n",
       "       [  53.21995 ,   14.84793 ],\n",
       "       [  67.9542  ,   26.25378 ],\n",
       "       [ 112.2675  ,   17.86063 ],\n",
       "       [ 110.807   ,   33.00238 ],\n",
       "       [ 103.905   ,   21.23128 ],\n",
       "       [  90.3683  ,   68.59468 ],\n",
       "       [  50.96725 ,   20.15513 ],\n",
       "       [  51.56605 ,   14.95048 ],\n",
       "       [  85.91775 ,   19.83553 ],\n",
       "       [  69.17075 ,   25.80438 ],\n",
       "       [  57.11145 ,   25.56808 ],\n",
       "       [  51.81145 ,   14.65778 ],\n",
       "       [ 140.141   ,   26.74803 ],\n",
       "       [  21.2339  ,   22.21688 ],\n",
       "       [ 102.8534  ,   29.45778 ],\n",
       "       [ 124.216   ,   45.18743 ],\n",
       "       [  83.0242  ,   32.74058 ],\n",
       "       [  99.117   ,   44.08128 ],\n",
       "       [  46.20455 ,   17.79895 ],\n",
       "       [  81.437   ,   20.63738 ],\n",
       "       [  72.8377  ,   26.46088 ],\n",
       "       [  91.9245  ,   61.15983 ],\n",
       "       [  59.2982  ,   67.50788 ],\n",
       "       [  90.4316  ,   68.13193 ],\n",
       "       [ 119.2625  ,   42.13023 ],\n",
       "       [ 122.296   ,   29.88188 ],\n",
       "       [ 103.4365  ,   29.20103 ],\n",
       "       [  94.289   ,   25.03413 ],\n",
       "       [  99.1482  ,   29.65538 ],\n",
       "       [  61.4386  ,   22.38018 ],\n",
       "       [  93.1145  ,   57.14578 ],\n",
       "       [  55.9602  ,   17.34358 ],\n",
       "       [  85.5945  ,   18.38433 ],\n",
       "       [ 122.318   ,   29.87048 ],\n",
       "       [  82.6736  ,   20.26928 ],\n",
       "       [  79.49675 ,   32.30193 ],\n",
       "       [  91.5585  ,   66.45903 ],\n",
       "       [  60.2948  ,   18.87308 ],\n",
       "       [  66.907   ,   28.55158 ],\n",
       "       [  83.50605 ,   20.60228 ],\n",
       "       [  72.4497  ,   39.50318 ],\n",
       "       [  61.77735 ,   60.62503 ],\n",
       "       [ 132.351   ,   27.52883 ],\n",
       "       [  51.30545 ,   16.42518 ],\n",
       "       [ 120.923   ,   66.09168 ],\n",
       "       [  86.7462  ,   27.84933 ],\n",
       "       [  90.40345 ,   67.69078 ],\n",
       "       [  39.5354  ,   24.29188 ],\n",
       "       [  58.0907  ,   41.98333 ],\n",
       "       [  73.2407  ,   32.74508 ],\n",
       "       [  81.10355 ,   16.51768 ],\n",
       "       [ 120.263   ,   68.37703 ],\n",
       "       [  92.301   ,   65.01288 ],\n",
       "       [ 100.10025 ,   29.43583 ],\n",
       "       [  95.81445 ,   36.44608 ],\n",
       "       [  86.65475 ,   18.96673 ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4720.9427 - val_loss: 4854.8840\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4537.2215 - val_loss: 4520.7934\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3997.5022 - val_loss: 3701.6850\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 2940.0416 - val_loss: 2354.2699\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1659.5264 - val_loss: 1223.8050\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1000.6096 - val_loss: 840.0736\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 739.7006 - val_loss: 650.4992\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 598.8961 - val_loss: 525.4996\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 473.2950 - val_loss: 440.7734\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 399.7910 - val_loss: 384.2068\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 360.5106 - val_loss: 349.9741\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 325.3376 - val_loss: 323.4339\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 313.4754 - val_loss: 298.1650\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 301.4124 - val_loss: 281.3495\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 286.7235 - val_loss: 267.3686\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 272.9166 - val_loss: 253.8134\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 253.6845 - val_loss: 243.9386\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 263.3650 - val_loss: 235.6474: 266.4\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 236.0079 - val_loss: 227.1921\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 231.1214 - val_loss: 216.8546\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 227.0330 - val_loss: 210.2826\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 223.9712 - val_loss: 204.9864\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 215.5504 - val_loss: 192.3938\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 211.7536 - val_loss: 183.3725\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 196.4465 - val_loss: 173.5078\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 196.0230 - val_loss: 168.1485\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 190.4251 - val_loss: 161.8393\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 178.1665 - val_loss: 153.1869\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 169.9230 - val_loss: 148.6553\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 168.8683 - val_loss: 146.9125\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 163.6670 - val_loss: 139.6729\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 151.8010 - val_loss: 139.8442\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 155.6097 - val_loss: 133.9552\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 151.4409 - val_loss: 132.4684\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 161.9097 - val_loss: 129.8383\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 152.7436 - val_loss: 128.8599\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 148.6439 - val_loss: 130.7942\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 147.4325 - val_loss: 124.8777\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 137.5099 - val_loss: 124.4556\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 147.0016 - val_loss: 121.4370\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 135.8907 - val_loss: 122.2592\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 139.1858 - val_loss: 119.5351\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 135.4150 - val_loss: 117.5397\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 136.3012 - val_loss: 116.4032\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 130.4919 - val_loss: 117.9025\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 140.2000 - val_loss: 114.1658\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 130.2588 - val_loss: 115.9487\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 136.0709 - val_loss: 110.1980\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 144.8546 - val_loss: 114.2100\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 131.3087 - val_loss: 109.7655\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 131.4901 - val_loss: 109.9184\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 133.2622 - val_loss: 110.9050\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 126.0907 - val_loss: 110.1930\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 128.8461 - val_loss: 107.7096\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 127.6174 - val_loss: 109.5239\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 123.4822 - val_loss: 107.6501\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 124.5974 - val_loss: 107.6523\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 121.3637 - val_loss: 107.2095\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 119.1968 - val_loss: 103.4255\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 121.3354 - val_loss: 102.6608\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 121.6943 - val_loss: 102.8772\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 117.3924 - val_loss: 101.1436\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 117.8371 - val_loss: 99.2034\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 116.4919 - val_loss: 103.5170\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 118.8363 - val_loss: 99.5095\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 119.6107 - val_loss: 101.0603\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 113.6114 - val_loss: 98.7112\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 118.4766 - val_loss: 100.4701\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 114.0349 - val_loss: 97.4662\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 110.4832 - val_loss: 98.9406\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 114.2837 - val_loss: 95.2267\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 114.7327 - val_loss: 98.4895\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 118.8666 - val_loss: 95.3924\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 109.6329 - val_loss: 98.5478\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 112.0110 - val_loss: 94.2212\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 113.2426 - val_loss: 96.4516\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 110.8775 - val_loss: 94.2649\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 108.0785 - val_loss: 93.8269\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 106.5579 - val_loss: 92.6851\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 107.3600 - val_loss: 92.4810\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 107.1522 - val_loss: 96.0505\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 107.7267 - val_loss: 91.3162\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 103.1904 - val_loss: 91.0009\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 103.7105 - val_loss: 96.1597\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 108.2791 - val_loss: 92.3248\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 107.4992 - val_loss: 95.0575\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 105.2952 - val_loss: 92.9302\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 105.8626 - val_loss: 89.8586\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 100.1486 - val_loss: 92.8600\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.5679 - val_loss: 88.7157\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 104.1572 - val_loss: 92.3680\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 100.0583 - val_loss: 88.3556\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 107.9785 - val_loss: 93.4719\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 105.3802 - val_loss: 87.5249\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 111.0718 - val_loss: 97.3390\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 103.3606 - val_loss: 88.6592\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 102.6662 - val_loss: 92.1731\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 103.2162 - val_loss: 86.5068\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 99.1170 - val_loss: 90.0691\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 99.2281 - val_loss: 84.6729\n",
      "Fold 1\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4743.4105 - val_loss: 4445.0871\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4552.1374 - val_loss: 4130.1292\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4034.6206 - val_loss: 3381.1396\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3003.3457 - val_loss: 2179.8563\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1732.5948 - val_loss: 1195.6664\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1036.0496 - val_loss: 862.8870\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 806.8556 - val_loss: 648.7956\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 615.2726 - val_loss: 514.4202\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 511.9919 - val_loss: 409.5030\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 434.8438 - val_loss: 339.4404\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 390.4943 - val_loss: 297.6774\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 359.5108 - val_loss: 268.5580\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 323.7022 - val_loss: 252.7530\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 310.0457 - val_loss: 237.0632\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 298.3369 - val_loss: 225.5470\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 276.2686 - val_loss: 217.4101\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 271.2340 - val_loss: 207.1492\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 258.8865 - val_loss: 201.9313\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 256.1289 - val_loss: 191.6414\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 245.0933 - val_loss: 180.6200\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 226.1877 - val_loss: 171.2845\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 226.2445 - val_loss: 164.6323\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 216.0731 - val_loss: 154.5178\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 212.1989 - val_loss: 148.6496\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 197.6370 - val_loss: 143.4305\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 183.1726 - val_loss: 137.3079\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 187.8080 - val_loss: 132.0794\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 172.8311 - val_loss: 129.3901\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 175.2076 - val_loss: 127.7279\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 167.6314 - val_loss: 122.7721\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 160.8206 - val_loss: 122.6503\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 159.2279 - val_loss: 120.7602\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 165.0841 - val_loss: 119.3230\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 160.2600 - val_loss: 115.9337: 157.559\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 151.7186 - val_loss: 116.0513\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 161.3621 - val_loss: 113.6718\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 151.4330 - val_loss: 112.8562\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 143.8145 - val_loss: 111.4829\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 152.9082 - val_loss: 110.6547\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 143.3540 - val_loss: 110.6096\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 142.6550 - val_loss: 109.1847\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 139.4738 - val_loss: 107.9946\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 139.7184 - val_loss: 107.1263\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 140.7111 - val_loss: 108.5956\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 139.4230 - val_loss: 107.4807\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 134.3851 - val_loss: 107.1117\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 138.9062 - val_loss: 105.1307\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 137.0606 - val_loss: 104.0843\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 130.5084 - val_loss: 101.3604\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 137.1538 - val_loss: 103.2521\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 127.1100 - val_loss: 100.7127\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 130.8664 - val_loss: 103.0219\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 125.8836 - val_loss: 102.4986\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 137.0160 - val_loss: 102.6270\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 128.2643 - val_loss: 102.0915\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 126.8573 - val_loss: 100.8556\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 130.1714 - val_loss: 99.0882\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 124.8722 - val_loss: 100.7729\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 125.0255 - val_loss: 97.3893\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 122.7423 - val_loss: 96.0871\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 126.9121 - val_loss: 99.2488\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 129.1842 - val_loss: 99.3962\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 123.0217 - val_loss: 97.1916\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 123.4979 - val_loss: 96.1519\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 122.0237 - val_loss: 96.9798\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 116.3714 - val_loss: 97.4343\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 124.2844 - val_loss: 94.7577\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 118.6304 - val_loss: 95.1470\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 114.5665 - val_loss: 93.2907\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 116.0568 - val_loss: 94.9008\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 120.1667 - val_loss: 92.5938\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 111.4043 - val_loss: 91.2555\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 117.4913 - val_loss: 93.5489\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 124.2968 - val_loss: 94.3705\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 112.9632 - val_loss: 92.9898\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 117.3716 - val_loss: 92.4478\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 111.1828 - val_loss: 94.1078\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 111.0539 - val_loss: 92.4118\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 111.1047 - val_loss: 89.4354\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 108.3438 - val_loss: 92.0280\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 110.5653 - val_loss: 91.9231\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 110.7305 - val_loss: 92.4434\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 117.7407 - val_loss: 92.6260\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 107.6867 - val_loss: 91.1002\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 111.8986 - val_loss: 89.4560\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 106.6303 - val_loss: 90.0380\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 106.8590 - val_loss: 90.9500\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 107.6414 - val_loss: 90.6149\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 108.7908 - val_loss: 92.3896\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 106.7977 - val_loss: 89.7530\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 105.8953 - val_loss: 90.0021\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 108.9364 - val_loss: 88.5356\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 105.7823 - val_loss: 88.9698\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 98.3485 - val_loss: 89.3446\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 107.3708 - val_loss: 88.6629\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 101.5642 - val_loss: 88.8748\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 105.0071 - val_loss: 92.6747\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 106.9098 - val_loss: 89.3725\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 113.5122 - val_loss: 88.1424\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 101.2528 - val_loss: 86.0799\n",
      "Fold 2\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4566.2777 - val_loss: 4752.2706\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4401.6252 - val_loss: 4452.8010\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3946.2354 - val_loss: 3716.3966\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3015.9588 - val_loss: 2490.4316\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1852.1469 - val_loss: 1385.7818\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1137.7215 - val_loss: 865.1783\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 798.1379 - val_loss: 613.1461\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 619.0856 - val_loss: 481.8107\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 511.5082 - val_loss: 394.3466\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 441.1353 - val_loss: 344.2449\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 367.3731 - val_loss: 313.6699\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 349.8598 - val_loss: 293.1415\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 334.8047 - val_loss: 276.2816\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 308.6641 - val_loss: 265.3284\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 280.8307 - val_loss: 255.1224\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 268.1333 - val_loss: 244.0740\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 266.4023 - val_loss: 239.3198\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 252.2092 - val_loss: 229.0495\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 249.9926 - val_loss: 222.2889\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 244.2917 - val_loss: 210.1376\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 225.1195 - val_loss: 198.9490\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 216.1814 - val_loss: 189.6758\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 210.5269 - val_loss: 181.3673\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 199.6701 - val_loss: 169.4447\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 190.7759 - val_loss: 164.5737\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 177.6734 - val_loss: 157.9258\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 184.7884 - val_loss: 154.3720\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 173.2587 - val_loss: 148.2409\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 168.3218 - val_loss: 140.1810\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 166.5673 - val_loss: 139.4736\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 162.9163 - val_loss: 140.2100\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 161.3720 - val_loss: 137.1882\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 159.7494 - val_loss: 134.7703\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 153.9915 - val_loss: 132.7635\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 148.9034 - val_loss: 129.2709\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 153.7015 - val_loss: 131.0751\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 149.4285 - val_loss: 128.4963\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 143.4613 - val_loss: 128.7010\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 138.7224 - val_loss: 124.4373\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 150.5617 - val_loss: 125.8498\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 143.8846 - val_loss: 123.8585: 137.85\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 144.1533 - val_loss: 120.8924\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 141.8859 - val_loss: 123.8819\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 133.9498 - val_loss: 118.6274\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 134.9534 - val_loss: 118.8265\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 134.5394 - val_loss: 121.7416\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 132.9949 - val_loss: 117.7269\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 132.0663 - val_loss: 116.9798\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 128.4243 - val_loss: 115.7322\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 130.6644 - val_loss: 119.3284\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 131.8768 - val_loss: 114.8841\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 133.7281 - val_loss: 115.5257\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 130.4717 - val_loss: 108.3963\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 131.3015 - val_loss: 109.7703\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 131.2911 - val_loss: 113.2197\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 122.7170 - val_loss: 109.1612\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 125.9450 - val_loss: 106.3757\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 127.9605 - val_loss: 107.6943\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 124.7692 - val_loss: 106.8911\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 122.1492 - val_loss: 106.8333\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 123.3275 - val_loss: 104.6684\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 119.4248 - val_loss: 105.9776\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 117.1165 - val_loss: 104.5401\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 114.9466 - val_loss: 105.0491\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 117.1260 - val_loss: 104.2096\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 119.4459 - val_loss: 106.0646\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 116.2173 - val_loss: 103.9505\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 115.0998 - val_loss: 105.6563\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 113.6191 - val_loss: 105.5821\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 118.6510 - val_loss: 103.3072\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 114.7721 - val_loss: 103.4019\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 114.0465 - val_loss: 100.8986\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 106.6623 - val_loss: 100.8773\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 113.7202 - val_loss: 96.7277\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 113.1290 - val_loss: 99.5964\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 116.7545 - val_loss: 97.6316\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 111.3619 - val_loss: 99.1063\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 113.2416 - val_loss: 94.5664\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 113.1307 - val_loss: 100.2099\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 113.1133 - val_loss: 97.0051\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 113.8727 - val_loss: 96.2449\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 108.5759 - val_loss: 97.9369\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 112.4739 - val_loss: 93.7657\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 111.1162 - val_loss: 95.5826\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 107.3716 - val_loss: 92.2835\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 108.0115 - val_loss: 97.9614\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 112.0845 - val_loss: 95.2356\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 107.1335 - val_loss: 92.8180\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 99.8858 - val_loss: 94.3222\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 104.6064 - val_loss: 91.8136\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 108.0001 - val_loss: 95.6367\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 105.5088 - val_loss: 91.0338\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 96.6995 - val_loss: 91.2895\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 103.0660 - val_loss: 90.1047\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 101.5775 - val_loss: 87.6787\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 107.3142 - val_loss: 91.3715\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 102.8247 - val_loss: 87.1471\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 108.0073 - val_loss: 88.9866\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 102.5463 - val_loss: 86.5442\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 108.8823 - val_loss: 90.6548\n",
      "Fold 3\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4590.2500 - val_loss: 4673.1948\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4378.6899 - val_loss: 4296.5934\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3814.8459 - val_loss: 3436.7441\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 2769.4562 - val_loss: 2130.7494\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1605.8081 - val_loss: 1146.0200\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1022.5102 - val_loss: 762.1731\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 736.3280 - val_loss: 568.6548\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 580.6383 - val_loss: 453.2327\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 475.4258 - val_loss: 377.5301\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 420.1920 - val_loss: 332.1569\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 372.6995 - val_loss: 299.3579\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 345.0839 - val_loss: 273.9180\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 328.3201 - val_loss: 258.2008\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 293.2614 - val_loss: 245.6827\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 290.6447 - val_loss: 238.3157\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 267.3654 - val_loss: 226.9721\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 262.4931 - val_loss: 216.6178\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 254.9135 - val_loss: 207.2667\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 237.0764 - val_loss: 201.8565\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 230.4293 - val_loss: 191.5601\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 229.4645 - val_loss: 178.7508\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 207.5843 - val_loss: 170.3576\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 195.3704 - val_loss: 159.5448\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 197.4568 - val_loss: 149.8479\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 185.9954 - val_loss: 142.7193\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 174.7198 - val_loss: 133.1564\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 170.3070 - val_loss: 129.4102\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 165.3429 - val_loss: 125.2010\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 158.7071 - val_loss: 120.6424\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 164.6223 - val_loss: 123.4957\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 159.0243 - val_loss: 116.8860\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 154.2310 - val_loss: 115.2507\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 141.4719 - val_loss: 113.6235\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 150.6515 - val_loss: 112.5313\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 148.6954 - val_loss: 111.0809\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 141.7868 - val_loss: 111.1362\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 141.3555 - val_loss: 109.4133\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 136.5581 - val_loss: 109.1526\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 141.5815 - val_loss: 107.3164\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 139.7106 - val_loss: 104.9449\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 133.0158 - val_loss: 106.3612\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 133.8176 - val_loss: 104.9567\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 134.6508 - val_loss: 102.5822\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 132.1330 - val_loss: 103.8534\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 134.0181 - val_loss: 102.0546\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 130.2104 - val_loss: 103.0615\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 131.2345 - val_loss: 100.0790\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 133.4616 - val_loss: 102.3278\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 128.6733 - val_loss: 98.0340\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 129.5728 - val_loss: 100.3869\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 121.0939 - val_loss: 99.0596\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 125.3926 - val_loss: 98.0733\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 125.7542 - val_loss: 96.0292\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 118.7853 - val_loss: 96.5025\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 125.7720 - val_loss: 95.4076\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 125.5369 - val_loss: 96.3580\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 121.2332 - val_loss: 96.0334\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 123.6922 - val_loss: 94.8891\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 116.0691 - val_loss: 93.9305\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 114.5977 - val_loss: 95.6340\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 118.3442 - val_loss: 91.9065\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 114.5786 - val_loss: 92.7315\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 112.9975 - val_loss: 91.2863\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 119.1465 - val_loss: 93.6401\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 119.2961 - val_loss: 89.5899\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 118.0171 - val_loss: 89.4365\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 111.1667 - val_loss: 89.1020\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 111.0031 - val_loss: 87.9905\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 109.8697 - val_loss: 88.3960\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 105.6173 - val_loss: 89.6639\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 111.8112 - val_loss: 88.3716\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 109.0387 - val_loss: 88.7370\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 108.7896 - val_loss: 86.7495\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 108.8582 - val_loss: 87.0573\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 113.6354 - val_loss: 86.2139\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 110.7611 - val_loss: 84.8029\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 103.0735 - val_loss: 84.9225\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 110.9418 - val_loss: 86.0495\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 106.8357 - val_loss: 83.6309\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 103.8129 - val_loss: 83.5585\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 98.8951 - val_loss: 83.4630\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 110.0709 - val_loss: 81.9652\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 105.1131 - val_loss: 84.5609\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 104.9266 - val_loss: 81.4510\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 105.6543 - val_loss: 81.7061\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 107.2068 - val_loss: 84.2822\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 105.3184 - val_loss: 80.1363\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 98.6206 - val_loss: 82.7718\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 104.3403 - val_loss: 81.6329\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 97.5115 - val_loss: 80.8383\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 102.2460 - val_loss: 81.7669\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 106.4620 - val_loss: 80.4827\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 101.2292 - val_loss: 78.8452\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 103.7493 - val_loss: 80.8933\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 107.7717 - val_loss: 80.1358\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 101.4791 - val_loss: 81.9912\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 98.8965 - val_loss: 78.0626\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 107.2504 - val_loss: 77.6991\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 100.2758 - val_loss: 79.6181\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 100.4921 - val_loss: 78.2139\n",
      "Fold 4\n",
      "Train on 809 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 0s - loss: 4623.5666 - val_loss: 4630.2732\n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 4430.5509 - val_loss: 4298.4139\n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 3888.6660 - val_loss: 3492.7505\n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 2822.1321 - val_loss: 2194.7790\n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 1546.1264 - val_loss: 1160.7760\n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 936.8373 - val_loss: 833.0999\n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 732.9213 - val_loss: 631.3118\n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 573.8105 - val_loss: 507.7204\n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 461.5750 - val_loss: 409.4775\n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 402.8043 - val_loss: 351.5707\n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 363.3118 - val_loss: 312.8758\n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 333.7157 - val_loss: 283.8807\n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 309.5559 - val_loss: 265.6875\n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 293.5547 - val_loss: 250.8550\n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 285.9659 - val_loss: 241.3242\n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 271.5298 - val_loss: 230.4844\n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 270.9373 - val_loss: 221.3166\n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 251.2013 - val_loss: 214.8039\n",
      "Epoch 19/100\n",
      "809/809 [==============================] - 0s - loss: 255.2812 - val_loss: 204.4853\n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 230.7056 - val_loss: 198.6726\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809/809 [==============================] - 0s - loss: 227.6529 - val_loss: 187.8167\n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 220.2503 - val_loss: 180.3324\n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 209.8221 - val_loss: 169.9666\n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 203.5893 - val_loss: 161.8475\n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 178.1809 - val_loss: 155.5143\n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 179.0098 - val_loss: 146.2018\n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 177.0029 - val_loss: 139.2833\n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 164.4768 - val_loss: 133.3987\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 159.0642 - val_loss: 131.8468\n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 158.2622 - val_loss: 128.5556\n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 156.1072 - val_loss: 126.1971\n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 150.4767 - val_loss: 123.0987\n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 158.5344 - val_loss: 121.6838\n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 143.2455 - val_loss: 120.1545\n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 142.7442 - val_loss: 116.7875\n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 146.2185 - val_loss: 117.0788\n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 150.1178 - val_loss: 116.9256: 161\n",
      "Epoch 38/100\n",
      "809/809 [==============================] - 0s - loss: 145.8742 - val_loss: 114.2899\n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 144.2221 - val_loss: 113.1290\n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 148.6146 - val_loss: 112.0462\n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 139.2143 - val_loss: 112.4020\n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 142.9047 - val_loss: 108.8611\n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 136.8195 - val_loss: 107.7342\n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 132.4679 - val_loss: 106.9419\n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 140.2173 - val_loss: 107.8152\n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 135.7686 - val_loss: 106.5634\n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 133.2234 - val_loss: 105.1957\n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 137.6056 - val_loss: 105.2514\n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 124.3371 - val_loss: 106.5537\n",
      "Epoch 50/100\n",
      "809/809 [==============================] - 0s - loss: 127.9037 - val_loss: 103.0349\n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 131.1712 - val_loss: 106.1076\n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 126.1957 - val_loss: 102.2696\n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 124.4687 - val_loss: 103.6162\n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 119.2589 - val_loss: 100.2140\n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 131.3611 - val_loss: 101.1622\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 119.4503 - val_loss: 100.7225\n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 119.3674 - val_loss: 99.4752\n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 123.3688 - val_loss: 99.7792\n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 124.6628 - val_loss: 99.5434\n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 119.8056 - val_loss: 98.3920\n",
      "Epoch 61/100\n",
      "809/809 [==============================] - ETA: 0s - loss: 116.310 - 0s - loss: 115.7765 - val_loss: 97.7009\n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 119.6653 - val_loss: 97.7458\n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 125.3531 - val_loss: 97.0545\n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 118.4501 - val_loss: 96.0260\n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 117.4572 - val_loss: 95.6141\n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 112.5245 - val_loss: 95.0281\n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 112.8004 - val_loss: 95.1824\n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 114.9116 - val_loss: 95.0955\n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 119.0329 - val_loss: 93.5595\n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 111.2485 - val_loss: 93.9682\n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 115.7258 - val_loss: 94.5117\n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 111.6774 - val_loss: 93.7402\n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 108.8891 - val_loss: 93.6403\n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 108.0443 - val_loss: 92.3499\n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 106.6103 - val_loss: 91.9464\n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 105.6559 - val_loss: 94.4357\n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 104.4307 - val_loss: 90.8756\n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 110.6936 - val_loss: 90.6534\n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 105.4081 - val_loss: 92.4021\n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 104.4677 - val_loss: 90.7467\n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 114.1647 - val_loss: 91.8877\n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 112.1009 - val_loss: 90.5163\n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 105.6772 - val_loss: 89.6501\n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 111.2583 - val_loss: 89.1049\n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 112.3703 - val_loss: 89.7539\n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 109.4637 - val_loss: 88.9570\n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 105.2745 - val_loss: 88.0999\n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 111.0363 - val_loss: 87.7758\n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 102.4007 - val_loss: 88.2298\n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 102.4377 - val_loss: 89.7362\n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 103.4016 - val_loss: 88.5532\n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 101.7900 - val_loss: 88.1784\n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 103.4252 - val_loss: 87.0061\n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 104.8013 - val_loss: 87.0359\n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 98.3956 - val_loss: 87.1133\n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 103.7598 - val_loss: 87.2637\n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 101.1032 - val_loss: 86.3327\n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 101.0210 - val_loss: 89.2259\n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 101.3373 - val_loss: 87.0802\n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 102.5446 - val_loss: 87.2123\n",
      "Fold 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-3eee47351a07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mblend_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblend_test1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mblend_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblend_test2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mblend_train4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblend_test4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mblend_train5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblend_test5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-1cb93574e03e>\u001b[0m in \u001b[0;36mget_oof\u001b[1;34m(clf, X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mkf_y_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf_X_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkf_y_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mblend_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkf_X_test\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 992*2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py\u001b[0m in \u001b[0;36mregression\u001b[1;34m(rss, locations)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\notebooks\\indoor position\\auto_regression.py\u001b[0m in \u001b[0;36mtrain_val\u001b[1;34m(rss, locations)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtrain_Xx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Yy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_Xx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Yy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predata' is not defined"
     ]
    }
   ],
   "source": [
    "blend_train1, blend_test1 = get_oof(clf1, trainX, trainY, testX)\n",
    "blend_train2, blend_test2 = get_oof(clf2, trainX, trainY, testX)\n",
    "blend_train4, blend_test4 = get_oof(clf4, trainX, trainY, testX)\n",
    "blend_train5, blend_test5 = get_oof(clf5, trainX, trainY, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_train1 =reshaped(blend_train1)\n",
    "blend_test1 = reshaped(blend_test1)\n",
    "blend_train2 =reshaped(blend_train2)\n",
    "blend_test2 = reshaped(blend_test2)\n",
    "blend_train3 =reshaped(blend_train3)\n",
    "blend_test3 = reshaped(blend_test3)\n",
    "blend_train4 =reshaped(blend_train4)\n",
    "blend_test4 = reshaped(blend_test4)\n",
    "blend_train5 =reshaped(blend_train5)\n",
    "blend_test5 = reshaped(blend_test5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_trainX = np.column_stack((blend_train1,blend_train2,blend_train3,blend_train4,blend_train5))\n",
    "blend_testX = np.column_stack((blend_test1,blend_test2,blend_test3,blend_test4,blend_test5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_s = reshaped(trainY)\n",
    "testY_s = reshaped(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking model 1\n",
    "from sklearn import linear_model\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(blend_trainX, trainY_s)\n",
    "pred1 = lr.predict(blend_testX)\n",
    "e, a = accuracy(pred1.reshape((226,2)), testY)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <function train_model at 0x0000016E1020A400>\n",
      "Fold 0\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4772.4495 - val_loss: 4640.5937\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4580.5283 - val_loss: 4301.8972\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4048.7716 - val_loss: 3501.5784\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3026.3709 - val_loss: 2262.0923\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1826.8906 - val_loss: 1311.9731\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1118.8428 - val_loss: 884.2793\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 783.5000 - val_loss: 644.1486\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 613.6579 - val_loss: 505.8835\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 510.0662 - val_loss: 413.3073\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 442.2402 - val_loss: 357.3569\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 395.9252 - val_loss: 316.3412\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 359.7618 - val_loss: 289.3404\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 332.4323 - val_loss: 270.8572\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 306.5305 - val_loss: 252.9656\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 305.8137 - val_loss: 245.6081\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 292.1738 - val_loss: 232.9654\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 275.9443 - val_loss: 222.5486\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 275.3529 - val_loss: 216.8672\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 254.4975 - val_loss: 206.6164\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 242.2780 - val_loss: 199.9100\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 240.0937 - val_loss: 193.4837\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 218.2296 - val_loss: 183.5675\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 222.3459 - val_loss: 176.3101\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 210.5334 - val_loss: 167.7171\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 209.3445 - val_loss: 161.6821\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 189.3433 - val_loss: 154.2148\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 182.6669 - val_loss: 146.9419\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 182.3186 - val_loss: 145.4749\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 173.2343 - val_loss: 140.4699\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 167.5253 - val_loss: 138.0908\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 162.6557 - val_loss: 134.7784\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 170.9693 - val_loss: 132.7964\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 158.8453 - val_loss: 130.8291\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 155.2656 - val_loss: 127.6101\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 151.1276 - val_loss: 126.5657\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 149.7685 - val_loss: 124.1619\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 154.6614 - val_loss: 126.1565\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 157.5312 - val_loss: 120.2600\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 149.9497 - val_loss: 122.1297\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 143.2656 - val_loss: 119.9666\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 147.8321 - val_loss: 119.6749\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 145.2302 - val_loss: 117.8620\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 141.6195 - val_loss: 115.8962\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 138.3438 - val_loss: 115.5766\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 152.9143 - val_loss: 114.0976\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 137.1759 - val_loss: 116.1346\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 145.6930 - val_loss: 111.0805\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 142.1944 - val_loss: 114.4875\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 136.1762 - val_loss: 111.8426\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 129.9450 - val_loss: 109.3137\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 134.4404 - val_loss: 109.4950\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 126.5326 - val_loss: 107.8841\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 126.5332 - val_loss: 108.6250\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 137.1883 - val_loss: 106.4717\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 121.3208 - val_loss: 109.0949\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 121.8152 - val_loss: 105.7763\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 125.0661 - val_loss: 105.4347\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 126.3727 - val_loss: 104.3404\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 120.6149 - val_loss: 107.0916\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 126.7808 - val_loss: 103.0236\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 123.0569 - val_loss: 103.3583\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 120.6675 - val_loss: 100.2906\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 111.8366 - val_loss: 100.9787\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 123.5391 - val_loss: 101.5257\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 115.1529 - val_loss: 102.2054\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 115.9208 - val_loss: 101.0450\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 117.3973 - val_loss: 99.3585\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 114.4905 - val_loss: 99.9687\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 126.7847 - val_loss: 96.9263\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 112.1303 - val_loss: 101.6268\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 119.7820 - val_loss: 98.1758\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 115.1963 - val_loss: 99.3716\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 106.2991 - val_loss: 94.8566\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 116.1861 - val_loss: 94.0770\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 110.9481 - val_loss: 94.6215\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 111.2224 - val_loss: 92.8053\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 115.2153 - val_loss: 92.3511\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 118.5890 - val_loss: 93.8975\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 106.2785 - val_loss: 92.8464\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 110.7265 - val_loss: 92.8594\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 107.8529 - val_loss: 92.2557\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 111.4620 - val_loss: 92.9608\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 111.1207 - val_loss: 95.4839\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 112.1969 - val_loss: 92.3370\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 108.2908 - val_loss: 90.4180\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 108.6955 - val_loss: 90.5200\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 107.2822 - val_loss: 89.8054\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 109.6533 - val_loss: 89.9213\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 102.1250 - val_loss: 89.6816\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.7372 - val_loss: 91.9743\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 105.7120 - val_loss: 87.9380\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 106.4221 - val_loss: 91.4754\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 103.8876 - val_loss: 86.8150\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 104.1479 - val_loss: 84.9420\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 103.8890 - val_loss: 84.2190\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 105.5086 - val_loss: 90.0635\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 104.2573 - val_loss: 85.1193\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 104.9343 - val_loss: 85.8087\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 102.0560 - val_loss: 87.2996\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 101.2841 - val_loss: 88.5339\n",
      "Fold 1\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4667.7040 - val_loss: 4703.0165\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4465.3607 - val_loss: 4351.8697\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3930.3594 - val_loss: 3535.6204\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 2893.9457 - val_loss: 2248.7289\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1647.0550 - val_loss: 1199.9655\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1024.2495 - val_loss: 865.5656\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 767.8163 - val_loss: 654.8630\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 601.5732 - val_loss: 522.0843\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 482.5824 - val_loss: 420.5019\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 428.4722 - val_loss: 357.7609\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 387.5897 - val_loss: 318.0688\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 357.9018 - val_loss: 289.0264\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 330.3932 - val_loss: 267.2426\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 310.2478 - val_loss: 252.8560\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 296.4046 - val_loss: 242.0721\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 285.2358 - val_loss: 230.7206\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 278.6968 - val_loss: 222.7336\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 273.7382 - val_loss: 212.8038\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 253.9556 - val_loss: 204.2475\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 235.6870 - val_loss: 197.4845\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 241.2457 - val_loss: 187.1947\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 230.3985 - val_loss: 178.5693\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 219.9770 - val_loss: 169.2788\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 213.5371 - val_loss: 159.9852\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 199.2237 - val_loss: 150.8307\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 195.7340 - val_loss: 144.2900\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 183.8086 - val_loss: 137.4144\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 179.4054 - val_loss: 134.0998\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 178.9019 - val_loss: 129.2801\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 160.7407 - val_loss: 126.4414\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 174.9727 - val_loss: 124.9730\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 162.1593 - val_loss: 123.1290\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 164.3505 - val_loss: 121.2228\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 157.8778 - val_loss: 118.9478\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 153.6529 - val_loss: 117.3267\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 147.7329 - val_loss: 115.0530\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 159.6833 - val_loss: 113.6392\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 148.1676 - val_loss: 112.5898\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 142.5814 - val_loss: 111.9792\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 146.3120 - val_loss: 109.8455\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 146.7289 - val_loss: 109.6402\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 139.4747 - val_loss: 108.8552\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 135.3921 - val_loss: 108.1405\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 140.9861 - val_loss: 106.8532\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 133.5303 - val_loss: 105.7965\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 140.8455 - val_loss: 105.1964\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 125.8469 - val_loss: 104.8869\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 137.4922 - val_loss: 105.3078\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 134.5365 - val_loss: 105.4887\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 135.2666 - val_loss: 106.4181\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 131.2552 - val_loss: 105.3327\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 125.4470 - val_loss: 103.9191\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 130.4294 - val_loss: 102.9629\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 129.3433 - val_loss: 102.1881\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 124.6093 - val_loss: 100.8968\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 131.2524 - val_loss: 100.5565\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 129.6080 - val_loss: 100.9115\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 122.8859 - val_loss: 101.0672\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 122.0423 - val_loss: 99.4859\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 127.9640 - val_loss: 98.2821\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 119.8024 - val_loss: 99.4905\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 119.9744 - val_loss: 97.9032\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 120.8982 - val_loss: 97.8996\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 114.8312 - val_loss: 96.7076\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 120.6393 - val_loss: 97.4722\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 118.5525 - val_loss: 96.9715\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 119.8882 - val_loss: 96.2566\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 116.0455 - val_loss: 96.9169\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 110.3638 - val_loss: 96.5894\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 119.3247 - val_loss: 94.0773\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.0894 - val_loss: 93.1979\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 114.1282 - val_loss: 93.8057\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 115.5467 - val_loss: 91.9086\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 114.4644 - val_loss: 92.6957\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 115.7478 - val_loss: 91.9154\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 111.0712 - val_loss: 92.5960\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 113.1139 - val_loss: 92.5831\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 114.2932 - val_loss: 91.6535\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 106.5626 - val_loss: 91.4921\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 111.0587 - val_loss: 91.6757\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 107.8344 - val_loss: 91.5114\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 111.7787 - val_loss: 91.5896\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 104.7042 - val_loss: 91.0780\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 106.1407 - val_loss: 91.7440\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 108.7030 - val_loss: 90.9294\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 106.5040 - val_loss: 91.5978\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 113.4763 - val_loss: 89.4336\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 107.3287 - val_loss: 91.4039\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 108.0307 - val_loss: 89.9113\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 109.1342 - val_loss: 89.0300\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 111.6898 - val_loss: 90.6646\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 102.1665 - val_loss: 89.1648\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 100.3341 - val_loss: 89.3472\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 101.8794 - val_loss: 90.6743\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 103.3956 - val_loss: 88.1019\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 110.4989 - val_loss: 90.7075\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 104.1985 - val_loss: 88.7325\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 98.2038 - val_loss: 88.0959\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 101.2244 - val_loss: 87.8376\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 100.7835 - val_loss: 86.6219\n",
      "Fold 2\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4561.4532 - val_loss: 4750.5541\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4392.2985 - val_loss: 4460.7461\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 3938.0222 - val_loss: 3757.0508\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3005.9386 - val_loss: 2565.0992\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1821.2718 - val_loss: 1430.13382044.\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1104.9248 - val_loss: 941.0561\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 803.7285 - val_loss: 724.3488\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 613.2543 - val_loss: 594.1773\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 524.9068 - val_loss: 481.5924\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 437.9370 - val_loss: 416.8867\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 393.8429 - val_loss: 369.8317\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 366.1070 - val_loss: 337.7902\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 333.9424 - val_loss: 303.6076\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 318.5348 - val_loss: 290.5592\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 290.8840 - val_loss: 267.6768\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 280.4235 - val_loss: 252.0221\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 265.6561 - val_loss: 245.0255\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 268.9152 - val_loss: 231.2394\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 253.1333 - val_loss: 220.8123\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 238.2172 - val_loss: 207.6477\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 228.8043 - val_loss: 194.8839\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 228.8907 - val_loss: 186.1930\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 197.5616 - val_loss: 174.7377\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 189.6827 - val_loss: 164.5114\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 192.3108 - val_loss: 162.3670\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 190.4593 - val_loss: 155.9794\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 175.4367 - val_loss: 151.7502\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 183.7709 - val_loss: 146.3706\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 173.4491 - val_loss: 144.2147\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 167.3455 - val_loss: 141.4658\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 164.0585 - val_loss: 138.5176\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 162.7091 - val_loss: 136.6150\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 159.2024 - val_loss: 139.9483\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 155.4444 - val_loss: 132.2293\n",
      "Epoch 35/100\n",
      "808/808 [==============================] - 0s - loss: 155.2941 - val_loss: 132.5453\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 144.2525 - val_loss: 130.5052\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 148.2325 - val_loss: 130.0585\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 146.4149 - val_loss: 129.1485\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 147.8810 - val_loss: 128.1575\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 142.1051 - val_loss: 124.6401\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 144.7665 - val_loss: 127.9571\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 153.0161 - val_loss: 122.0400\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 146.0846 - val_loss: 124.4153\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 139.7934 - val_loss: 121.0209\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 134.0579 - val_loss: 125.7947\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 134.4913 - val_loss: 118.8272\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 142.6298 - val_loss: 121.0540\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 133.6099 - val_loss: 123.2915\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 130.2103 - val_loss: 118.9706\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 126.8065 - val_loss: 119.6721\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 130.3092 - val_loss: 120.4533\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 130.7512 - val_loss: 115.7854\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 120.5768 - val_loss: 117.2649\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 128.6807 - val_loss: 115.8351\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 127.7445 - val_loss: 113.6157\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 130.4411 - val_loss: 114.6029\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 124.0644 - val_loss: 114.1236\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 120.8488 - val_loss: 112.9613\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 119.9951 - val_loss: 112.3705\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 127.2782 - val_loss: 113.8438\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 122.1985 - val_loss: 114.3501\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 122.8530 - val_loss: 112.8491\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 123.2283 - val_loss: 111.6730\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 117.1869 - val_loss: 111.9229\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 109.7939 - val_loss: 112.3414\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 112.0619 - val_loss: 108.8717\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 111.9464 - val_loss: 110.7687\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 117.7913 - val_loss: 109.7366\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 117.7553 - val_loss: 110.6776\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 110.8256 - val_loss: 108.8364\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.2831 - val_loss: 108.8778\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 117.0628 - val_loss: 106.6300\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 115.2681 - val_loss: 107.3921\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 114.3448 - val_loss: 106.4201\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 105.2188 - val_loss: 105.7909\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 109.0645 - val_loss: 105.0423\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 111.4414 - val_loss: 106.6953\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 115.5860 - val_loss: 105.6495\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 110.8976 - val_loss: 104.7265\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 111.1662 - val_loss: 105.9267\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 109.3507 - val_loss: 103.7907\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 107.2699 - val_loss: 106.5498\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 108.2479 - val_loss: 108.5365\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 113.8969 - val_loss: 102.3650\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 103.6714 - val_loss: 103.5086\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 100.1145 - val_loss: 102.3107\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 107.6916 - val_loss: 103.1564\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 101.9172 - val_loss: 104.3078\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 102.4762 - val_loss: 102.4571\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.6236 - val_loss: 102.3928\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 103.2239 - val_loss: 103.7197\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 104.1986 - val_loss: 102.5619\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 99.8103 - val_loss: 101.2557\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 104.9861 - val_loss: 102.6072\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 99.9725 - val_loss: 102.9607\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 105.6094 - val_loss: 99.5945\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 107.3039 - val_loss: 103.6705\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 101.2857 - val_loss: 100.9786\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 101.6940 - val_loss: 101.3194\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 101.5891 - val_loss: 99.3880\n",
      "Fold 3\n",
      "Train on 808 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "808/808 [==============================] - 0s - loss: 4659.6294 - val_loss: 4517.3850\n",
      "Epoch 2/100\n",
      "808/808 [==============================] - 0s - loss: 4504.3169 - val_loss: 4256.7005\n",
      "Epoch 3/100\n",
      "808/808 [==============================] - 0s - loss: 4066.7032 - val_loss: 3599.1973\n",
      "Epoch 4/100\n",
      "808/808 [==============================] - 0s - loss: 3145.2579 - val_loss: 2462.0438\n",
      "Epoch 5/100\n",
      "808/808 [==============================] - 0s - loss: 1924.0442 - val_loss: 1363.5372\n",
      "Epoch 6/100\n",
      "808/808 [==============================] - 0s - loss: 1135.2838 - val_loss: 910.5405\n",
      "Epoch 7/100\n",
      "808/808 [==============================] - 0s - loss: 812.2250 - val_loss: 696.6948\n",
      "Epoch 8/100\n",
      "808/808 [==============================] - 0s - loss: 648.1728 - val_loss: 563.7726\n",
      "Epoch 9/100\n",
      "808/808 [==============================] - 0s - loss: 535.6470 - val_loss: 457.1715\n",
      "Epoch 10/100\n",
      "808/808 [==============================] - 0s - loss: 452.1432 - val_loss: 385.3471\n",
      "Epoch 11/100\n",
      "808/808 [==============================] - 0s - loss: 404.8978 - val_loss: 339.7868\n",
      "Epoch 12/100\n",
      "808/808 [==============================] - 0s - loss: 360.6287 - val_loss: 307.8616\n",
      "Epoch 13/100\n",
      "808/808 [==============================] - 0s - loss: 339.9667 - val_loss: 281.7125\n",
      "Epoch 14/100\n",
      "808/808 [==============================] - 0s - loss: 328.0847 - val_loss: 262.0331\n",
      "Epoch 15/100\n",
      "808/808 [==============================] - 0s - loss: 305.9513 - val_loss: 241.8550\n",
      "Epoch 16/100\n",
      "808/808 [==============================] - 0s - loss: 297.7659 - val_loss: 230.4316\n",
      "Epoch 17/100\n",
      "808/808 [==============================] - 0s - loss: 273.6988 - val_loss: 216.2693\n",
      "Epoch 18/100\n",
      "808/808 [==============================] - 0s - loss: 256.7508 - val_loss: 207.9887\n",
      "Epoch 19/100\n",
      "808/808 [==============================] - 0s - loss: 255.0968 - val_loss: 198.9484\n",
      "Epoch 20/100\n",
      "808/808 [==============================] - 0s - loss: 241.5962 - val_loss: 189.8802\n",
      "Epoch 21/100\n",
      "808/808 [==============================] - 0s - loss: 231.3172 - val_loss: 179.4572\n",
      "Epoch 22/100\n",
      "808/808 [==============================] - 0s - loss: 226.9230 - val_loss: 173.6735\n",
      "Epoch 23/100\n",
      "808/808 [==============================] - 0s - loss: 212.9660 - val_loss: 164.6026\n",
      "Epoch 24/100\n",
      "808/808 [==============================] - 0s - loss: 202.2192 - val_loss: 157.4175\n",
      "Epoch 25/100\n",
      "808/808 [==============================] - 0s - loss: 198.9920 - val_loss: 148.1431\n",
      "Epoch 26/100\n",
      "808/808 [==============================] - 0s - loss: 182.9888 - val_loss: 141.6970\n",
      "Epoch 27/100\n",
      "808/808 [==============================] - 0s - loss: 182.3726 - val_loss: 137.9063\n",
      "Epoch 28/100\n",
      "808/808 [==============================] - 0s - loss: 179.8147 - val_loss: 137.3593\n",
      "Epoch 29/100\n",
      "808/808 [==============================] - 0s - loss: 162.8733 - val_loss: 129.8383\n",
      "Epoch 30/100\n",
      "808/808 [==============================] - 0s - loss: 168.9573 - val_loss: 128.2208: 173.95 - ETA: 0s - loss: 171.222\n",
      "Epoch 31/100\n",
      "808/808 [==============================] - 0s - loss: 166.8833 - val_loss: 126.7105\n",
      "Epoch 32/100\n",
      "808/808 [==============================] - 0s - loss: 166.9809 - val_loss: 125.3909\n",
      "Epoch 33/100\n",
      "808/808 [==============================] - 0s - loss: 163.0879 - val_loss: 124.6523\n",
      "Epoch 34/100\n",
      "808/808 [==============================] - 0s - loss: 158.2623 - val_loss: 120.2279\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808/808 [==============================] - 0s - loss: 157.6064 - val_loss: 122.5402\n",
      "Epoch 36/100\n",
      "808/808 [==============================] - 0s - loss: 151.4503 - val_loss: 117.0512\n",
      "Epoch 37/100\n",
      "808/808 [==============================] - 0s - loss: 143.3432 - val_loss: 116.4519\n",
      "Epoch 38/100\n",
      "808/808 [==============================] - 0s - loss: 145.5523 - val_loss: 115.8898\n",
      "Epoch 39/100\n",
      "808/808 [==============================] - 0s - loss: 152.8682 - val_loss: 114.5363\n",
      "Epoch 40/100\n",
      "808/808 [==============================] - 0s - loss: 152.9077 - val_loss: 111.6395\n",
      "Epoch 41/100\n",
      "808/808 [==============================] - 0s - loss: 144.2571 - val_loss: 112.1791\n",
      "Epoch 42/100\n",
      "808/808 [==============================] - 0s - loss: 142.9322 - val_loss: 110.6159\n",
      "Epoch 43/100\n",
      "808/808 [==============================] - 0s - loss: 141.3125 - val_loss: 110.0932\n",
      "Epoch 44/100\n",
      "808/808 [==============================] - 0s - loss: 135.3025 - val_loss: 109.7531\n",
      "Epoch 45/100\n",
      "808/808 [==============================] - 0s - loss: 139.0961 - val_loss: 110.5549\n",
      "Epoch 46/100\n",
      "808/808 [==============================] - 0s - loss: 139.8776 - val_loss: 108.2969\n",
      "Epoch 47/100\n",
      "808/808 [==============================] - 0s - loss: 136.7304 - val_loss: 113.5132\n",
      "Epoch 48/100\n",
      "808/808 [==============================] - 0s - loss: 127.3299 - val_loss: 106.3025\n",
      "Epoch 49/100\n",
      "808/808 [==============================] - 0s - loss: 136.3339 - val_loss: 108.1835\n",
      "Epoch 50/100\n",
      "808/808 [==============================] - 0s - loss: 133.3990 - val_loss: 103.7366\n",
      "Epoch 51/100\n",
      "808/808 [==============================] - 0s - loss: 129.6637 - val_loss: 106.5759\n",
      "Epoch 52/100\n",
      "808/808 [==============================] - 0s - loss: 127.5254 - val_loss: 104.7675\n",
      "Epoch 53/100\n",
      "808/808 [==============================] - 0s - loss: 132.2351 - val_loss: 103.3879\n",
      "Epoch 54/100\n",
      "808/808 [==============================] - 0s - loss: 134.5150 - val_loss: 103.2043\n",
      "Epoch 55/100\n",
      "808/808 [==============================] - 0s - loss: 126.1098 - val_loss: 101.8478\n",
      "Epoch 56/100\n",
      "808/808 [==============================] - 0s - loss: 117.5494 - val_loss: 101.7829\n",
      "Epoch 57/100\n",
      "808/808 [==============================] - 0s - loss: 133.8056 - val_loss: 101.9528\n",
      "Epoch 58/100\n",
      "808/808 [==============================] - 0s - loss: 125.3172 - val_loss: 100.1262\n",
      "Epoch 59/100\n",
      "808/808 [==============================] - 0s - loss: 120.7315 - val_loss: 101.1850\n",
      "Epoch 60/100\n",
      "808/808 [==============================] - 0s - loss: 122.4761 - val_loss: 99.8698\n",
      "Epoch 61/100\n",
      "808/808 [==============================] - 0s - loss: 125.5269 - val_loss: 99.5905\n",
      "Epoch 62/100\n",
      "808/808 [==============================] - 0s - loss: 115.4527 - val_loss: 98.4324\n",
      "Epoch 63/100\n",
      "808/808 [==============================] - 0s - loss: 126.2173 - val_loss: 99.6028\n",
      "Epoch 64/100\n",
      "808/808 [==============================] - 0s - loss: 120.4664 - val_loss: 97.0634\n",
      "Epoch 65/100\n",
      "808/808 [==============================] - 0s - loss: 119.6387 - val_loss: 98.6210\n",
      "Epoch 66/100\n",
      "808/808 [==============================] - 0s - loss: 118.0745 - val_loss: 96.9664\n",
      "Epoch 67/100\n",
      "808/808 [==============================] - 0s - loss: 112.5888 - val_loss: 93.8096\n",
      "Epoch 68/100\n",
      "808/808 [==============================] - 0s - loss: 114.0672 - val_loss: 97.6500\n",
      "Epoch 69/100\n",
      "808/808 [==============================] - 0s - loss: 114.8260 - val_loss: 95.9619\n",
      "Epoch 70/100\n",
      "808/808 [==============================] - 0s - loss: 110.0272 - val_loss: 96.4171\n",
      "Epoch 71/100\n",
      "808/808 [==============================] - 0s - loss: 113.6112 - val_loss: 96.1166\n",
      "Epoch 72/100\n",
      "808/808 [==============================] - 0s - loss: 112.1138 - val_loss: 95.7657\n",
      "Epoch 73/100\n",
      "808/808 [==============================] - 0s - loss: 110.3059 - val_loss: 95.1692\n",
      "Epoch 74/100\n",
      "808/808 [==============================] - 0s - loss: 117.9345 - val_loss: 97.0277\n",
      "Epoch 75/100\n",
      "808/808 [==============================] - 0s - loss: 114.5213 - val_loss: 96.4393\n",
      "Epoch 76/100\n",
      "808/808 [==============================] - 0s - loss: 115.8963 - val_loss: 96.6044\n",
      "Epoch 77/100\n",
      "808/808 [==============================] - 0s - loss: 114.3094 - val_loss: 95.6948\n",
      "Epoch 78/100\n",
      "808/808 [==============================] - 0s - loss: 105.7911 - val_loss: 95.0974\n",
      "Epoch 79/100\n",
      "808/808 [==============================] - 0s - loss: 109.5286 - val_loss: 93.8298\n",
      "Epoch 80/100\n",
      "808/808 [==============================] - 0s - loss: 109.3982 - val_loss: 92.4208\n",
      "Epoch 81/100\n",
      "808/808 [==============================] - 0s - loss: 108.3238 - val_loss: 92.8237\n",
      "Epoch 82/100\n",
      "808/808 [==============================] - 0s - loss: 112.9447 - val_loss: 92.9711\n",
      "Epoch 83/100\n",
      "808/808 [==============================] - 0s - loss: 113.8448 - val_loss: 94.2258\n",
      "Epoch 84/100\n",
      "808/808 [==============================] - 0s - loss: 106.4842 - val_loss: 92.0957\n",
      "Epoch 85/100\n",
      "808/808 [==============================] - 0s - loss: 103.4633 - val_loss: 90.7264\n",
      "Epoch 86/100\n",
      "808/808 [==============================] - 0s - loss: 109.3379 - val_loss: 90.5027\n",
      "Epoch 87/100\n",
      "808/808 [==============================] - 0s - loss: 102.7414 - val_loss: 91.6341\n",
      "Epoch 88/100\n",
      "808/808 [==============================] - 0s - loss: 109.2562 - val_loss: 91.1635\n",
      "Epoch 89/100\n",
      "808/808 [==============================] - 0s - loss: 104.1715 - val_loss: 89.5310\n",
      "Epoch 90/100\n",
      "808/808 [==============================] - 0s - loss: 103.7760 - val_loss: 93.5970\n",
      "Epoch 91/100\n",
      "808/808 [==============================] - 0s - loss: 108.4033 - val_loss: 88.1100\n",
      "Epoch 92/100\n",
      "808/808 [==============================] - 0s - loss: 99.9842 - val_loss: 90.4759\n",
      "Epoch 93/100\n",
      "808/808 [==============================] - 0s - loss: 105.4798 - val_loss: 90.1501\n",
      "Epoch 94/100\n",
      "808/808 [==============================] - 0s - loss: 98.9645 - val_loss: 91.2668\n",
      "Epoch 95/100\n",
      "808/808 [==============================] - 0s - loss: 104.9930 - val_loss: 91.5581\n",
      "Epoch 96/100\n",
      "808/808 [==============================] - 0s - loss: 106.6334 - val_loss: 87.1761\n",
      "Epoch 97/100\n",
      "808/808 [==============================] - 0s - loss: 99.7803 - val_loss: 87.2458\n",
      "Epoch 98/100\n",
      "808/808 [==============================] - 0s - loss: 99.8949 - val_loss: 87.5604\n",
      "Epoch 99/100\n",
      "808/808 [==============================] - 0s - loss: 103.8974 - val_loss: 88.7255\n",
      "Epoch 100/100\n",
      "808/808 [==============================] - 0s - loss: 100.9595 - val_loss: 86.9961\n",
      "Fold 4\n",
      "Train on 809 samples, validate on 203 samples\n",
      "Epoch 1/100\n",
      "809/809 [==============================] - 0s - loss: 4709.8958 - val_loss: 4328.4418\n",
      "Epoch 2/100\n",
      "809/809 [==============================] - 0s - loss: 4545.6195 - val_loss: 4042.9103\n",
      "Epoch 3/100\n",
      "809/809 [==============================] - 0s - loss: 4086.0087 - val_loss: 3352.0183\n",
      "Epoch 4/100\n",
      "809/809 [==============================] - 0s - loss: 3159.1504 - val_loss: 2204.0706\n",
      "Epoch 5/100\n",
      "809/809 [==============================] - 0s - loss: 1933.6068 - val_loss: 1177.4936\n",
      "Epoch 6/100\n",
      "809/809 [==============================] - 0s - loss: 1158.5693 - val_loss: 775.4289\n",
      "Epoch 7/100\n",
      "809/809 [==============================] - 0s - loss: 814.4858 - val_loss: 546.1533\n",
      "Epoch 8/100\n",
      "809/809 [==============================] - 0s - loss: 658.9352 - val_loss: 422.5983\n",
      "Epoch 9/100\n",
      "809/809 [==============================] - 0s - loss: 527.6782 - val_loss: 345.6455\n",
      "Epoch 10/100\n",
      "809/809 [==============================] - 0s - loss: 434.3515 - val_loss: 300.5253\n",
      "Epoch 11/100\n",
      "809/809 [==============================] - 0s - loss: 382.2034 - val_loss: 272.1679\n",
      "Epoch 12/100\n",
      "809/809 [==============================] - 0s - loss: 363.7057 - val_loss: 250.9049\n",
      "Epoch 13/100\n",
      "809/809 [==============================] - 0s - loss: 333.2821 - val_loss: 236.5287\n",
      "Epoch 14/100\n",
      "809/809 [==============================] - 0s - loss: 322.6051 - val_loss: 223.4058\n",
      "Epoch 15/100\n",
      "809/809 [==============================] - 0s - loss: 301.9060 - val_loss: 213.4477\n",
      "Epoch 16/100\n",
      "809/809 [==============================] - 0s - loss: 303.2105 - val_loss: 205.9170\n",
      "Epoch 17/100\n",
      "809/809 [==============================] - 0s - loss: 281.8691 - val_loss: 196.8300\n",
      "Epoch 18/100\n",
      "809/809 [==============================] - 0s - loss: 269.2022 - val_loss: 190.5836\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809/809 [==============================] - 0s - loss: 269.3947 - val_loss: 181.6825\n",
      "Epoch 20/100\n",
      "809/809 [==============================] - 0s - loss: 266.2401 - val_loss: 172.6825\n",
      "Epoch 21/100\n",
      "809/809 [==============================] - 0s - loss: 246.9815 - val_loss: 167.5511\n",
      "Epoch 22/100\n",
      "809/809 [==============================] - 0s - loss: 237.9934 - val_loss: 156.6410\n",
      "Epoch 23/100\n",
      "809/809 [==============================] - 0s - loss: 227.8983 - val_loss: 147.0431\n",
      "Epoch 24/100\n",
      "809/809 [==============================] - 0s - loss: 209.9623 - val_loss: 137.0303\n",
      "Epoch 25/100\n",
      "809/809 [==============================] - 0s - loss: 202.8689 - val_loss: 129.8829\n",
      "Epoch 26/100\n",
      "809/809 [==============================] - 0s - loss: 208.6081 - val_loss: 120.9383\n",
      "Epoch 27/100\n",
      "809/809 [==============================] - 0s - loss: 193.0405 - val_loss: 115.2023\n",
      "Epoch 28/100\n",
      "809/809 [==============================] - 0s - loss: 185.4166 - val_loss: 111.6078\n",
      "Epoch 29/100\n",
      "809/809 [==============================] - 0s - loss: 170.4553 - val_loss: 106.3502\n",
      "Epoch 30/100\n",
      "809/809 [==============================] - 0s - loss: 178.5359 - val_loss: 104.9428\n",
      "Epoch 31/100\n",
      "809/809 [==============================] - 0s - loss: 179.0472 - val_loss: 102.0599\n",
      "Epoch 32/100\n",
      "809/809 [==============================] - 0s - loss: 167.9920 - val_loss: 100.9409\n",
      "Epoch 33/100\n",
      "809/809 [==============================] - 0s - loss: 167.4525 - val_loss: 99.1543\n",
      "Epoch 34/100\n",
      "809/809 [==============================] - 0s - loss: 159.6613 - val_loss: 96.1683\n",
      "Epoch 35/100\n",
      "809/809 [==============================] - 0s - loss: 165.8499 - val_loss: 95.2958\n",
      "Epoch 36/100\n",
      "809/809 [==============================] - 0s - loss: 159.8605 - val_loss: 95.6085\n",
      "Epoch 37/100\n",
      "809/809 [==============================] - 0s - loss: 151.0703 - val_loss: 95.0255\n",
      "Epoch 38/100\n",
      "809/809 [==============================] - 0s - loss: 155.7174 - val_loss: 95.6901\n",
      "Epoch 39/100\n",
      "809/809 [==============================] - 0s - loss: 157.5469 - val_loss: 91.5722\n",
      "Epoch 40/100\n",
      "809/809 [==============================] - 0s - loss: 158.9979 - val_loss: 91.4098\n",
      "Epoch 41/100\n",
      "809/809 [==============================] - 0s - loss: 153.6677 - val_loss: 90.0183\n",
      "Epoch 42/100\n",
      "809/809 [==============================] - 0s - loss: 153.9337 - val_loss: 89.8406\n",
      "Epoch 43/100\n",
      "809/809 [==============================] - 0s - loss: 152.8657 - val_loss: 90.2600\n",
      "Epoch 44/100\n",
      "809/809 [==============================] - 0s - loss: 139.5044 - val_loss: 87.2658\n",
      "Epoch 45/100\n",
      "809/809 [==============================] - 0s - loss: 150.2569 - val_loss: 89.1828\n",
      "Epoch 46/100\n",
      "809/809 [==============================] - 0s - loss: 144.8902 - val_loss: 86.7195\n",
      "Epoch 47/100\n",
      "809/809 [==============================] - 0s - loss: 149.2905 - val_loss: 86.4434\n",
      "Epoch 48/100\n",
      "809/809 [==============================] - 0s - loss: 140.9986 - val_loss: 84.3948\n",
      "Epoch 49/100\n",
      "809/809 [==============================] - 0s - loss: 137.5397 - val_loss: 83.8774\n",
      "Epoch 50/100\n",
      "809/809 [==============================] - 0s - loss: 136.6743 - val_loss: 83.6312\n",
      "Epoch 51/100\n",
      "809/809 [==============================] - 0s - loss: 143.9925 - val_loss: 82.7782\n",
      "Epoch 52/100\n",
      "809/809 [==============================] - 0s - loss: 129.7636 - val_loss: 84.0863\n",
      "Epoch 53/100\n",
      "809/809 [==============================] - 0s - loss: 143.0705 - val_loss: 81.8828\n",
      "Epoch 54/100\n",
      "809/809 [==============================] - 0s - loss: 132.9166 - val_loss: 81.3693\n",
      "Epoch 55/100\n",
      "809/809 [==============================] - 0s - loss: 131.1698 - val_loss: 82.9902\n",
      "Epoch 56/100\n",
      "809/809 [==============================] - 0s - loss: 133.0080 - val_loss: 81.3472\n",
      "Epoch 57/100\n",
      "809/809 [==============================] - 0s - loss: 131.6749 - val_loss: 80.6923\n",
      "Epoch 58/100\n",
      "809/809 [==============================] - 0s - loss: 136.5414 - val_loss: 81.2433\n",
      "Epoch 59/100\n",
      "809/809 [==============================] - 0s - loss: 132.1188 - val_loss: 80.9572\n",
      "Epoch 60/100\n",
      "809/809 [==============================] - 0s - loss: 135.4400 - val_loss: 81.4201\n",
      "Epoch 61/100\n",
      "809/809 [==============================] - 0s - loss: 131.5834 - val_loss: 78.9112\n",
      "Epoch 62/100\n",
      "809/809 [==============================] - 0s - loss: 127.4339 - val_loss: 79.3915\n",
      "Epoch 63/100\n",
      "809/809 [==============================] - 0s - loss: 128.6503 - val_loss: 78.1683\n",
      "Epoch 64/100\n",
      "809/809 [==============================] - 0s - loss: 120.3752 - val_loss: 76.9014\n",
      "Epoch 65/100\n",
      "809/809 [==============================] - 0s - loss: 124.0933 - val_loss: 78.7252\n",
      "Epoch 66/100\n",
      "809/809 [==============================] - 0s - loss: 120.8820 - val_loss: 77.5295\n",
      "Epoch 67/100\n",
      "809/809 [==============================] - 0s - loss: 126.7966 - val_loss: 76.5681\n",
      "Epoch 68/100\n",
      "809/809 [==============================] - 0s - loss: 126.2810 - val_loss: 77.4910\n",
      "Epoch 69/100\n",
      "809/809 [==============================] - 0s - loss: 125.8501 - val_loss: 77.7882\n",
      "Epoch 70/100\n",
      "809/809 [==============================] - 0s - loss: 116.1419 - val_loss: 77.6936\n",
      "Epoch 71/100\n",
      "809/809 [==============================] - 0s - loss: 121.0682 - val_loss: 78.2609\n",
      "Epoch 72/100\n",
      "809/809 [==============================] - 0s - loss: 114.9232 - val_loss: 75.1610\n",
      "Epoch 73/100\n",
      "809/809 [==============================] - 0s - loss: 116.2283 - val_loss: 75.1190\n",
      "Epoch 74/100\n",
      "809/809 [==============================] - 0s - loss: 111.3659 - val_loss: 79.8599\n",
      "Epoch 75/100\n",
      "809/809 [==============================] - 0s - loss: 119.6955 - val_loss: 74.7567\n",
      "Epoch 76/100\n",
      "809/809 [==============================] - 0s - loss: 115.8641 - val_loss: 75.0045\n",
      "Epoch 77/100\n",
      "809/809 [==============================] - 0s - loss: 111.8685 - val_loss: 73.6494\n",
      "Epoch 78/100\n",
      "809/809 [==============================] - 0s - loss: 122.6598 - val_loss: 73.7541\n",
      "Epoch 79/100\n",
      "809/809 [==============================] - 0s - loss: 119.0854 - val_loss: 73.8401\n",
      "Epoch 80/100\n",
      "809/809 [==============================] - 0s - loss: 112.8904 - val_loss: 74.2809\n",
      "Epoch 81/100\n",
      "809/809 [==============================] - 0s - loss: 113.3239 - val_loss: 73.5218\n",
      "Epoch 82/100\n",
      "809/809 [==============================] - 0s - loss: 112.6807 - val_loss: 72.5215\n",
      "Epoch 83/100\n",
      "809/809 [==============================] - 0s - loss: 111.9385 - val_loss: 72.4924\n",
      "Epoch 84/100\n",
      "809/809 [==============================] - 0s - loss: 115.9394 - val_loss: 72.1265\n",
      "Epoch 85/100\n",
      "809/809 [==============================] - 0s - loss: 118.2744 - val_loss: 72.4520\n",
      "Epoch 86/100\n",
      "809/809 [==============================] - 0s - loss: 111.1091 - val_loss: 71.7772\n",
      "Epoch 87/100\n",
      "809/809 [==============================] - 0s - loss: 112.8202 - val_loss: 72.5548\n",
      "Epoch 88/100\n",
      "809/809 [==============================] - 0s - loss: 117.3933 - val_loss: 70.9232\n",
      "Epoch 89/100\n",
      "809/809 [==============================] - 0s - loss: 113.7544 - val_loss: 72.8804\n",
      "Epoch 90/100\n",
      "809/809 [==============================] - 0s - loss: 112.2343 - val_loss: 71.9317\n",
      "Epoch 91/100\n",
      "809/809 [==============================] - 0s - loss: 112.5332 - val_loss: 70.8824\n",
      "Epoch 92/100\n",
      "809/809 [==============================] - 0s - loss: 108.3940 - val_loss: 72.5073\n",
      "Epoch 93/100\n",
      "809/809 [==============================] - 0s - loss: 111.7654 - val_loss: 69.6693\n",
      "Epoch 94/100\n",
      "809/809 [==============================] - 0s - loss: 112.1406 - val_loss: 71.5999\n",
      "Epoch 95/100\n",
      "809/809 [==============================] - 0s - loss: 106.9034 - val_loss: 69.7198\n",
      "Epoch 96/100\n",
      "809/809 [==============================] - 0s - loss: 106.2016 - val_loss: 71.1750\n",
      "Epoch 97/100\n",
      "809/809 [==============================] - 0s - loss: 112.1195 - val_loss: 69.6313\n",
      "Epoch 98/100\n",
      "809/809 [==============================] - 0s - loss: 105.4604 - val_loss: 70.6459\n",
      "Epoch 99/100\n",
      "809/809 [==============================] - 0s - loss: 101.3325 - val_loss: 69.5780\n",
      "Epoch 100/100\n",
      "809/809 [==============================] - 0s - loss: 109.4718 - val_loss: 68.1884\n",
      "1 <function regression at 0x0000016E1020A268>\n",
      "2 <bound method SupervisedFloatMixin.fit of KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
      "          weights='uniform')>\n",
      "3 <bound method DecisionTreeRegressor.fit of DecisionTreeRegressor(criterion='mse', max_depth=7, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "           splitter='best')>\n",
      "4 <bound method DecisionTreeRegressor.fit of ExtraTreeRegressor(criterion='mse', max_depth=7, max_features='auto',\n",
      "          max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "          min_samples_leaf=1, min_samples_split=2,\n",
      "          min_weight_fraction_leaf=0.0, random_state=None,\n",
      "          splitter='random')>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for j, clf in enumerate(clfs):    \n",
    "    print(j, clf)\n",
    "    blend_test_j = np.zeros(shape=(5, len(testX),2))\n",
    "    for i, (train_index, test_index) in enumerate(skf):\n",
    "        print(\"Fold\", i)   \n",
    "\n",
    "        X_train = trainX[train_index]\n",
    "        y_train = trainY[train_index]\n",
    "        X_test = trainX[test_index]\n",
    "        y_test = trainY[test_index]\n",
    "\n",
    "    #     model1 = rr.train_model(X_train,y_train)\n",
    "        model = clf(X_train,y_train)\n",
    "        blend_train[j, test_index, :] =  model.predict(X_test)\n",
    "        blend_test_j[:,:,i] =   model.predict(testX)\n",
    "        \n",
    "\n",
    "    blend_test[:,:,j] = blend_test_j.mean(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 49.99036026,   0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [ 34.67944336,   0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_train[69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-0851c735d657>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mneig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mneig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblend_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpred3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblend_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0me3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    739\u001b[0m         \"\"\"\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\software\\WinPython-64bit-3.6.2.0Qt5\\python-3.6.2.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 405\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "# stacking model 3\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neig = KNeighborsRegressor(n_neighbors=4)\n",
    "neig.fit(blend_train,trainY)\n",
    "pred3 = neig.predict(blend_test)\n",
    "e3, a3 = accuracy(pred3, test_y)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.]]]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((3,2,5))\n",
    "print(a)\n",
    "print(a[:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0],\n",
       "       [1, 9],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.array([[2,0],[1,9],[3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
